{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eb37793e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BigQuery client initialized for project: scope3-dev\n",
      "üöÄ STARTING GEMINI PRO 1.5 OMNI ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üì∞ ANALYZING META DATASET...\n",
      "üì• Loading data from BA_Meta_Gemini_Pro_Judge_Results...\n",
      "‚úÖ Loaded 299 records from BA_Meta_Gemini_Pro_Judge_Results\n",
      "\n",
      "üéØ CALCULATING ML METRICS FOR META DATASET\n",
      "============================================================\n",
      "üìä Class Distribution (Meta):\n",
      "   Flash Positive (Aligned): 0 (0.0%)\n",
      "   Flash Negative (Not-Aligned): 112 (37.5%)\n",
      "   Pro 1.5 Positive: 138 (46.2%)\n",
      "   Pro 1.5 Negative: 161 (53.8%)\n",
      "‚ùå Error calculating metrics for Meta: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
      "\n",
      "üåê ANALYZING WEB DATASET...\n",
      "üì• Loading data from BA_Web_Gemini_Pro_Judge_Results...\n",
      "‚úÖ Loaded 294 records from BA_Web_Gemini_Pro_Judge_Results\n",
      "\n",
      "üéØ CALCULATING ML METRICS FOR WEB DATASET\n",
      "============================================================\n",
      "üìä Class Distribution (Web):\n",
      "   Flash Positive (Aligned): 0 (0.0%)\n",
      "   Flash Negative (Not-Aligned): 42 (14.3%)\n",
      "   Pro 1.5 Positive: 226 (76.9%)\n",
      "   Pro 1.5 Negative: 68 (23.1%)\n",
      "‚ùå Error calculating metrics for Web: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
      "\n",
      "üîÑ ANALYZING COMBINED DATASET...\n",
      "\n",
      "üéØ CALCULATING ML METRICS FOR COMBINED DATASET\n",
      "============================================================\n",
      "üìä Class Distribution (Combined):\n",
      "   Flash Positive (Aligned): 0 (0.0%)\n",
      "   Flash Negative (Not-Aligned): 154 (26.0%)\n",
      "   Pro 1.5 Positive: 364 (61.4%)\n",
      "   Pro 1.5 Negative: 229 (38.6%)\n",
      "‚ùå Error calculating metrics for Combined: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
      "\n",
      "‚úÖ Summary saved to: gemini_pro_15_omni_summary_20250725_135709.json\n",
      "\n",
      "üé® CREATING OMNI HTML DASHBOARD...\n",
      "‚úÖ Created Gemini Pro 1.5 Omni Dashboard: gemini_pro_15_omni_dashboard_20250725_135709.html\n",
      "\n",
      "üéâ GEMINI PRO 1.5 OMNI ANALYSIS COMPLETED!\n",
      "üìä Datasets analyzed: ['meta', 'web', 'combined']\n",
      "üì∞ Meta: 299 evaluations, F1: N/A\n",
      "üåê Web: 294 evaluations, F1: N/A\n",
      "üîÑ Combined: 593 evaluations, F1: N/A\n",
      "üìÑ Dashboard: gemini_pro_15_omni_dashboard_20250725_135709.html\n",
      "üìä Summary: gemini_pro_15_omni_summary_20250725_135709.json\n",
      "\n",
      "‚úÖ Analysis completed successfully!\n",
      "üåê Open the dashboard: gemini_pro_15_omni_dashboard_20250725_135709.html\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Gemini Pro 1.5 Omni Dashboard - HTML-Based Complete Analysis\n",
    "Creates comprehensive HTML dashboard with ML metrics for Gemini 1.5 Pro evaluations\n",
    "Similar to your enhanced Pro 2.5 analysis but for existing 1.5 results\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, classification_report\n",
    "import traceback\n",
    "\n",
    "# Configuration - Using your existing Gemini 1.5 Pro tables\n",
    "PROJECT_ID = \"scope3-dev\"\n",
    "DATASET_ID = \"research_bs_monitoring\"\n",
    "META_RESULTS_TABLE = \"BA_Meta_Gemini_Pro_Judge_Results\"\n",
    "WEB_RESULTS_TABLE = \"BA_Web_Gemini_Pro_Judge_Results\"\n",
    "\n",
    "class GeminiPro15OmniAnalysis:\n",
    "    \"\"\"Complete analysis class for Gemini Pro 1.5 with HTML dashboard generation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = self.initialize_bigquery_client()\n",
    "        \n",
    "    def initialize_bigquery_client(self):\n",
    "        \"\"\"Initialize BigQuery client\"\"\"\n",
    "        try:\n",
    "            client = bigquery.Client(project=PROJECT_ID)\n",
    "            print(f\"‚úÖ BigQuery client initialized for project: {PROJECT_ID}\")\n",
    "            return client\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå BigQuery initialization failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def load_gemini15_results(self, table_name: str) -> pd.DataFrame:\n",
    "        \"\"\"Load Gemini 1.5 Pro results from BigQuery\"\"\"\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            artifact_id,\n",
    "            data_source,\n",
    "            source,\n",
    "            -- Flash results (ground truth)\n",
    "            flash_classification,\n",
    "            flash_reasoning,\n",
    "            model_prompt,\n",
    "            \n",
    "            -- Pro 1.5 results (judge evaluations)\n",
    "            pro_judge_agreement,\n",
    "            pro_verdict,\n",
    "            pro_confidence,\n",
    "            pro_would_reach_same_conclusion,\n",
    "            pro_reasoning,\n",
    "            flash_vs_pro_analysis,\n",
    "            improvements,\n",
    "            api_call_time,\n",
    "            batch_number,\n",
    "            created_at,\n",
    "            model_used,\n",
    "            error_message,\n",
    "            \n",
    "            -- ML Analysis Fields\n",
    "            CASE \n",
    "                WHEN flash_classification = 1 THEN 1 \n",
    "                WHEN flash_classification = 0 THEN 0 \n",
    "                ELSE flash_classification\n",
    "            END as flash_binary,\n",
    "            \n",
    "            CASE \n",
    "                WHEN pro_verdict = 'Aligned' THEN 1 \n",
    "                WHEN pro_verdict = 'Not-Aligned' THEN 0 \n",
    "                ELSE NULL\n",
    "            END as pro_binary,\n",
    "            \n",
    "            -- Agreement analysis\n",
    "            CASE \n",
    "                WHEN (flash_classification = 1 AND pro_verdict = 'Aligned') OR\n",
    "                     (flash_classification = 0 AND pro_verdict = 'Not-Aligned') \n",
    "                THEN TRUE \n",
    "                ELSE FALSE \n",
    "            END as classification_agreement\n",
    "            \n",
    "        FROM `{PROJECT_ID}.{DATASET_ID}.{table_name}`\n",
    "        WHERE error_message IS NULL  -- Only successful evaluations\n",
    "        ORDER BY created_at DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"üì• Loading data from {table_name}...\")\n",
    "            df = self.client.query(query).to_dataframe()\n",
    "            \n",
    "            if not df.empty:\n",
    "                df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "                # Clean data for ML analysis\n",
    "                df = df.dropna(subset=['flash_binary', 'pro_binary'])\n",
    "                print(f\"‚úÖ Loaded {len(df)} records from {table_name}\")\n",
    "                return df\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No data found in {table_name}\")\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading from {table_name}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def calculate_comprehensive_metrics(self, df: pd.DataFrame, dataset_name: str) -> dict:\n",
    "        \"\"\"Calculate comprehensive ML metrics similar to your Pro 2.5 analysis\"\"\"\n",
    "        \n",
    "        if df.empty:\n",
    "            return {}\n",
    "        \n",
    "        print(f\"\\nüéØ CALCULATING ML METRICS FOR {dataset_name.upper()} DATASET\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Flash = ground truth, Pro 1.5 = predictions being evaluated\n",
    "        flash_truth = df['flash_binary'].astype(int)\n",
    "        pro_pred = df['pro_binary'].astype(int)\n",
    "        total = len(df)\n",
    "        \n",
    "        # Basic accuracy\n",
    "        accuracy = (flash_truth == pro_pred).mean()\n",
    "        \n",
    "        # Class distribution\n",
    "        flash_pos = (flash_truth == 1).sum()\n",
    "        flash_neg = (flash_truth == 0).sum()\n",
    "        pro_pos = (pro_pred == 1).sum()\n",
    "        pro_neg = (pro_pred == 0).sum()\n",
    "        \n",
    "        print(f\"üìä Class Distribution ({dataset_name}):\")\n",
    "        print(f\"   Flash Positive (Aligned): {flash_pos} ({flash_pos/total:.1%})\")\n",
    "        print(f\"   Flash Negative (Not-Aligned): {flash_neg} ({flash_neg/total:.1%})\")\n",
    "        print(f\"   Pro 1.5 Positive: {pro_pos} ({pro_pos/total:.1%})\")\n",
    "        print(f\"   Pro 1.5 Negative: {pro_neg} ({pro_neg/total:.1%})\")\n",
    "        \n",
    "        # Full ML metrics calculation\n",
    "        try:\n",
    "            # Check if we have both classes in the ground truth\n",
    "            unique_classes = np.unique(flash_truth)\n",
    "            if len(unique_classes) < 2:\n",
    "                print(f\"‚ö†Ô∏è Warning: Only one class found in Flash ground truth for {dataset_name}\")\n",
    "                print(f\"   Available classes: {unique_classes}\")\n",
    "                print(f\"   Cannot calculate full binary classification metrics\")\n",
    "                \n",
    "                # Return basic metrics only\n",
    "                return {\n",
    "                    'dataset_name': dataset_name,\n",
    "                    'total_evaluations': total,\n",
    "                    'accuracy': round(accuracy, 3),\n",
    "                    'class_distribution': {\n",
    "                        'flash_positive': {'count': int(flash_pos), 'percentage': round(flash_pos/total, 3)},\n",
    "                        'flash_negative': {'count': int(flash_neg), 'percentage': round(flash_neg/total, 3)},\n",
    "                        'pro_positive': {'count': int(pro_pos), 'percentage': round(pro_pos/total, 3)},\n",
    "                        'pro_negative': {'count': int(pro_neg), 'percentage': round(pro_neg/total, 3)}\n",
    "                    },\n",
    "                    'error': 'Single class in ground truth - limited metrics available'\n",
    "                }\n",
    "            \n",
    "            precision = precision_score(flash_truth, pro_pred, zero_division=0, average='binary')\n",
    "            recall = recall_score(flash_truth, pro_pred, zero_division=0, average='binary')\n",
    "            f1 = f1_score(flash_truth, pro_pred, zero_division=0, average='binary')\n",
    "            \n",
    "            # Confusion matrix\n",
    "            cm = confusion_matrix(flash_truth, pro_pred)\n",
    "            if cm.shape == (2, 2):\n",
    "                tn, fp, fn, tp = cm.ravel()\n",
    "            else:\n",
    "                tn, fp, fn, tp = 0, 0, 0, total\n",
    "            \n",
    "            # Additional metrics\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "            fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "            npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "            ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            \n",
    "            # Balanced accuracy\n",
    "            balanced_accuracy = (sensitivity + specificity) / 2\n",
    "            \n",
    "            # Matthews Correlation Coefficient\n",
    "            mcc_num = (tp * tn) - (fp * fn)\n",
    "            mcc_den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) if (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn) > 0 else 1\n",
    "            mcc = mcc_num / mcc_den if mcc_den > 0 else 0\n",
    "            \n",
    "            print(f\"\\nüéØ CORE METRICS ({dataset_name}):\")\n",
    "            print(f\"   Accuracy: {accuracy:.3f} ({accuracy:.1%})\")\n",
    "            print(f\"   Balanced Accuracy: {balanced_accuracy:.3f} ({balanced_accuracy:.1%})\")\n",
    "            print(f\"   F1-Score: {f1:.3f}\")\n",
    "            print(f\"   Precision: {precision:.3f}\")\n",
    "            print(f\"   Recall (Sensitivity): {recall:.3f}\")\n",
    "            print(f\"   Specificity: {specificity:.3f}\")\n",
    "            print(f\"   Matthews Correlation: {mcc:.3f}\")\n",
    "            \n",
    "            print(f\"\\nüìä CONFUSION MATRIX ({dataset_name}):\")\n",
    "            print(f\"                    Flash Truth\")\n",
    "            print(f\"                Not-Aligned  Aligned\")\n",
    "            print(f\"Pro 1.5  Not       {tn:3d}      {fn:3d}   = {tn+fn}\")\n",
    "            print(f\"         Aligned    {fp:3d}      {tp:3d}   = {fp+tp}\")\n",
    "            print(f\"                    ___      ___\")\n",
    "            print(f\"                    {tn+fp:3d}      {fn+tp:3d}   = {total}\")\n",
    "            \n",
    "            print(f\"\\nüìà ERROR ANALYSIS ({dataset_name}):\")\n",
    "            print(f\"   False Positive Rate: {fpr:.3f} (Pro 1.5 more liberal than Flash)\")\n",
    "            print(f\"   False Negative Rate: {fnr:.3f} (Pro 1.5 more conservative than Flash)\")\n",
    "            \n",
    "            # Agreement scenario analysis (matching your Pro 2.5 logic)\n",
    "            process_agreement = df['pro_judge_agreement'].mean()\n",
    "            classification_agreement = df['classification_agreement'].mean()\n",
    "            would_reach_same = df['pro_would_reach_same_conclusion'].mean() if 'pro_would_reach_same_conclusion' in df.columns else 0\n",
    "            \n",
    "            # Calculate agreement scenarios\n",
    "            scenario_counts = df.groupby(['pro_judge_agreement', 'classification_agreement']).size()\n",
    "            both_agree = scenario_counts.get((True, True), 0)\n",
    "            process_only = scenario_counts.get((True, False), 0)\n",
    "            classification_only = scenario_counts.get((False, True), 0)\n",
    "            both_disagree = scenario_counts.get((False, False), 0)\n",
    "            \n",
    "            print(f\"\\n‚öñÔ∏è PRO 1.5 AGREEMENT ANALYSIS ({dataset_name}):\")\n",
    "            print(f\"   Process Agreement: {process_agreement:.1%}\")\n",
    "            print(f\"   Classification Agreement: {classification_agreement:.1%}\")\n",
    "            print(f\"   Would Reach Same: {would_reach_same:.1%}\")\n",
    "            \n",
    "            print(f\"\\nüîç AGREEMENT SCENARIOS ({dataset_name}):\")\n",
    "            print(f\"   ‚úÖ Perfect Alignment: {both_agree} ({both_agree/total:.1%})\")\n",
    "            print(f\"   üü° Good Reasoning, Wrong Verdict: {process_only} ({process_only/total:.1%})\")\n",
    "            print(f\"   üü† Right Verdict, Poor Reasoning: {classification_only} ({classification_only/total:.1%})\")\n",
    "            print(f\"   ‚ùå Complete Disagreement: {both_disagree} ({both_disagree/total:.1%})\")\n",
    "            \n",
    "            # Performance assessment\n",
    "            if f1 >= 0.8:\n",
    "                f1_assessment = \"üü¢ EXCELLENT\"\n",
    "            elif f1 >= 0.7:\n",
    "                f1_assessment = \"üü° GOOD\"\n",
    "            elif f1 >= 0.6:\n",
    "                f1_assessment = \"üü† MODERATE\"\n",
    "            else:\n",
    "                f1_assessment = \"üî¥ POOR\"\n",
    "            \n",
    "            if accuracy >= 0.8:\n",
    "                acc_assessment = \"üü¢ EXCELLENT\"\n",
    "            elif accuracy >= 0.7:\n",
    "                acc_assessment = \"üü° GOOD\"\n",
    "            elif accuracy >= 0.6:\n",
    "                acc_assessment = \"üü† MODERATE\"\n",
    "            else:\n",
    "                acc_assessment = \"üî¥ POOR\"\n",
    "            \n",
    "            print(f\"\\nüéØ PERFORMANCE ASSESSMENT ({dataset_name}):\")\n",
    "            print(f\"   F1-Score: {f1_assessment} ({f1:.3f})\")\n",
    "            print(f\"   Accuracy: {acc_assessment} ({accuracy:.3f})\")\n",
    "            print(f\"   Primary Issue: {'Pro 1.5 more liberal than Flash' if fp > fn else 'Pro 1.5 more conservative than Flash' if fn > fp else 'Balanced disagreement'}\")\n",
    "            \n",
    "            return {\n",
    "                'dataset_name': dataset_name,\n",
    "                'total_evaluations': total,\n",
    "                'accuracy': round(accuracy, 3),\n",
    "                'balanced_accuracy': round(balanced_accuracy, 3),\n",
    "                'f1_score': round(f1, 3),\n",
    "                'precision': round(precision, 3),\n",
    "                'recall': round(recall, 3),\n",
    "                'specificity': round(specificity, 3),\n",
    "                'sensitivity': round(sensitivity, 3),\n",
    "                'false_positive_rate': round(fpr, 3),\n",
    "                'false_negative_rate': round(fnr, 3),\n",
    "                'positive_predictive_value': round(ppv, 3),\n",
    "                'negative_predictive_value': round(npv, 3),\n",
    "                'matthews_correlation_coefficient': round(mcc, 3),\n",
    "                'confusion_matrix': {\n",
    "                    'true_positives': int(tp),\n",
    "                    'false_positives': int(fp), \n",
    "                    'true_negatives': int(tn),\n",
    "                    'false_negatives': int(fn),\n",
    "                    'total': int(total)\n",
    "                },\n",
    "                'class_distribution': {\n",
    "                    'flash_positive': {'count': int(flash_pos), 'percentage': round(flash_pos/total, 3)},\n",
    "                    'flash_negative': {'count': int(flash_neg), 'percentage': round(flash_neg/total, 3)},\n",
    "                    'pro_positive': {'count': int(pro_pos), 'percentage': round(pro_pos/total, 3)},\n",
    "                    'pro_negative': {'count': int(pro_neg), 'percentage': round(pro_neg/total, 3)}\n",
    "                },\n",
    "                'pro_15_agreement': {\n",
    "                    'process_agreement_rate': round(process_agreement, 3),\n",
    "                    'classification_agreement_rate': round(classification_agreement, 3),\n",
    "                    'would_reach_same_rate': round(would_reach_same, 3)\n",
    "                },\n",
    "                'scenario_breakdown': {\n",
    "                    'both_agree': {'count': int(both_agree), 'percentage': round(both_agree/total, 3)},\n",
    "                    'process_only': {'count': int(process_only), 'percentage': round(process_only/total, 3)},\n",
    "                    'classification_only': {'count': int(classification_only), 'percentage': round(classification_only/total, 3)},\n",
    "                    'both_disagree': {'count': int(both_disagree), 'percentage': round(both_disagree/total, 3)}\n",
    "                },\n",
    "                'performance_assessment': {\n",
    "                    'f1_category': f1_assessment,\n",
    "                    'accuracy_category': acc_assessment,\n",
    "                    'primary_issue': 'Pro 1.5 more liberal than Flash' if fp > fn else 'Pro 1.5 more conservative than Flash' if fn > fp else 'Balanced disagreement',\n",
    "                    'recommendation': 'Pro 1.5 tends to be more permissive' if fp > fn else 'Pro 1.5 tends to be more strict' if fn > fp else 'Review edge cases where models disagree'\n",
    "                },\n",
    "                'confidence_analysis': {\n",
    "                    'average_confidence': round(df['pro_confidence'].mean(), 3),\n",
    "                    'high_confidence_agreement': round(df[df['pro_confidence'] > 0.8]['pro_judge_agreement'].mean() if len(df[df['pro_confidence'] > 0.8]) > 0 else 0, 3),\n",
    "                    'low_confidence_agreement': round(df[df['pro_confidence'] <= 0.7]['pro_judge_agreement'].mean() if len(df[df['pro_confidence'] <= 0.7]) > 0 else 0, 3)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error calculating metrics for {dataset_name}: {e}\")\n",
    "            return {\n",
    "                'dataset_name': dataset_name,\n",
    "                'total_evaluations': total,\n",
    "                'accuracy': round(accuracy, 3),\n",
    "                'class_distribution': {\n",
    "                    'flash_positive': {'count': int(flash_pos), 'percentage': round(flash_pos/total, 3)},\n",
    "                    'flash_negative': {'count': int(flash_neg), 'percentage': round(flash_neg/total, 3)},\n",
    "                    'pro_positive': {'count': int(pro_pos), 'percentage': round(pro_pos/total, 3)},\n",
    "                    'pro_negative': {'count': int(pro_neg), 'percentage': round(pro_neg/total, 3)}\n",
    "                },\n",
    "                'error': f'Calculation failed: {str(e)}'\n",
    "            }\n",
    "    \n",
    "    def create_omni_html_dashboard(self, meta_results: dict, web_results: dict, combined_results: dict):\n",
    "        \"\"\"Create comprehensive HTML dashboard matching your enhanced analysis style\"\"\"\n",
    "        \n",
    "        # Determine which results are available\n",
    "        datasets_available = []\n",
    "        if meta_results: datasets_available.append('Meta')\n",
    "        if web_results: datasets_available.append('Web') \n",
    "        if combined_results: datasets_available.append('Combined')\n",
    "        \n",
    "        if not datasets_available:\n",
    "            print(\"‚ùå No results available for dashboard creation\")\n",
    "            return\n",
    "        \n",
    "        html_content = f\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Gemini Pro 1.5 Omni Dashboard - Complete ML Analysis</title>\n",
    "    <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "    <style>\n",
    "        body {{ \n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; \n",
    "            margin: 0; \n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \n",
    "            min-height: 100vh; \n",
    "        }}\n",
    "        .container {{ \n",
    "            max-width: 1600px; \n",
    "            margin: 0 auto; \n",
    "            padding: 20px; \n",
    "        }}\n",
    "        .header {{ \n",
    "            background: rgba(255,255,255,0.95); \n",
    "            padding: 40px; \n",
    "            border-radius: 20px; \n",
    "            text-align: center; \n",
    "            margin-bottom: 30px; \n",
    "            backdrop-filter: blur(10px); \n",
    "            box-shadow: 0 20px 40px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "        .dataset-tabs {{\n",
    "            display: flex;\n",
    "            justify-content: center;\n",
    "            margin-bottom: 30px;\n",
    "            gap: 10px;\n",
    "        }}\n",
    "        .tab-button {{\n",
    "            padding: 15px 30px;\n",
    "            border: none;\n",
    "            border-radius: 25px;\n",
    "            font-weight: bold;\n",
    "            cursor: pointer;\n",
    "            transition: all 0.3s ease;\n",
    "            font-size: 16px;\n",
    "        }}\n",
    "        .tab-button.meta {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; }}\n",
    "        .tab-button.web {{ background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); color: white; }}\n",
    "        .tab-button.combined {{ background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); color: white; }}\n",
    "        .tab-button:hover {{ transform: translateY(-2px); box-shadow: 0 10px 20px rgba(0,0,0,0.2); }}\n",
    "        .dataset-content {{ display: none; }}\n",
    "        .dataset-content.active {{ display: block; }}\n",
    "        .metrics-grid {{ \n",
    "            display: grid; \n",
    "            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); \n",
    "            gap: 20px; \n",
    "            margin: 20px 0; \n",
    "        }}\n",
    "        .metric-card {{ \n",
    "            background: rgba(255,255,255,0.95); \n",
    "            padding: 25px; \n",
    "            border-radius: 15px; \n",
    "            box-shadow: 0 10px 30px rgba(0,0,0,0.15); \n",
    "            text-align: center; \n",
    "            backdrop-filter: blur(10px); \n",
    "            transition: transform 0.3s ease;\n",
    "        }}\n",
    "        .metric-card:hover {{ transform: translateY(-5px); }}\n",
    "        .metric-value {{ \n",
    "            font-size: 2.8em; \n",
    "            font-weight: bold; \n",
    "            margin: 15px 0; \n",
    "        }}\n",
    "        .metric-label {{ \n",
    "            color: #666; \n",
    "            font-weight: 600; \n",
    "            font-size: 1.1em;\n",
    "        }}\n",
    "        .excellent {{ color: #10B981; }}\n",
    "        .good {{ color: #F59E0B; }}\n",
    "        .moderate {{ color: #FF6B35; }}\n",
    "        .poor {{ color: #EF4444; }}\n",
    "        .section {{ \n",
    "            background: rgba(255,255,255,0.95); \n",
    "            padding: 30px; \n",
    "            border-radius: 20px; \n",
    "            margin: 20px 0; \n",
    "            backdrop-filter: blur(10px); \n",
    "            box-shadow: 0 15px 35px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "        .two-column {{ \n",
    "            display: grid; \n",
    "            grid-template-columns: 1fr 1fr; \n",
    "            gap: 30px; \n",
    "        }}\n",
    "        .three-column {{ \n",
    "            display: grid; \n",
    "            grid-template-columns: repeat(3, 1fr); \n",
    "            gap: 20px; \n",
    "        }}\n",
    "        .confusion-matrix {{ \n",
    "            display: grid; \n",
    "            grid-template-columns: repeat(3, 1fr); \n",
    "            gap: 10px; \n",
    "            margin: 20px 0; \n",
    "            text-align: center; \n",
    "        }}\n",
    "        .cm-cell {{ \n",
    "            padding: 20px; \n",
    "            border-radius: 12px; \n",
    "            font-weight: bold; \n",
    "            transition: transform 0.2s ease;\n",
    "        }}\n",
    "        .cm-cell:hover {{ transform: scale(1.05); }}\n",
    "        .cm-header {{ background: linear-gradient(135deg, #f8f9fa, #e9ecef); color: #333; }}\n",
    "        .cm-tp {{ background: linear-gradient(135deg, #d4edda, #c3e6cb); color: #155724; }}\n",
    "        .cm-fp {{ background: linear-gradient(135deg, #f8d7da, #f1b0b7); color: #721c24; }}\n",
    "        .cm-fn {{ background: linear-gradient(135deg, #fff3cd, #fce4a6); color: #856404; }}\n",
    "        .cm-tn {{ background: linear-gradient(135deg, #d1ecf1, #bee5eb); color: #0c5460; }}\n",
    "        .breakdown-grid {{ \n",
    "            display: grid; \n",
    "            grid-template-columns: repeat(2, 1fr); \n",
    "            gap: 20px; \n",
    "        }}\n",
    "        .breakdown-item {{ \n",
    "            padding: 25px; \n",
    "            border-radius: 15px; \n",
    "            text-align: center; \n",
    "            transition: transform 0.3s ease;\n",
    "        }}\n",
    "        .breakdown-item:hover {{ transform: translateY(-3px); }}\n",
    "        .both-agree {{ background: linear-gradient(135deg, #d4edda, #c3e6cb); }}\n",
    "        .process-only {{ background: linear-gradient(135deg, #fff3cd, #fce4a6); }}\n",
    "        .classification-only {{ background: linear-gradient(135deg, #e2e3e5, #d1d2d4); }}\n",
    "        .both-disagree {{ background: linear-gradient(135deg, #f8d7da, #f1b0b7); }}\n",
    "        .insight-box {{ \n",
    "            background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%); \n",
    "            padding: 30px; \n",
    "            border-radius: 20px; \n",
    "            margin: 25px 0; \n",
    "            border: 3px solid rgba(255,255,255,0.4);\n",
    "            box-shadow: 0 15px 35px rgba(0,0,0,0.1);\n",
    "        }}\n",
    "        .dataset-badge {{ \n",
    "            display: inline-block; \n",
    "            padding: 8px 18px; \n",
    "            border-radius: 25px; \n",
    "            font-weight: bold; \n",
    "            margin: 8px; \n",
    "            color: white; \n",
    "            font-size: 0.95em;\n",
    "            box-shadow: 0 4px 15px rgba(0,0,0,0.2);\n",
    "        }}\n",
    "        .comparison-section {{\n",
    "            background: linear-gradient(135deg, #f1f3f4 0%, #e8eaed 100%);\n",
    "            padding: 30px;\n",
    "            border-radius: 20px;\n",
    "            margin: 25px 0;\n",
    "            border: 2px solid rgba(255,255,255,0.6);\n",
    "        }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <div class=\"header\">\n",
    "            <h1>üß† Gemini Pro 1.5 Omni Dashboard</h1>\n",
    "            <h2>Complete ML Analysis Suite - Flash vs Pro 1.5 Judge Evaluation</h2>\n",
    "            <p>Comprehensive analysis of Pro 1.5's performance in judging Flash classifications</p>\n",
    "            <p><strong>Generated:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | <strong>Datasets Available:</strong> {', '.join(datasets_available)}</p>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"dataset-tabs\">\"\"\"\n",
    "        \n",
    "        # Add tab buttons for available datasets\n",
    "        for dataset in datasets_available:\n",
    "            tab_class = dataset.lower()\n",
    "            html_content += f'<button class=\"tab-button {tab_class}\" onclick=\"showDataset(\\'{dataset.lower()}\\')\">{dataset} Dataset</button>'\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "        </div>\"\"\"\n",
    "        \n",
    "            # Generate content for each available dataset\n",
    "        for results, dataset_key in [(meta_results, 'meta'), (web_results, 'web'), (combined_results, 'combined')]:\n",
    "            if not results or 'total_evaluations' not in results:\n",
    "                continue\n",
    "                \n",
    "            dataset_name = results['dataset_name']\n",
    "            cm = results.get('confusion_matrix', {})\n",
    "            \n",
    "            # Handle cases where full ML metrics aren't available\n",
    "            if 'error' in results:\n",
    "                # Generate simplified dashboard for single-class data\n",
    "                html_content += f\"\"\"\n",
    "        \n",
    "        <div id=\"{dataset_key}\" class=\"dataset-content\">\n",
    "            <div class=\"section\">\n",
    "                <h2>‚ö†Ô∏è Limited Analysis - {dataset_name} Dataset</h2>\n",
    "                <p><em>Single class detected in ground truth data - limited metrics available</em></p>\n",
    "                <div class=\"insight-box\">\n",
    "                    <h3>üîç Data Analysis Issue</h3>\n",
    "                    <p><strong>Problem:</strong> {results.get('error', 'Unknown error')}</p>\n",
    "                    <p><strong>Flash Positive Cases:</strong> {results.get('class_distribution', {}).get('flash_positive', {}).get('count', 0)}</p>\n",
    "                    <p><strong>Flash Negative Cases:</strong> {results.get('class_distribution', {}).get('flash_negative', {}).get('count', 0)}</p>\n",
    "                    <p><strong>Total Evaluations:</strong> {results.get('total_evaluations', 0):,}</p>\n",
    "                    <p><strong>Basic Agreement Rate:</strong> {results.get('accuracy', 0):.1%}</p>\n",
    "                    <p><strong>Recommendation:</strong> This dataset appears to have only one class in the Flash ground truth, making binary classification analysis impossible. Please check your data source.</p>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\"\"\"\n",
    "                continue\n",
    "            \n",
    "            # Full dashboard for complete data\n",
    "            ml = results\n",
    "            html_content += f\"\"\"\n",
    "        \n",
    "        <div id=\"{dataset_key}\" class=\"dataset-content\">\n",
    "            <div class=\"section\">\n",
    "                <h2>üéØ Core ML Performance Metrics - {dataset_name} Dataset</h2>\n",
    "                <p><em>Pro 1.5 performance in agreeing with Flash classifications (Flash = Ground Truth)</em></p>\n",
    "                <div class=\"metrics-grid\">\n",
    "                    <div class=\"metric-card\">\n",
    "                        <div class=\"metric-value {'excellent' if ml.get('f1_score', 0) >= 0.8 else 'good' if ml.get('f1_score', 0) >= 0.7 else 'moderate' if ml.get('f1_score', 0) >= 0.6 else 'poor'}\">{ml.get('f1_score', 0):.3f}</div>\n",
    "                        <div class=\"metric-label\">F1-Score</div>\n",
    "                        <small>Harmonic Mean of Precision & Recall</small>\n",
    "                    </div>\n",
    "                    <div class=\"metric-card\">\n",
    "                        <div class=\"metric-value {'excellent' if ml.get('accuracy', 0) >= 0.8 else 'good' if ml.get('accuracy', 0) >= 0.7 else 'moderate' if ml.get('accuracy', 0) >= 0.6 else 'poor'}\">{ml.get('accuracy', 0):.3f}</div>\n",
    "                        <div class=\"metric-label\">Agreement Rate</div>\n",
    "                        <small>Pro 1.5 vs Flash Overall</small>\n",
    "                    </div>\n",
    "                    <div class=\"metric-card\">\n",
    "                        <div class=\"metric-value {'excellent' if ml.get('balanced_accuracy', 0) >= 0.8 else 'good' if ml.get('balanced_accuracy', 0) >= 0.7 else 'moderate' if ml.get('balanced_accuracy', 0) >= 0.6 else 'poor'}\">{ml.get('balanced_accuracy', 0):.3f}</div>\n",
    "                        <div class=\"metric-label\">Balanced Accuracy</div>\n",
    "                        <small>Handles Class Imbalance</small>\n",
    "                    </div>\n",
    "                    <div class=\"metric-card\">\n",
    "                        <div class=\"metric-value {'excellent' if ml.get('precision', 0) >= 0.8 else 'good' if ml.get('precision', 0) >= 0.7 else 'moderate' if ml.get('precision', 0) >= 0.6 else 'poor'}\">{ml.get('precision', 0):.3f}</div>\n",
    "                        <div class=\"metric-label\">Precision</div>\n",
    "                        <small>TP / (TP + FP)</small>\n",
    "                    </div>\n",
    "                    <div class=\"metric-card\">\n",
    "                        <div class=\"metric-value {'excellent' if ml.get('recall', 0) >= 0.8 else 'good' if ml.get('recall', 0) >= 0.7 else 'moderate' if ml.get('recall', 0) >= 0.6 else 'poor'}\">{ml.get('recall', 0):.3f}</div>\n",
    "                        <div class=\"metric-label\">Recall (TPR)</div>\n",
    "                        <small>TP / (TP + FN)</small>\n",
    "                    </div>\n",
    "                    <div class=\"metric-card\">\n",
    "                        <div class=\"metric-value {'excellent' if ml.get('confidence_analysis', {}).get('average_confidence', 0) >= 0.9 else 'good' if ml.get('confidence_analysis', {}).get('average_confidence', 0) >= 0.8 else 'moderate' if ml.get('confidence_analysis', {}).get('average_confidence', 0) >= 0.7 else 'poor'}\">{ml.get('confidence_analysis', {}).get('average_confidence', 0):.3f}</div>\n",
    "                        <div class=\"metric-label\">Avg Confidence</div>\n",
    "                        <small>Pro 1.5 Confidence Score</small>\n",
    "                    </div>\n",
    "                    <div class=\"metric-card\">\n",
    "                        <div class=\"metric-value {'excellent' if ml.get('false_positive_rate', 1) <= 0.1 else 'good' if ml.get('false_positive_rate', 1) <= 0.2 else 'moderate' if ml.get('false_positive_rate', 1) <= 0.3 else 'poor'}\">{ml.get('false_positive_rate', 0):.3f}</div>\n",
    "                        <div class=\"metric-label\">False Positive Rate</div>\n",
    "                        <small>Pro 1.5 More Liberal</small>\n",
    "                    </div>\n",
    "                    <div class=\"metric-card\">\n",
    "                        <div class=\"metric-value {'excellent' if ml.get('false_negative_rate', 1) <= 0.1 else 'good' if ml.get('false_negative_rate', 1) <= 0.2 else 'moderate' if ml.get('false_negative_rate', 1) <= 0.3 else 'poor'}\">{ml.get('false_negative_rate', 0):.3f}</div>\n",
    "                        <div class=\"metric-label\">False Negative Rate</div>\n",
    "                        <small>Pro 1.5 More Conservative</small>\n",
    "                    </div>\n",
    "                </div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"two-column\">\n",
    "                <div class=\"section\">\n",
    "                    <h2>üìä Confusion Matrix - {dataset_name}</h2>\n",
    "                    <p><em>Flash (Ground Truth) vs Pro 1.5 (Judge Predictions)</em></p>\n",
    "                    <div class=\"confusion-matrix\">\n",
    "                        <div class=\"cm-cell cm-header\"></div>\n",
    "                        <div class=\"cm-cell cm-header\">Flash: Not-Aligned</div>\n",
    "                        <div class=\"cm-cell cm-header\">Flash: Aligned</div>\n",
    "                        \n",
    "                        <div class=\"cm-cell cm-header\">Pro: Not-Aligned</div>\n",
    "                        <div class=\"cm-cell cm-tn\">\n",
    "                            <div style=\"font-size: 1.8em;\">{cm.get('true_negatives', 0)}</div>\n",
    "                            <small>True Negatives<br/>Both say Not-Aligned</small>\n",
    "                        </div>\n",
    "                        <div class=\"cm-cell cm-fn\">\n",
    "                            <div style=\"font-size: 1.8em;\">{cm.get('false_negatives', 0)}</div>\n",
    "                            <small>False Negatives<br/>Flash=Aligned, Pro=Not</small>\n",
    "                        </div>\n",
    "                        \n",
    "                        <div class=\"cm-cell cm-header\">Pro: Aligned</div>\n",
    "                        <div class=\"cm-cell cm-fp\">\n",
    "                            <div style=\"font-size: 1.8em;\">{cm.get('false_positives', 0)}</div>\n",
    "                            <small>False Positives<br/>Flash=Not, Pro=Aligned</small>\n",
    "                        </div>\n",
    "                        <div class=\"cm-cell cm-tp\">\n",
    "                            <div style=\"font-size: 1.8em;\">{cm.get('true_positives', 0)}</div>\n",
    "                            <small>True Positives<br/>Both say Aligned</small>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                    \n",
    "                    <div style=\"margin-top: 25px;\">\n",
    "                        <h4>üìà Matrix Interpretation:</h4>\n",
    "                        <p><strong>‚úÖ Pro 1.5 Agreement:</strong> {cm.get('true_positives', 0) + cm.get('true_negatives', 0)} ({((cm.get('true_positives', 0) + cm.get('true_negatives', 0)) / cm.get('total', 1)):.1%})</p>\n",
    "                        <p><strong>‚ùå Pro 1.5 Disagreement:</strong> {cm.get('false_positives', 0) + cm.get('false_negatives', 0)} ({((cm.get('false_positives', 0) + cm.get('false_negatives', 0)) / cm.get('total', 1)):.1%})</p>\n",
    "                        <p><strong>‚ö†Ô∏è Error Pattern:</strong> {ml.get('performance_assessment', {}).get('primary_issue', 'Unknown')}</p>\n",
    "                    </div>\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"section\">\n",
    "                    <h2>‚öñÔ∏è Pro 1.5 Agreement Analysis - {dataset_name}</h2>\n",
    "                    <div class=\"breakdown-grid\">\n",
    "                        <div class=\"breakdown-item both-agree\">\n",
    "                            <h3>‚úÖ Perfect Alignment</h3>\n",
    "                            <div style=\"font-size: 2.2em; font-weight: bold; margin: 15px 0;\">{ml.get('scenario_breakdown', {}).get('both_agree', {}).get('count', 0)}</div>\n",
    "                            <p>{ml.get('scenario_breakdown', {}).get('both_agree', {}).get('percentage', 0):.1%} of cases</p>\n",
    "                            <small>Process + Classification Agreement</small>\n",
    "                        </div>\n",
    "                        <div class=\"breakdown-item process-only\">\n",
    "                            <h3>üü° Good Reasoning, Wrong Verdict</h3>\n",
    "                            <div style=\"font-size: 2.2em; font-weight: bold; margin: 15px 0;\">{ml.get('scenario_breakdown', {}).get('process_only', {}).get('count', 0)}</div>\n",
    "                            <p>{ml.get('scenario_breakdown', {}).get('process_only', {}).get('percentage', 0):.1%} of cases</p>\n",
    "                            <small>Pro likes reasoning but disagrees with classification</small>\n",
    "                        </div>\n",
    "                        <div class=\"breakdown-item classification-only\">\n",
    "                            <h3>üü† Right Verdict, Poor Reasoning</h3>\n",
    "                            <div style=\"font-size: 2.2em; font-weight: bold; margin: 15px 0;\">{ml.get('scenario_breakdown', {}).get('classification_only', {}).get('count', 0)}</div>\n",
    "                            <p>{ml.get('scenario_breakdown', {}).get('classification_only', {}).get('percentage', 0):.1%} of cases</p>\n",
    "                            <small>Right classification but poor reasoning process</small>\n",
    "                        </div>\n",
    "                        <div class=\"breakdown-item both-disagree\">\n",
    "                            <h3>‚ùå Complete Disagreement</h3>\n",
    "                            <div style=\"font-size: 2.2em; font-weight: bold; margin: 15px 0;\">{ml.get('scenario_breakdown', {}).get('both_disagree', {}).get('count', 0)}</div>\n",
    "                            <p>{ml.get('scenario_breakdown', {}).get('both_disagree', {}).get('percentage', 0):.1%} of cases</p>\n",
    "                            <small>Pro 1.5 disagrees with both reasoning and verdict</small>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                    \n",
    "                    <div style=\"margin-top: 25px;\">\n",
    "                        <h4>Agreement Rate Summary:</h4>\n",
    "                        <p><strong>Process Agreement:</strong> {ml.get('pro_15_agreement', {}).get('process_agreement_rate', 0):.1%}</p>\n",
    "                        <p><strong>Classification Agreement:</strong> {ml.get('pro_15_agreement', {}).get('classification_agreement_rate', 0):.1%}</p>\n",
    "                        <p><strong>Would Reach Same Conclusion:</strong> {ml.get('pro_15_agreement', {}).get('would_reach_same_rate', 0):.1%}</p>\n",
    "                    </div>\n",
    "                </div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"insight-box\">\n",
    "                <h2>üí° Performance Assessment & Recommendations - {dataset_name}</h2>\n",
    "                <div class=\"three-column\">\n",
    "                    <div>\n",
    "                        <h3>üéØ Overall Performance</h3>\n",
    "                        <p><strong>F1-Score:</strong> {ml.get('f1_score', 0):.3f} ({ml.get('performance_assessment', {}).get('f1_category', 'N/A')})</p>\n",
    "                        <p><strong>Pro 1.5 Agreement:</strong> {ml.get('accuracy', 0):.1%}</p>\n",
    "                        <p><strong>Primary Issue:</strong> {ml.get('performance_assessment', {}).get('primary_issue', 'N/A')}</p>\n",
    "                    </div>\n",
    "                    <div>\n",
    "                        <h3>üéØ Key Recommendation</h3>\n",
    "                        <p><strong>Pattern:</strong> {ml.get('performance_assessment', {}).get('recommendation', 'N/A')}</p>\n",
    "                        <p><strong>Focus:</strong> {ml.get('scenario_breakdown', {}).get('process_only', {}).get('count', 0)} \"Good Reasoning\" cases represent improvement opportunities</p>\n",
    "                        <p><strong>Confidence:</strong> {\"High confidence in judgments\" if ml.get('confidence_analysis', {}).get('average_confidence', 0) > 0.8 else \"Moderate confidence in judgments\"}</p>\n",
    "                    </div>\n",
    "                    <div>\n",
    "                        <h3>üìà Success Metrics</h3>\n",
    "                        <p><strong>Current F1:</strong> {ml.get('f1_score', 0):.3f}</p>\n",
    "                        <p><strong>Current Agreement:</strong> {ml.get('accuracy', 0):.1%}</p>\n",
    "                        <p><strong>Perfect Alignment:</strong> {ml.get('scenario_breakdown', {}).get('both_agree', {}).get('percentage', 0):.1%}</p>\n",
    "                    </div>\n",
    "                </div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"section\">\n",
    "                <h2>üìã Technical Summary - {dataset_name} Dataset</h2>\n",
    "                <div style=\"display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 25px;\">\n",
    "                    <div>\n",
    "                        <h4>üéØ Classification Performance</h4>\n",
    "                        <p><strong>F1-Score:</strong> {ml.get('f1_score', 0):.3f}</p>\n",
    "                        <p><strong>Precision:</strong> {ml.get('precision', 0):.3f}</p>\n",
    "                        <p><strong>Recall:</strong> {ml.get('recall', 0):.3f}</p>\n",
    "                        <p><strong>Accuracy:</strong> {ml.get('accuracy', 0):.3f}</p>\n",
    "                        <p><strong>Balanced Accuracy:</strong> {ml.get('balanced_accuracy', 0):.3f}</p>\n",
    "                    </div>\n",
    "                    <div>\n",
    "                        <h4>‚öñÔ∏è Pro 1.5 Analysis</h4>\n",
    "                        <p><strong>Process Agreement:</strong> {ml.get('pro_15_agreement', {}).get('process_agreement_rate', 0):.1%}</p>\n",
    "                        <p><strong>Classification Agreement:</strong> {ml.get('pro_15_agreement', {}).get('classification_agreement_rate', 0):.1%}</p>\n",
    "                        <p><strong>Average Confidence:</strong> {ml.get('confidence_analysis', {}).get('average_confidence', 0):.3f}</p>\n",
    "                        <p><strong>High Conf Agreement:</strong> {ml.get('confidence_analysis', {}).get('high_confidence_agreement', 0):.1%}</p>\n",
    "                    </div>\n",
    "                    <div>\n",
    "                        <h4>üìä Error Analysis</h4>\n",
    "                        <p><strong>False Positive Rate:</strong> {ml.get('false_positive_rate', 0):.3f}</p>\n",
    "                        <p><strong>False Negative Rate:</strong> {ml.get('false_negative_rate', 0):.3f}</p>\n",
    "                        <p><strong>Type I Errors:</strong> {cm.get('false_positives', 0)} (Pro too liberal)</p>\n",
    "                        <p><strong>Type II Errors:</strong> {cm.get('false_negatives', 0)} (Pro too conservative)</p>\n",
    "                    </div>\n",
    "                    <div>\n",
    "                        <h4>üìà Data Quality</h4>\n",
    "                        <p><strong>Total Evaluations:</strong> {ml.get('total_evaluations', 0):,}</p>\n",
    "                        <p><strong>Flash Positive Rate:</strong> {ml.get('class_distribution', {}).get('flash_positive', {}).get('percentage', 0):.1%}</p>\n",
    "                        <p><strong>Pro Positive Rate:</strong> {ml.get('class_distribution', {}).get('pro_positive', {}).get('percentage', 0):.1%}</p>\n",
    "                        <p><strong>Matthews Correlation:</strong> {ml.get('matthews_correlation_coefficient', 0):.3f}</p>\n",
    "                    </div>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\"\"\"\n",
    "        \n",
    "        # Add dataset comparison if multiple datasets available\n",
    "        if len(datasets_available) > 1:\n",
    "            html_content += f\"\"\"\n",
    "        \n",
    "        <div class=\"comparison-section\">\n",
    "            <h2>üîÑ Dataset Comparison Summary</h2>\n",
    "            <div class=\"three-column\">\"\"\"\n",
    "            \n",
    "            for results in [meta_results, web_results, combined_results]:\n",
    "                if results:\n",
    "                    dataset_name = results['dataset_name']\n",
    "                    html_content += f\"\"\"\n",
    "                <div>\n",
    "                    <h3>{dataset_name} Dataset</h3>\n",
    "                    <p><strong>Total Evaluations:</strong> {results.get('total_evaluations', 0):,}</p>\n",
    "                    <p><strong>F1-Score:</strong> {results.get('f1_score', 0):.3f}</p>\n",
    "                    <p><strong>Agreement Rate:</strong> {results.get('accuracy', 0):.1%}</p>\n",
    "                    <p><strong>Perfect Alignment:</strong> {results.get('scenario_breakdown', {}).get('both_agree', {}).get('percentage', 0):.1%}</p>\n",
    "                    <p><strong>Primary Issue:</strong> {results.get('performance_assessment', {}).get('primary_issue', 'N/A')}</p>\n",
    "                </div>\"\"\"\n",
    "            \n",
    "            html_content += \"\"\"\n",
    "            </div>\n",
    "        </div>\"\"\"\n",
    "        \n",
    "        html_content += f\"\"\"\n",
    "    </div>\n",
    "    \n",
    "    <script>\n",
    "        function showDataset(dataset) {{\n",
    "            // Hide all dataset content\n",
    "            const contents = document.querySelectorAll('.dataset-content');\n",
    "            contents.forEach(content => content.classList.remove('active'));\n",
    "            \n",
    "            // Show selected dataset\n",
    "            document.getElementById(dataset).classList.add('active');\n",
    "            \n",
    "            // Update button states\n",
    "            const buttons = document.querySelectorAll('.tab-button');\n",
    "            buttons.forEach(button => button.style.opacity = '0.7');\n",
    "            event.target.style.opacity = '1';\n",
    "        }}\n",
    "        \n",
    "        // Show first available dataset by default\n",
    "        document.addEventListener('DOMContentLoaded', function() {{\n",
    "            const firstDataset = '{datasets_available[0].lower()}';\n",
    "            showDataset(firstDataset);\n",
    "            \n",
    "            // Highlight first button\n",
    "            const firstButton = document.querySelector('.tab-button.{datasets_available[0].lower()}');\n",
    "            if (firstButton) firstButton.style.opacity = '1';\n",
    "        }});\n",
    "        \n",
    "        console.log('Gemini Pro 1.5 Omni Dashboard Loaded - Datasets: {\", \".join(datasets_available)}');\n",
    "    </script>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "        \n",
    "        # Save the dashboard\n",
    "        filename = f\"gemini_pro_15_omni_dashboard_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html\"\n",
    "        with open(filename, \"w\", encoding='utf-8') as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        print(f\"‚úÖ Created Gemini Pro 1.5 Omni Dashboard: {filename}\")\n",
    "        return filename\n",
    "    \n",
    "    def run_complete_analysis(self):\n",
    "        \"\"\"Run complete analysis on both datasets and generate omni dashboard\"\"\"\n",
    "        \n",
    "        print(\"üöÄ STARTING GEMINI PRO 1.5 OMNI ANALYSIS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Analyze Meta dataset\n",
    "        print(\"\\nüì∞ ANALYZING META DATASET...\")\n",
    "        meta_df = self.load_gemini15_results(META_RESULTS_TABLE)\n",
    "        if not meta_df.empty:\n",
    "            results['meta'] = self.calculate_comprehensive_metrics(meta_df, \"Meta\")\n",
    "        else:\n",
    "            results['meta'] = None\n",
    "            print(\"‚ö†Ô∏è No Meta dataset results available\")\n",
    "        \n",
    "        # Analyze Web dataset  \n",
    "        print(\"\\nüåê ANALYZING WEB DATASET...\")\n",
    "        web_df = self.load_gemini15_results(WEB_RESULTS_TABLE)\n",
    "        if not web_df.empty:\n",
    "            results['web'] = self.calculate_comprehensive_metrics(web_df, \"Web\")\n",
    "        else:\n",
    "            results['web'] = None\n",
    "            print(\"‚ö†Ô∏è No Web dataset results available\")\n",
    "        \n",
    "        # Analyze Combined dataset\n",
    "        print(\"\\nüîÑ ANALYZING COMBINED DATASET...\")\n",
    "        if not meta_df.empty and not web_df.empty:\n",
    "            combined_df = pd.concat([meta_df, web_df], ignore_index=True)\n",
    "            results['combined'] = self.calculate_comprehensive_metrics(combined_df, \"Combined\")\n",
    "        elif not meta_df.empty:\n",
    "            results['combined'] = self.calculate_comprehensive_metrics(meta_df, \"Combined (Meta Only)\")\n",
    "        elif not web_df.empty:\n",
    "            results['combined'] = self.calculate_comprehensive_metrics(web_df, \"Combined (Web Only)\")\n",
    "        else:\n",
    "            results['combined'] = None\n",
    "            print(\"‚ö†Ô∏è No data available for combined analysis\")\n",
    "        \n",
    "        # Create summary JSON\n",
    "        summary = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'analysis_type': 'gemini_pro_15_omni_analysis',\n",
    "            'datasets_analyzed': [k for k, v in results.items() if v is not None],\n",
    "            'results': results\n",
    "        }\n",
    "        \n",
    "        # Save summary\n",
    "        summary_filename = f\"gemini_pro_15_omni_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        with open(summary_filename, 'w') as f:\n",
    "            json.dump(summary, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Summary saved to: {summary_filename}\")\n",
    "        \n",
    "        # Create HTML dashboard\n",
    "        print(f\"\\nüé® CREATING OMNI HTML DASHBOARD...\")\n",
    "        dashboard_filename = self.create_omni_html_dashboard(\n",
    "            results['meta'], \n",
    "            results['web'], \n",
    "            results['combined']\n",
    "        )\n",
    "        \n",
    "        # Final summary\n",
    "        print(f\"\\nüéâ GEMINI PRO 1.5 OMNI ANALYSIS COMPLETED!\")\n",
    "        print(f\"üìä Datasets analyzed: {[k for k, v in results.items() if v is not None]}\")\n",
    "        if results['meta'] and 'total_evaluations' in results['meta']:\n",
    "            f1_score = results['meta'].get('f1_score', 'N/A')\n",
    "            print(f\"üì∞ Meta: {results['meta']['total_evaluations']:,} evaluations, F1: {f1_score}\")\n",
    "        if results['web'] and 'total_evaluations' in results['web']:\n",
    "            f1_score = results['web'].get('f1_score', 'N/A')\n",
    "            print(f\"üåê Web: {results['web']['total_evaluations']:,} evaluations, F1: {f1_score}\")\n",
    "        if results['combined'] and 'total_evaluations' in results['combined']:\n",
    "            f1_score = results['combined'].get('f1_score', 'N/A')\n",
    "            print(f\"üîÑ Combined: {results['combined']['total_evaluations']:,} evaluations, F1: {f1_score}\")\n",
    "        \n",
    "        print(f\"üìÑ Dashboard: {dashboard_filename}\")\n",
    "        print(f\"üìä Summary: {summary_filename}\")\n",
    "        \n",
    "        return results, dashboard_filename\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    try:\n",
    "        analyzer = GeminiPro15OmniAnalysis()\n",
    "        results, dashboard_file = analyzer.run_complete_analysis()\n",
    "        \n",
    "        print(f\"\\n‚úÖ Analysis completed successfully!\")\n",
    "        print(f\"üåê Open the dashboard: {dashboard_file}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Analysis failed: {type(e).__name__}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2b2f8835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (6.2.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from plotly) (1.45.0)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from plotly) (23.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0cc99e17",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected 'except' or 'finally' block (475267135.py, line 105)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 105\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(f\"\\nüîç DEBUG - Sample comparisons (first 5 records):\")\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expected 'except' or 'finally' block\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Gemini 2.5 Pro Classification Metrics Dashboard - Real Results vs Ground Truth\n",
    "Calculates traditional ML metrics (F1, Precision, Recall, Confusion Matrix) for 2.5 Pro decisions\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "PROJECT_ID = \"scope3-dev\"\n",
    "DATASET_ID = \"research_bs_monitoring\"\n",
    "META_TABLE = \"BA_Meta_Ground_Truth\"\n",
    "RESULTS_TABLE = \"BA_Meta_Gemini_25_Pro_Judge_Results\"\n",
    "\n",
    "class Gemini25ProClassificationDashboard:\n",
    "    def __init__(self):\n",
    "        self.client = bigquery.Client(project=PROJECT_ID)\n",
    "        \n",
    "    def load_results_with_ground_truth(self) -> pd.DataFrame:\n",
    "        \"\"\"Load 2.5 Pro results joined with Meta ground truth\"\"\"\n",
    "        print(\"üì• Loading Gemini 2.5 Pro results with ground truth...\")\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            r.artifact_id,\n",
    "            r.flash_classification,\n",
    "            r.flash_reasoning,\n",
    "            r.pro_judge_agreement,\n",
    "            r.pro_verdict,\n",
    "            r.pro_confidence,\n",
    "            r.pro_would_reach_same_conclusion,\n",
    "            r.pro_reasoning,\n",
    "            r.api_call_time,\n",
    "            r.created_at,\n",
    "            r.error_message,\n",
    "            -- Ground truth from Meta\n",
    "            m.correct_classification as ground_truth,\n",
    "            m.correct_reasoning as ground_truth_reasoning,\n",
    "            m.source as data_source\n",
    "        FROM `{PROJECT_ID}.{DATASET_ID}.{RESULTS_TABLE}` r\n",
    "        JOIN `{PROJECT_ID}.{DATASET_ID}.{META_TABLE}` m\n",
    "        ON r.artifact_id = m.artifact_id\n",
    "        WHERE (r.error_message IS NULL OR r.error_message = '')\n",
    "        AND r.pro_judge_agreement IS NOT NULL\n",
    "        AND r.pro_would_reach_same_conclusion IS NOT NULL\n",
    "        ORDER BY r.created_at DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            df = self.client.query(query).to_dataframe()\n",
    "            print(f\"‚úÖ Loaded {len(df)} records with ground truth\")\n",
    "            \n",
    "            if len(df) == 0:\n",
    "                print(\"‚ùå No valid joined data found\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Convert ground truth to binary (100 = Aligned = 1, 0 = Not-Aligned = 0)\n",
    "            df['ground_truth_binary'] = (df['ground_truth'] == 100).astype(int)\n",
    "            \n",
    "            # Convert Flash decisions to binary (100 = Aligned = 1, 0 = Not-Aligned = 0)\n",
    "            # Flash should have same encoding as ground truth: 100 for aligned, 0 for not-aligned\n",
    "            df['flash_binary'] = (df['flash_classification'] == 100).astype(int)\n",
    "            \n",
    "            # Convert 2.5 Pro \"would reach same conclusion\" to predictions\n",
    "            # If 2.5 Pro would reach same conclusion as Flash, use Flash's decision\n",
    "            # If not, use opposite of Flash's decision\n",
    "            df['pro_25_prediction'] = np.where(\n",
    "                df['pro_would_reach_same_conclusion'] == True,\n",
    "                df['flash_binary'],  # Same as Flash\n",
    "                1 - df['flash_binary']  # Opposite of Flash\n",
    "            )\n",
    "            \n",
    "            # Clean up any invalid values in predictions\n",
    "            df['pro_25_prediction'] = df['pro_25_prediction'].astype(int)\n",
    "            df['flash_binary'] = df['flash_binary'].astype(int)\n",
    "            df['ground_truth_binary'] = df['ground_truth_binary'].astype(int)\n",
    "            \n",
    "            print(f\"üìä Data distribution:\")\n",
    "            print(f\"   Ground truth: {df['ground_truth_binary'].value_counts().to_dict()}\")\n",
    "            print(f\"   Flash predictions: {df['flash_binary'].value_counts().to_dict()}\")\n",
    "            print(f\"   2.5 Pro predictions: {df['pro_25_prediction'].value_counts().to_dict()}\")\n",
    "            print(f\"   Agreement rate: {df['pro_judge_agreement'].mean():.1%}\")\n",
    "            \n",
    "            # Validate that all predictions are binary\n",
    "            if not all(df['ground_truth_binary'].isin([0, 1])):\n",
    "                print(\"‚ö†Ô∏è Warning: Ground truth contains non-binary values\")\n",
    "                df = df[df['ground_truth_binary'].isin([0, 1])]\n",
    "            \n",
    "            if not all(df['flash_binary'].isin([0, 1])):\n",
    "                print(\"‚ö†Ô∏è Warning: Flash predictions contain non-binary values\")\n",
    "                df = df[df['flash_binary'].isin([0, 1])]\n",
    "                \n",
    "            if not all(df['pro_25_prediction'].isin([0, 1])):\n",
    "                print(\"‚ö†Ô∏è Warning: 2.5 Pro predictions contain non-binary values\")\n",
    "                df = df[df['pro_25_prediction'].isin([0, 1])]\n",
    "            \n",
    "            print(f\"üìä Final clean dataset: {len(df)} records\")\n",
    "        \n",
    "        # Debug: Show sample comparisons\n",
    "        print(f\"\\nüîç DEBUG - Sample comparisons (first 5 records):\")\n",
    "        for i in range(min(5, len(df))):\n",
    "            print(f\"   Record {i+1}: Ground Truth={df.iloc[i]['ground_truth_binary']}, Flash={df.iloc[i]['flash_binary']}, 2.5Pro={df.iloc[i]['pro_25_prediction']}\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading data: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def calculate_classification_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate comprehensive classification metrics for both models\"\"\"\n",
    "        print(\"üìä Calculating classification metrics...\")\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            return {}\n",
    "        \n",
    "        y_true = df['ground_truth_binary'].values\n",
    "        flash_pred = df['flash_binary'].values  \n",
    "        pro_pred = df['pro_25_prediction'].values\n",
    "        \n",
    "        # Flash metrics - Compare Flash predictions with ground truth\n",
    "        flash_metrics = self._calculate_model_metrics(y_true, flash_pred, \"Flash\")\n",
    "        \n",
    "        # 2.5 Pro metrics - Compare 2.5 Pro predictions with ground truth  \n",
    "        pro_metrics = self._calculate_model_metrics(y_true, pro_pred, \"Gemini 2.5 Pro\")\n",
    "        \n",
    "        # Agreement analysis\n",
    "        flash_vs_ground_truth_agreement = (flash_pred == y_true).mean()\n",
    "        pro_vs_ground_truth_agreement = (pro_pred == y_true).mean()\n",
    "        judge_agreement_rate = df['pro_judge_agreement'].mean()\n",
    "        \n",
    "        # Cross-analysis: When do they agree/disagree with each other and ground truth?\n",
    "        both_correct = ((flash_pred == y_true) & (pro_pred == y_true)).sum()\n",
    "        both_wrong = ((flash_pred != y_true) & (pro_pred != y_true)).sum()\n",
    "        flash_right_pro_wrong = ((flash_pred == y_true) & (pro_pred != y_true)).sum()\n",
    "        pro_right_flash_wrong = ((flash_pred != y_true) & (pro_pred == y_true)).sum()\n",
    "        \n",
    "        cross_analysis = {\n",
    "            'both_correct': int(both_correct),\n",
    "            'both_wrong': int(both_wrong), \n",
    "            'flash_right_pro_wrong': int(flash_right_pro_wrong),\n",
    "            'pro_right_flash_wrong': int(pro_right_flash_wrong),\n",
    "            'total': len(df)\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Metrics calculated:\")\n",
    "        print(f\"   Flash F1: {flash_metrics['f1_score']:.3f}\")\n",
    "        print(f\"   2.5 Pro F1: {pro_metrics['f1_score']:.3f}\")\n",
    "        print(f\"   Judge agreement: {judge_agreement_rate:.1%}\")\n",
    "        \n",
    "        return {\n",
    "            'dataset_info': {\n",
    "                'total_samples': len(df),\n",
    "                'positive_samples': int((y_true == 1).sum()),\n",
    "                'negative_samples': int((y_true == 0).sum()),\n",
    "                'class_balance': float((y_true == 1).mean())\n",
    "            },\n",
    "            'flash_metrics': flash_metrics,\n",
    "            'pro_25_metrics': pro_metrics,\n",
    "            'agreement_analysis': {\n",
    "                'flash_vs_ground_truth': flash_vs_ground_truth_agreement,\n",
    "                'pro_vs_ground_truth': pro_vs_ground_truth_agreement,\n",
    "                'pro_judge_agreement_with_flash': judge_agreement_rate,\n",
    "                'cross_analysis': cross_analysis\n",
    "            },\n",
    "            'model_comparison': {\n",
    "                'f1_difference': pro_metrics['f1_score'] - flash_metrics['f1_score'],\n",
    "                'accuracy_difference': pro_metrics['accuracy'] - flash_metrics['accuracy'],\n",
    "                'precision_difference': pro_metrics['precision'] - flash_metrics['precision'],\n",
    "                'recall_difference': pro_metrics['recall'] - flash_metrics['recall'],\n",
    "                'better_f1': 'Gemini 2.5 Pro' if pro_metrics['f1_score'] > flash_metrics['f1_score'] else 'Flash'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _calculate_model_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, model_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate detailed metrics for a single model\"\"\"\n",
    "        \n",
    "        # Ensure binary values\n",
    "        y_true = np.array(y_true).astype(int)\n",
    "        y_pred = np.array(y_pred).astype(int)\n",
    "        \n",
    "        # Validate binary values\n",
    "        if not (set(np.unique(y_true)) <= {0, 1} and set(np.unique(y_pred)) <= {0, 1}):\n",
    "            print(f\"‚ö†Ô∏è Warning: Non-binary values detected in {model_name}\")\n",
    "            print(f\"   y_true unique: {np.unique(y_true)}\")\n",
    "            print(f\"   y_pred unique: {np.unique(y_pred)}\")\n",
    "            # Force to binary\n",
    "            y_true = np.clip(y_true, 0, 1)\n",
    "            y_pred = np.clip(y_pred, 0, 1)\n",
    "        \n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        # Handle edge cases where only one class is predicted\n",
    "        unique_pred = np.unique(y_pred)\n",
    "        unique_true = np.unique(y_true)\n",
    "        \n",
    "        if len(unique_pred) == 1 or len(unique_true) == 1:\n",
    "            # Use macro average for edge cases\n",
    "            precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        else:\n",
    "            precision = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "        \n",
    "        # Confusion matrix - ensure we get 2x2 matrix\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "        \n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        elif cm.shape == (1, 1):\n",
    "            # Handle case where only one class exists\n",
    "            if unique_true[0] == 0 and unique_pred[0] == 0:\n",
    "                tn, fp, fn, tp = cm[0,0], 0, 0, 0\n",
    "            elif unique_true[0] == 1 and unique_pred[0] == 1:\n",
    "                tn, fp, fn, tp = 0, 0, 0, cm[0,0]\n",
    "            else:\n",
    "                tn, fp, fn, tp = 0, cm[0,0], 0, 0\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Unexpected confusion matrix shape for {model_name}: {cm.shape}\")\n",
    "            tn = fp = fn = tp = 0\n",
    "        \n",
    "        # Additional metrics\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # Same as recall\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # Negative Predictive Value\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Positive Predictive Value (same as precision)\n",
    "        \n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # False Negative Rate\n",
    "        \n",
    "        balanced_accuracy = (sensitivity + specificity) / 2\n",
    "        \n",
    "        # Matthews Correlation Coefficient\n",
    "        mcc_num = (tp * tn) - (fp * fn)\n",
    "        mcc_den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "        mcc = mcc_num / mcc_den if mcc_den > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'specificity': specificity,\n",
    "            'sensitivity': sensitivity,\n",
    "            'balanced_accuracy': balanced_accuracy,\n",
    "            'negative_predictive_value': npv,\n",
    "            'positive_predictive_value': ppv,\n",
    "            'false_positive_rate': fpr,\n",
    "            'false_negative_rate': fnr,\n",
    "            'matthews_correlation': mcc,\n",
    "            'confusion_matrix': {\n",
    "                'true_positives': int(tp),\n",
    "                'false_positives': int(fp), \n",
    "                'true_negatives': int(tn),\n",
    "                'false_negatives': int(fn)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def create_classification_dashboard(self, metrics: Dict[str, Any], df: pd.DataFrame) -> str:\n",
    "        \"\"\"Create comprehensive classification metrics dashboard\"\"\"\n",
    "        \n",
    "        if not metrics:\n",
    "            return \"<html><body><h1>No valid metrics to display</h1></body></html>\"\n",
    "        \n",
    "        flash_metrics = metrics['flash_metrics']\n",
    "        pro_metrics = metrics['pro_25_metrics']\n",
    "        dataset_info = metrics['dataset_info']\n",
    "        agreement = metrics['agreement_analysis']\n",
    "        comparison = metrics['model_comparison']\n",
    "        \n",
    "        flash_cm = flash_metrics['confusion_matrix']\n",
    "        pro_cm = pro_metrics['confusion_matrix']\n",
    "        \n",
    "        # Performance assessment\n",
    "        if pro_metrics['f1_score'] >= 0.8:\n",
    "            pro_assessment = \"üü¢ EXCELLENT\"\n",
    "            pro_color = \"#10B981\"\n",
    "        elif pro_metrics['f1_score'] >= 0.7:\n",
    "            pro_assessment = \"üü° GOOD\" \n",
    "            pro_color = \"#F59E0B\"\n",
    "        elif pro_metrics['f1_score'] >= 0.6:\n",
    "            pro_assessment = \"üü† MODERATE\"\n",
    "            pro_color = \"#FF6B35\"\n",
    "        else:\n",
    "            pro_assessment = \"üî¥ POOR\"\n",
    "            pro_color = \"#EF4444\"\n",
    "        \n",
    "        html_content = f\"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Gemini 2.5 Pro Classification Metrics Dashboard</title>\n",
    "    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/Chart.js/4.4.0/chart.min.js\"></script>\n",
    "    <style>\n",
    "        * {{ margin: 0; padding: 0; box-sizing: border-box; }}\n",
    "        body {{ font-family: 'Inter', sans-serif; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: #333; line-height: 1.6; min-height: 100vh; }}\n",
    "        .dashboard {{ max-width: 1800px; margin: 0 auto; padding: 20px; }}\n",
    "        .header {{ background: rgba(255,255,255,0.95); padding: 40px; border-radius: 25px; text-align: center; margin-bottom: 30px; backdrop-filter: blur(15px); box-shadow: 0 20px 60px rgba(0,0,0,0.1); }}\n",
    "        .header h1 {{ font-size: 3rem; margin-bottom: 15px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; }}\n",
    "        .header h2 {{ font-size: 1.5rem; color: #666; margin-bottom: 20px; }}\n",
    "        .model-badge {{ display: inline-block; padding: 10px 25px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border-radius: 30px; font-weight: bold; font-size: 1.1rem; margin: 5px; }}\n",
    "        .metrics-overview {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(180px, 1fr)); gap: 20px; margin-bottom: 40px; }}\n",
    "        .metric-card {{ background: rgba(255,255,255,0.95); padding: 25px; border-radius: 20px; text-align: center; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); transition: transform 0.3s ease; }}\n",
    "        .metric-card:hover {{ transform: translateY(-5px); }}\n",
    "        .metric-value {{ font-size: 2.5rem; font-weight: bold; margin-bottom: 10px; }}\n",
    "        .metric-label {{ font-size: 1rem; color: #666; font-weight: 600; margin-bottom: 5px; }}\n",
    "        .metric-description {{ font-size: 0.85rem; color: #888; }}\n",
    "        .excellent {{ color: #10B981; }}\n",
    "        .good {{ color: #F59E0B; }}\n",
    "        .moderate {{ color: #FF6B35; }}\n",
    "        .poor {{ color: #EF4444; }}\n",
    "        .dashboard-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(450px, 1fr)); gap: 25px; margin-bottom: 40px; }}\n",
    "        .chart-section {{ background: rgba(255,255,255,0.95); padding: 30px; border-radius: 20px; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); }}\n",
    "        .chart-title {{ font-size: 1.3rem; font-weight: bold; margin-bottom: 20px; color: #333; text-align: center; }}\n",
    "        .chart-container {{ position: relative; height: 300px; margin-bottom: 15px; }}\n",
    "        .confusion-matrix {{ display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px; margin: 20px 0; text-align: center; font-size: 0.9rem; }}\n",
    "        .cm-cell {{ padding: 15px; border-radius: 12px; font-weight: bold; display: flex; flex-direction: column; justify-content: center; align-items: center; }}\n",
    "        .cm-header {{ background: linear-gradient(135deg, #f8f9fa, #e9ecef); color: #495057; }}\n",
    "        .cm-tp {{ background: linear-gradient(135deg, #d4edda, #c3e6cb); color: #155724; font-size: 1.8rem; }}\n",
    "        .cm-fp {{ background: linear-gradient(135deg, #f8d7da, #f1b0b7); color: #721c24; font-size: 1.8rem; }}\n",
    "        .cm-fn {{ background: linear-gradient(135deg, #fff3cd, #fce4a6); color: #856404; font-size: 1.8rem; }}\n",
    "        .cm-tn {{ background: linear-gradient(135deg, #d1ecf1, #bee5eb); color: #0c5460; font-size: 1.8rem; }}\n",
    "        .cm-label {{ font-size: 0.75rem; margin-top: 5px; opacity: 0.8; }}\n",
    "        .performance-summary {{ background: linear-gradient(135deg, #e3f2fd, #bbdefb); padding: 30px; border-radius: 20px; margin: 30px 0; text-align: center; }}\n",
    "        .performance-score {{ font-size: 3.5rem; font-weight: bold; color: {pro_color}; margin-bottom: 10px; }}\n",
    "        .performance-label {{ font-size: 1.3rem; color: #37474f; margin-bottom: 8px; }}\n",
    "        .performance-description {{ font-size: 1rem; color: #546e7a; }}\n",
    "        .comparison-section {{ background: rgba(255,255,255,0.95); padding: 30px; border-radius: 20px; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); margin-bottom: 30px; }}\n",
    "        .comparison-grid {{ display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin-top: 20px; }}\n",
    "        .model-column {{ padding: 20px; border-radius: 15px; }}\n",
    "        .flash-column {{ background: linear-gradient(135deg, #e8f5e8, #d4edda); }}\n",
    "        .pro-column {{ background: linear-gradient(135deg, #e3f2fd, #bbdefb); }}\n",
    "        .model-title {{ font-size: 1.5rem; font-weight: bold; text-align: center; margin-bottom: 20px; }}\n",
    "        .metric-row {{ display: flex; justify-content: space-between; margin: 8px 0; padding: 8px; background: rgba(255,255,255,0.5); border-radius: 8px; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"dashboard\">\n",
    "        <div class=\"header\">\n",
    "            <h1>üéØ Classification Metrics Dashboard</h1>\n",
    "            <h2>Gemini Models vs Meta Ground Truth</h2>\n",
    "            <div>\n",
    "                <span class=\"model-badge\">Flash Model</span>\n",
    "                <span class=\"model-badge\">Gemini 2.5 Pro</span>\n",
    "            </div>\n",
    "            <p style=\"margin-top: 20px; color: #666;\">\n",
    "                Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | \n",
    "                Samples: {dataset_info['total_samples']:,} | \n",
    "                Positive: {dataset_info['positive_samples']} | Negative: {dataset_info['negative_samples']}\n",
    "            </p>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"performance-summary\">\n",
    "            <div class=\"performance-score\">{pro_metrics['f1_score']:.3f}</div>\n",
    "            <div class=\"performance-label\">{pro_assessment} F1-Score (Gemini 2.5 Pro)</div>\n",
    "            <div class=\"performance-description\">\n",
    "                2.5 Pro achieves {pro_metrics['accuracy']:.1%} accuracy vs {flash_metrics['accuracy']:.1%} for Flash\n",
    "                <br>Judge Agreement: {agreement['pro_judge_agreement_with_flash']:.1%} | Better Model: {comparison['better_f1']}\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"comparison-section\">\n",
    "            <div class=\"chart-title\">üìä Model Performance Comparison</div>\n",
    "            <div class=\"comparison-grid\">\n",
    "                <div class=\"model-column flash-column\">\n",
    "                    <div class=\"model-title\">‚ö° Flash Model</div>\n",
    "                    <div class=\"metric-row\"><span>F1-Score:</span><span class=\"{'excellent' if flash_metrics['f1_score'] >= 0.8 else 'good' if flash_metrics['f1_score'] >= 0.7 else 'moderate' if flash_metrics['f1_score'] >= 0.6 else 'poor'}\">{flash_metrics['f1_score']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Accuracy:</span><span>{flash_metrics['accuracy']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Precision:</span><span>{flash_metrics['precision']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Recall:</span><span>{flash_metrics['recall']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Specificity:</span><span>{flash_metrics['specificity']:.3f}</span></div>\n",
    "                </div>\n",
    "                <div class=\"model-column pro-column\">\n",
    "                    <div class=\"model-title\">üß† Gemini 2.5 Pro</div>\n",
    "                    <div class=\"metric-row\"><span>F1-Score:</span><span class=\"{'excellent' if pro_metrics['f1_score'] >= 0.8 else 'good' if pro_metrics['f1_score'] >= 0.7 else 'moderate' if pro_metrics['f1_score'] >= 0.6 else 'poor'}\">{pro_metrics['f1_score']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Accuracy:</span><span>{pro_metrics['accuracy']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Precision:</span><span>{pro_metrics['precision']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Recall:</span><span>{pro_metrics['recall']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Specificity:</span><span>{pro_metrics['specificity']:.3f}</span></div>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"dashboard-grid\">\n",
    "            <div class=\"chart-section\">\n",
    "                <div class=\"chart-title\">üìä F1-Score Comparison</div>\n",
    "                <div class=\"chart-container\">\n",
    "                    <canvas id=\"f1Chart\"></canvas>\n",
    "                </div>\n",
    "            </div>\n",
    "\n",
    "            <div class=\"chart-section\">\n",
    "                <div class=\"chart-title\">üéØ Precision vs Recall</div>\n",
    "                <div class=\"chart-container\">\n",
    "                    <canvas id=\"precisionRecallChart\"></canvas>\n",
    "                </div>\n",
    "            </div>\n",
    "\n",
    "            <div class=\"chart-section\">\n",
    "                <div class=\"chart-title\">üîç Flash Confusion Matrix</div>\n",
    "                <div class=\"confusion-matrix\">\n",
    "                    <div class=\"cm-cell cm-header\"></div>\n",
    "                    <div class=\"cm-cell cm-header\">Ground Truth: 0</div>\n",
    "                    <div class=\"cm-cell cm-header\">Ground Truth: 1</div>\n",
    "                    \n",
    "                    <div class=\"cm-cell cm-header\">Flash: 0</div>\n",
    "                    <div class=\"cm-cell cm-tn\">\n",
    "                        {flash_cm['true_negatives']}\n",
    "                        <div class=\"cm-label\">True Negatives</div>\n",
    "                    </div>\n",
    "                    <div class=\"cm-cell cm-fn\">\n",
    "                        {flash_cm['false_negatives']}\n",
    "                        <div class=\"cm-label\">False Negatives</div>\n",
    "                    </div>\n",
    "                    \n",
    "                    <div class=\"cm-cell cm-header\">Flash: 1</div>\n",
    "                    <div class=\"cm-cell cm-fp\">\n",
    "                        {flash_cm['false_positives']}\n",
    "                        <div class=\"cm-label\">False Positives</div>\n",
    "                    </div>\n",
    "                    <div class=\"cm-cell cm-tp\">\n",
    "                        {flash_cm['true_positives']}\n",
    "                        <div class=\"cm-label\">True Positives</div>\n",
    "                    </div>\n",
    "                </div>\n",
    "            </div>\n",
    "\n",
    "            <div class=\"chart-section\">\n",
    "                <div class=\"chart-title\">üß† Gemini 2.5 Pro Confusion Matrix</div>\n",
    "                <div class=\"confusion-matrix\">\n",
    "                    <div class=\"cm-cell cm-header\"></div>\n",
    "                    <div class=\"cm-cell cm-header\">Ground Truth: 0</div>\n",
    "                    <div class=\"cm-cell cm-header\">Ground Truth: 1</div>\n",
    "                    \n",
    "                    <div class=\"cm-cell cm-header\">2.5 Pro: 0</div>\n",
    "                    <div class=\"cm-cell cm-tn\">\n",
    "                        {pro_cm['true_negatives']}\n",
    "                        <div class=\"cm-label\">True Negatives</div>\n",
    "                    </div>\n",
    "                    <div class=\"cm-cell cm-fn\">\n",
    "                        {pro_cm['false_negatives']}\n",
    "                        <div class=\"cm-label\">False Negatives</div>\n",
    "                    </div>\n",
    "                    \n",
    "                    <div class=\"cm-cell cm-header\">2.5 Pro: 1</div>\n",
    "                    <div class=\"cm-cell cm-fp\">\n",
    "                        {pro_cm['false_positives']}\n",
    "                        <div class=\"cm-label\">False Positives</div>\n",
    "                    </div>\n",
    "                    <div class=\"cm-cell cm-tp\">\n",
    "                        {pro_cm['true_positives']}\n",
    "                        <div class=\"cm-label\">True Positives</div>\n",
    "                    </div>\n",
    "                </div>\n",
    "            </div>\n",
    "\n",
    "            <div class=\"chart-section\">\n",
    "                <div class=\"chart-title\">‚öñÔ∏è Agreement Analysis</div>\n",
    "                <div class=\"chart-container\">\n",
    "                    <canvas id=\"agreementChart\"></canvas>\n",
    "                </div>\n",
    "            </div>\n",
    "\n",
    "            <div class=\"chart-section\">\n",
    "                <div class=\"chart-title\">üìà Performance Radar</div>\n",
    "                <div class=\"chart-container\">\n",
    "                    <canvas id=\"radarChart\"></canvas>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        window.addEventListener('load', function() {{\n",
    "            setTimeout(function() {{\n",
    "                // F1-Score Comparison Chart\n",
    "                const f1Ctx = document.getElementById('f1Chart');\n",
    "                if (f1Ctx && window.Chart) {{\n",
    "                    new Chart(f1Ctx.getContext('2d'), {{\n",
    "                        type: 'bar',\n",
    "                        data: {{\n",
    "                            labels: ['Flash', 'Gemini 2.5 Pro'],\n",
    "                            datasets: [{{\n",
    "                                label: 'F1-Score',\n",
    "                                data: [{flash_metrics['f1_score']:.3f}, {pro_metrics['f1_score']:.3f}],\n",
    "                                backgroundColor: ['#FF6B6B', '#4ECDC4'],\n",
    "                                borderRadius: 8\n",
    "                            }}]\n",
    "                        }},\n",
    "                        options: {{ \n",
    "                            responsive: true, \n",
    "                            maintainAspectRatio: false,\n",
    "                            scales: {{ y: {{ beginAtZero: true, max: 1 }} }},\n",
    "                            plugins: {{ legend: {{ position: 'bottom' }} }}\n",
    "                        }}\n",
    "                    }});\n",
    "                }}\n",
    "\n",
    "                // Precision vs Recall Chart\n",
    "                const prCtx = document.getElementById('precisionRecallChart');\n",
    "                if (prCtx && window.Chart) {{\n",
    "                    new Chart(prCtx.getContext('2d'), {{\n",
    "                        type: 'scatter',\n",
    "                        data: {{\n",
    "                            datasets: [{{\n",
    "                                label: 'Flash',\n",
    "                                data: [{{x: {flash_metrics['recall']:.3f}, y: {flash_metrics['precision']:.3f}}}],\n",
    "                                backgroundColor: '#FF6B6B',\n",
    "                                pointRadius: 12\n",
    "                            }}, {{\n",
    "                                label: 'Gemini 2.5 Pro',\n",
    "                                data: [{{x: {pro_metrics['recall']:.3f}, y: {pro_metrics['precision']:.3f}}}],\n",
    "                                backgroundColor: '#4ECDC4',\n",
    "                                pointRadius: 12\n",
    "                            }}]\n",
    "                        }},\n",
    "                        options: {{ \n",
    "                            responsive: true, \n",
    "                            maintainAspectRatio: false,\n",
    "                            scales: {{ \n",
    "                                x: {{ title: {{ display: true, text: 'Recall' }}, min: 0, max: 1 }},\n",
    "                                y: {{ title: {{ display: true, text: 'Precision' }}, min: 0, max: 1 }}\n",
    "                            }},\n",
    "                            plugins: {{ legend: {{ position: 'bottom' }} }}\n",
    "                        }}\n",
    "                    }});\n",
    "                }}\n",
    "\n",
    "                // Agreement Analysis Chart\n",
    "                const agreeCtx = document.getElementById('agreementChart');\n",
    "                if (agreeCtx && window.Chart) {{\n",
    "                    new Chart(agreeCtx.getContext('2d'), {{\n",
    "                        type: 'bar',\n",
    "                        data: {{\n",
    "                            labels: ['Flash vs Ground Truth', '2.5 Pro vs Ground Truth', '2.5 Pro Judge Agreement'],\n",
    "                            datasets: [{{\n",
    "                                label: 'Agreement Rate',\n",
    "                                data: [{agreement['flash_vs_ground_truth']:.3f}, {agreement['pro_vs_ground_truth']:.3f}, {agreement['pro_judge_agreement_with_flash']:.3f}],\n",
    "                                backgroundColor: ['#FF6B6B', '#4ECDC4', '#FFD93D'],\n",
    "                                borderRadius: 8\n",
    "                            }}]\n",
    "                        }},\n",
    "                        options: {{ \n",
    "                            responsive: true, \n",
    "                            maintainAspectRatio: false,\n",
    "                            scales: {{ y: {{ beginAtZero: true, max: 1 }} }},\n",
    "                            plugins: {{ legend: {{ position: 'bottom' }} }}\n",
    "                        }}\n",
    "                    }});\n",
    "                }}\n",
    "\n",
    "                // Performance Radar Chart\n",
    "                const radarCtx = document.getElementById('radarChart');\n",
    "                if (radarCtx && window.Chart) {{\n",
    "                    new Chart(radarCtx.getContext('2d'), {{\n",
    "                        type: 'radar',\n",
    "                        data: {{\n",
    "                            labels: ['F1-Score', 'Precision', 'Recall', 'Accuracy', 'Specificity'],\n",
    "                            datasets: [{{\n",
    "                                label: 'Flash',\n",
    "                                data: [{flash_metrics['f1_score']:.3f}, {flash_metrics['precision']:.3f}, {flash_metrics['recall']:.3f}, {flash_metrics['accuracy']:.3f}, {flash_metrics['specificity']:.3f}],\n",
    "                                backgroundColor: 'rgba(255, 107, 107, 0.2)',\n",
    "                                borderColor: '#FF6B6B',\n",
    "                                borderWidth: 2\n",
    "                            }}, {{\n",
    "                                label: 'Gemini 2.5 Pro',\n",
    "                                data: [{pro_metrics['f1_score']:.3f}, {pro_metrics['precision']:.3f}, {pro_metrics['recall']:.3f}, {pro_metrics['accuracy']:.3f}, {pro_metrics['specificity']:.3f}],\n",
    "                                backgroundColor: 'rgba(78, 205, 196, 0.2)',\n",
    "                                borderColor: '#4ECDC4',\n",
    "                                borderWidth: 2\n",
    "                            }}]\n",
    "                        }},\n",
    "                        options: {{ \n",
    "                            responsive: true, \n",
    "                            maintainAspectRatio: false,\n",
    "                            scales: {{ r: {{ beginAtZero: true, max: 1 }} }},\n",
    "                            plugins: {{ legend: {{ position: 'bottom' }} }}\n",
    "                        }}\n",
    "                    }});\n",
    "                }}\n",
    "\n",
    "                console.log('Classification Dashboard Loaded');\n",
    "            }}, 1000);\n",
    "        }});\n",
    "    </script>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "        \n",
    "        return html_content\n",
    "    \n",
    "    def generate_classification_dashboard(self):\n",
    "        \"\"\"Main function to generate the classification dashboard\"\"\"\n",
    "        print(\"üöÄ STARTING CLASSIFICATION METRICS DASHBOARD GENERATION\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        try:\n",
    "            # Load results with ground truth\n",
    "            df = self.load_results_with_ground_truth()\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"‚ùå No data available for dashboard\")\n",
    "                return None\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = self.calculate_classification_metrics(df)\n",
    "            \n",
    "            if not metrics:\n",
    "                print(\"‚ùå Could not calculate metrics\")\n",
    "                return None\n",
    "            \n",
    "            # Create dashboard\n",
    "            print(\"\\nüìä Creating Classification Dashboard...\")\n",
    "            dashboard_html = self.create_classification_dashboard(metrics, df)\n",
    "            \n",
    "            # Save files\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            dashboard_filename = f\"gemini_classification_dashboard_{timestamp}.html\"\n",
    "            metrics_filename = f\"classification_metrics_{timestamp}.json\"\n",
    "            \n",
    "            with open(dashboard_filename, \"w\", encoding='utf-8') as f:\n",
    "                f.write(dashboard_html)\n",
    "            \n",
    "            with open(metrics_filename, \"w\") as f:\n",
    "                json.dump(metrics, f, indent=2, default=str)\n",
    "            \n",
    "            print(f\"‚úÖ Dashboard saved: {dashboard_filename}\")\n",
    "            print(f\"‚úÖ Metrics saved: {metrics_filename}\")\n",
    "            \n",
    "            # Summary\n",
    "            flash_metrics = metrics['flash_metrics']\n",
    "            pro_metrics = metrics['pro_25_metrics']\n",
    "            agreement = metrics['agreement_analysis']\n",
    "            \n",
    "            print(f\"\\nüìä CLASSIFICATION METRICS SUMMARY:\")\n",
    "            print(f\"   üìä Flash Model:\")\n",
    "            print(f\"      F1-Score: {flash_metrics['f1_score']:.3f}\")\n",
    "            print(f\"      Precision: {flash_metrics['precision']:.3f}\")\n",
    "            print(f\"      Recall: {flash_metrics['recall']:.3f}\")\n",
    "            print(f\"      Accuracy: {flash_metrics['accuracy']:.3f}\")\n",
    "            print(f\"      TP: {flash_metrics['confusion_matrix']['true_positives']}, FP: {flash_metrics['confusion_matrix']['false_positives']}\")\n",
    "            print(f\"      TN: {flash_metrics['confusion_matrix']['true_negatives']}, FN: {flash_metrics['confusion_matrix']['false_negatives']}\")\n",
    "            \n",
    "            print(f\"\\n   üß† Gemini 2.5 Pro:\")\n",
    "            print(f\"      F1-Score: {pro_metrics['f1_score']:.3f}\")\n",
    "            print(f\"      Precision: {pro_metrics['precision']:.3f}\")\n",
    "            print(f\"      Recall: {pro_metrics['recall']:.3f}\") \n",
    "            print(f\"      Accuracy: {pro_metrics['accuracy']:.3f}\")\n",
    "            print(f\"      TP: {pro_metrics['confusion_matrix']['true_positives']}, FP: {pro_metrics['confusion_matrix']['false_positives']}\")\n",
    "            print(f\"      TN: {pro_metrics['confusion_matrix']['true_negatives']}, FN: {pro_metrics['confusion_matrix']['false_negatives']}\")\n",
    "            \n",
    "            print(f\"\\n   ‚öñÔ∏è Agreement Rates:\")\n",
    "            print(f\"      Flash vs Ground Truth: {agreement['flash_vs_ground_truth']:.1%}\")\n",
    "            print(f\"      2.5 Pro vs Ground Truth: {agreement['pro_vs_ground_truth']:.1%}\")\n",
    "            print(f\"      2.5 Pro Judge Agreement: {agreement['pro_judge_agreement_with_flash']:.1%}\")\n",
    "            \n",
    "            return dashboard_filename, metrics_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Dashboard generation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the classification dashboard generation\"\"\"\n",
    "    dashboard_generator = Gemini25ProClassificationDashboard()\n",
    "    result = dashboard_generator.generate_classification_dashboard()\n",
    "    \n",
    "    if result:\n",
    "        dashboard_file, metrics_file = result\n",
    "        print(f\"\\nüéâ SUCCESS!\")\n",
    "        print(f\"üìÅ Files generated:\")\n",
    "        print(f\"   1. {dashboard_file}\")\n",
    "        print(f\"   2. {metrics_file}\")\n",
    "        print(f\"\\nüí° Open {dashboard_file} in your browser to view the dashboard!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Dashboard generation failed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "583789d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING CLASSIFICATION METRICS DASHBOARD GENERATION\n",
      "======================================================================\n",
      "üì• Loading Gemini 2.5 Pro results with ground truth...\n",
      "üìä Loading results data...\n",
      "   Found 299 result records\n",
      "üìä Loading ground truth data...\n",
      "   Found 299 ground truth records\n",
      "   Ground truth distribution from Meta table: {100: 187, 0: 112}\n",
      "‚úÖ Merged dataset: 299 records\n",
      "   Sample artifact_ids from results: ['meta:aWdfbWVkaWFfM3B2OjE3OTUxMjQ0NTg3OTc2MTk3', 'meta:aWdfbWVkaWFfM3B2OjE4MDQ1MjA0Njk4MjMzMDYz', 'meta:aWdfbWVkaWFfM3B2OjE4MDQ5OTgyNjc1MTMzMjM5', 'meta:993741799587060', 'meta:aWdfbWVkaWFfM3B2OjE4MDYwMjIyMDk0OTgxNzk5']\n",
      "   Sample artifact_ids from meta: ['meta:122140809938790694', 'meta:4050755845195196', 'meta:1173810271458869', 'meta:1297387721748313', 'meta:1138085658345605']\n",
      "   Sample flash_classification: [0, 0, 0, 0, 0]\n",
      "   Sample ground_truth after merge: [0, 0, 0, 0, 0]\n",
      "   Flash vs Ground Truth identical BEFORE conversion: 299/299\n",
      "   üö® CRITICAL ERROR: flash_classification is identical to ground_truth!\n",
      "   This means your ground truth data IS the Flash predictions, not human annotations.\n",
      "   Check your data pipeline - Meta table might contain Flash results instead of human labels.\n",
      "   üìã Sample records to verify:\n",
      "      Record 1: artifact_id=meta:aWdfbWVkaWFfM3B2OjE3OTUxMjQ0NTg3OTc2MTk3, flash=0, ground_truth=0\n",
      "      Record 2: artifact_id=meta:aWdfbWVkaWFfM3B2OjE4MDQ1MjA0Njk4MjMzMDYz, flash=0, ground_truth=0\n",
      "      Record 3: artifact_id=meta:aWdfbWVkaWFfM3B2OjE4MDQ5OTgyNjc1MTMzMjM5, flash=0, ground_truth=0\n",
      "üìä Data distribution:\n",
      "   Ground truth: {1: 187, 0: 112}\n",
      "   Flash predictions: {1: 187, 0: 112}\n",
      "   2.5 Pro predictions: {0: 156, 1: 143}\n",
      "   Judge agreement rate: 73.9%\n",
      "üìä Final clean dataset: 299 records\n",
      "\n",
      "üîç DEBUG - Sample comparisons (first 5 records):\n",
      "   Columns available: ['artifact_id', 'flash_classification', 'flash_reasoning', 'pro_judge_agreement', 'pro_would_reach_same_conclusion', 'pro_confidence', 'pro_reasoning', 'ground_truth', 'ground_truth_reasoning', 'source', 'ground_truth_binary', 'flash_binary', 'pro_25_prediction']\n",
      "   Record 1:\n",
      "      artifact_id: meta:aWdfbWVkaWFfM3B2OjE3OTUxMjQ0NTg3OTc2MTk3\n",
      "      ground_truth (from Meta): 0 -> binary: 0\n",
      "      flash_classification: 0 -> binary: 0\n",
      "      pro_would_reach_same_conclusion: True\n",
      "      pro_25_prediction: 0\n",
      "      Flash vs Ground Truth: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 2:\n",
      "      artifact_id: meta:aWdfbWVkaWFfM3B2OjE4MDQ1MjA0Njk4MjMzMDYz\n",
      "      ground_truth (from Meta): 0 -> binary: 0\n",
      "      flash_classification: 0 -> binary: 0\n",
      "      pro_would_reach_same_conclusion: True\n",
      "      pro_25_prediction: 0\n",
      "      Flash vs Ground Truth: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 3:\n",
      "      artifact_id: meta:aWdfbWVkaWFfM3B2OjE4MDQ5OTgyNjc1MTMzMjM5\n",
      "      ground_truth (from Meta): 0 -> binary: 0\n",
      "      flash_classification: 0 -> binary: 0\n",
      "      pro_would_reach_same_conclusion: True\n",
      "      pro_25_prediction: 0\n",
      "      Flash vs Ground Truth: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 4:\n",
      "      artifact_id: meta:993741799587060\n",
      "      ground_truth (from Meta): 0 -> binary: 0\n",
      "      flash_classification: 0 -> binary: 0\n",
      "      pro_would_reach_same_conclusion: True\n",
      "      pro_25_prediction: 0\n",
      "      Flash vs Ground Truth: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 5:\n",
      "      artifact_id: meta:aWdfbWVkaWFfM3B2OjE4MDYwMjIyMDk0OTgxNzk5\n",
      "      ground_truth (from Meta): 0 -> binary: 0\n",
      "      flash_classification: 0 -> binary: 0\n",
      "      pro_would_reach_same_conclusion: True\n",
      "      pro_25_prediction: 0\n",
      "      Flash vs Ground Truth: ‚úÖ MATCH\n",
      "      ---\n",
      "\n",
      "üö® CRITICAL CHECK:\n",
      "   Flash predictions matching ground truth: 299/299 (100.0%)\n",
      "   üö® ERROR: Flash predictions are 100% identical to ground truth!\n",
      "   This suggests the ground truth data is actually Flash's predictions, not human annotations.\n",
      "   Check your data pipeline - the 'ground_truth' field might be populated with Flash results.\n",
      "üìä Calculating classification metrics...\n",
      "‚úÖ Metrics calculated:\n",
      "   Flash F1: 1.000\n",
      "   2.5 Pro F1: 0.764\n",
      "   Judge agreement: 73.9%\n",
      "\n",
      "üìä Creating Classification Dashboard...\n",
      "‚úÖ Dashboard saved: gemini_classification_dashboard_20250725_135732.html\n",
      "‚úÖ Metrics saved: classification_metrics_20250725_135732.json\n",
      "\n",
      "üìä CLASSIFICATION METRICS SUMMARY:\n",
      "   üìä Flash Model:\n",
      "      F1-Score: 1.000\n",
      "      Precision: 1.000\n",
      "      Recall: 1.000\n",
      "      Accuracy: 1.000\n",
      "      TP: 187, FP: 0\n",
      "      TN: 112, FN: 0\n",
      "\n",
      "   üß† Gemini 2.5 Pro:\n",
      "      F1-Score: 0.764\n",
      "      Precision: 0.881\n",
      "      Recall: 0.674\n",
      "      Accuracy: 0.739\n",
      "      TP: 126, FP: 17\n",
      "      TN: 95, FN: 61\n",
      "\n",
      "   ‚öñÔ∏è Agreement Rates:\n",
      "      Flash vs Ground Truth: 100.0%\n",
      "      2.5 Pro vs Ground Truth: 73.9%\n",
      "      2.5 Pro Judge Agreement: 73.9%\n",
      "\n",
      "üéâ SUCCESS!\n",
      "üìÅ Files generated:\n",
      "   1. gemini_classification_dashboard_20250725_135732.html\n",
      "   2. classification_metrics_20250725_135732.json\n",
      "\n",
      "üí° Open gemini_classification_dashboard_20250725_135732.html in your browser to view the dashboard!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Gemini 2.5 Pro Classification Metrics Dashboard - Real Results vs Ground Truth\n",
    "Calculates traditional ML metrics (F1, Precision, Recall, Confusion Matrix) for 2.5 Pro decisions\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "PROJECT_ID = \"scope3-dev\"\n",
    "DATASET_ID = \"research_bs_monitoring\"\n",
    "META_TABLE = \"BA_Meta_Ground_Truth\"\n",
    "RESULTS_TABLE = \"BA_Meta_Gemini_25_Pro_Judge_Results\"\n",
    "\n",
    "class Gemini25ProClassificationDashboard:\n",
    "    def __init__(self):\n",
    "        self.client = bigquery.Client(project=PROJECT_ID)\n",
    "        \n",
    "    def load_results_with_ground_truth(self) -> pd.DataFrame:\n",
    "        \"\"\"Load 2.5 Pro results with real ground truth from Meta table\"\"\"\n",
    "        print(\"üì• Loading Gemini 2.5 Pro results with ground truth...\")\n",
    "        \n",
    "        # First, let's get the results data\n",
    "        results_query = f\"\"\"\n",
    "        SELECT \n",
    "            artifact_id,\n",
    "            flash_classification,\n",
    "            flash_reasoning,\n",
    "            pro_judge_agreement,\n",
    "            pro_would_reach_same_conclusion,\n",
    "            pro_confidence,\n",
    "            pro_reasoning\n",
    "        FROM `{PROJECT_ID}.{DATASET_ID}.{RESULTS_TABLE}`\n",
    "        WHERE pro_judge_agreement IS NOT NULL\n",
    "        AND pro_would_reach_same_conclusion IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get real ground truth from Meta table\n",
    "        meta_query = f\"\"\"\n",
    "        SELECT \n",
    "            artifact_id,\n",
    "            correct_classification as ground_truth,\n",
    "            correct_reasoning as ground_truth_reasoning,\n",
    "            source\n",
    "        FROM `{PROJECT_ID}.{DATASET_ID}.{META_TABLE}`\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(\"üìä Loading results data...\")\n",
    "            results_df = self.client.query(results_query).to_dataframe()\n",
    "            print(f\"   Found {len(results_df)} result records\")\n",
    "            \n",
    "            print(\"üìä Loading ground truth data...\")\n",
    "            meta_df = self.client.query(meta_query).to_dataframe()\n",
    "            print(f\"   Found {len(meta_df)} ground truth records\")\n",
    "            \n",
    "            # Debug: Check ground truth distribution\n",
    "            if len(meta_df) > 0:\n",
    "                gt_dist = meta_df['ground_truth'].value_counts().to_dict()\n",
    "                print(f\"   Ground truth distribution from Meta table: {gt_dist}\")\n",
    "            \n",
    "            # Join results with ground truth\n",
    "            df = results_df.merge(meta_df, on='artifact_id', how='inner')\n",
    "            print(f\"‚úÖ Merged dataset: {len(df)} records\")\n",
    "            \n",
    "            if len(df) == 0:\n",
    "                print(\"‚ùå No matching records found after merge\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Debug: Check what we actually got from the merge\n",
    "            print(f\"   Sample artifact_ids from results: {results_df['artifact_id'].head().tolist()}\")\n",
    "            print(f\"   Sample artifact_ids from meta: {meta_df['artifact_id'].head().tolist()}\")\n",
    "            print(f\"   Sample flash_classification: {results_df['flash_classification'].head().tolist()}\")\n",
    "            print(f\"   Sample ground_truth after merge: {df['ground_truth'].head().tolist()}\")\n",
    "            \n",
    "            # Check if flash_classification and ground_truth are identical BEFORE conversion\n",
    "            identical_before_conversion = (df['flash_classification'] == df['ground_truth']).sum()\n",
    "            print(f\"   Flash vs Ground Truth identical BEFORE conversion: {identical_before_conversion}/{len(df)}\")\n",
    "            \n",
    "            if identical_before_conversion == len(df):\n",
    "                print(\"   üö® CRITICAL ERROR: flash_classification is identical to ground_truth!\")\n",
    "                print(\"   This means your ground truth data IS the Flash predictions, not human annotations.\")\n",
    "                print(\"   Check your data pipeline - Meta table might contain Flash results instead of human labels.\")\n",
    "                \n",
    "                # Let's check a few individual records to confirm\n",
    "                print(\"   üìã Sample records to verify:\")\n",
    "                for i in range(min(3, len(df))):\n",
    "                    row = df.iloc[i]\n",
    "                    print(f\"      Record {i+1}: artifact_id={row['artifact_id']}, flash={row['flash_classification']}, ground_truth={row['ground_truth']}\")\n",
    "            \n",
    "            # Convert to binary format\n",
    "            df['ground_truth_binary'] = (df['ground_truth'] == 100).astype(int)\n",
    "            df['flash_binary'] = (df['flash_classification'] == 100).astype(int)\n",
    "            \n",
    "            # Convert 2.5 Pro predictions based on \"would reach same conclusion\"\n",
    "            df['pro_25_prediction'] = np.where(\n",
    "                df['pro_would_reach_same_conclusion'] == True,\n",
    "                df['flash_binary'],  # Same as Flash\n",
    "                1 - df['flash_binary']  # Opposite of Flash\n",
    "            ).astype(int)\n",
    "            \n",
    "            print(f\"üìä Data distribution:\")\n",
    "            print(f\"   Ground truth: {df['ground_truth_binary'].value_counts().to_dict()}\")\n",
    "            print(f\"   Flash predictions: {df['flash_binary'].value_counts().to_dict()}\")\n",
    "            print(f\"   2.5 Pro predictions: {df['pro_25_prediction'].value_counts().to_dict()}\")\n",
    "            print(f\"   Judge agreement rate: {df['pro_judge_agreement'].mean():.1%}\")\n",
    "            \n",
    "            # Validate binary data\n",
    "            for col, name in [('ground_truth_binary', 'Ground truth'), \n",
    "                            ('flash_binary', 'Flash'), \n",
    "                            ('pro_25_prediction', '2.5 Pro')]:\n",
    "                if not all(df[col].isin([0, 1])):\n",
    "                    print(f\"‚ö†Ô∏è Warning: {name} contains non-binary values\")\n",
    "                    df = df[df[col].isin([0, 1])]\n",
    "            \n",
    "            print(f\"üìä Final clean dataset: {len(df)} records\")\n",
    "            \n",
    "            # Debug sample comparisons\n",
    "            print(f\"\\nüîç DEBUG - Sample comparisons (first 5 records):\")\n",
    "            print(f\"   Columns available: {df.columns.tolist()}\")\n",
    "            for i in range(min(5, len(df))):\n",
    "                row = df.iloc[i]\n",
    "                print(f\"   Record {i+1}:\")\n",
    "                print(f\"      artifact_id: {row['artifact_id']}\")\n",
    "                print(f\"      ground_truth (from Meta): {row['ground_truth']} -> binary: {row['ground_truth_binary']}\")\n",
    "                print(f\"      flash_classification: {row['flash_classification']} -> binary: {row['flash_binary']}\")\n",
    "                print(f\"      pro_would_reach_same_conclusion: {row['pro_would_reach_same_conclusion']}\")\n",
    "                print(f\"      pro_25_prediction: {row['pro_25_prediction']}\")\n",
    "                print(f\"      Flash vs Ground Truth: {'‚úÖ MATCH' if row['flash_binary'] == row['ground_truth_binary'] else '‚ùå DIFFER'}\")\n",
    "                print(f\"      ---\")\n",
    "            \n",
    "            # Check if Flash predictions are identical to ground truth\n",
    "            flash_matches_gt = (df['flash_binary'] == df['ground_truth_binary']).sum()\n",
    "            print(f\"\\nüö® CRITICAL CHECK:\")\n",
    "            print(f\"   Flash predictions matching ground truth: {flash_matches_gt}/{len(df)} ({flash_matches_gt/len(df):.1%})\")\n",
    "            \n",
    "            if flash_matches_gt == len(df):\n",
    "                print(\"   üö® ERROR: Flash predictions are 100% identical to ground truth!\")\n",
    "                print(\"   This suggests the ground truth data is actually Flash's predictions, not human annotations.\")\n",
    "                print(\"   Check your data pipeline - the 'ground_truth' field might be populated with Flash results.\")\n",
    "            elif flash_matches_gt > len(df) * 0.95:\n",
    "                print(\"   ‚ö†Ô∏è WARNING: Flash predictions are suspiciously similar to ground truth (>95% match)\")\n",
    "                print(\"   This might indicate data contamination.\")\n",
    "            else:\n",
    "                print(\"   ‚úÖ Good: Flash predictions differ from ground truth as expected.\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading data: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def calculate_classification_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate comprehensive classification metrics for both models\"\"\"\n",
    "        print(\"üìä Calculating classification metrics...\")\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            return {}\n",
    "        \n",
    "        y_true = df['ground_truth_binary'].values\n",
    "        flash_pred = df['flash_binary'].values  \n",
    "        pro_pred = df['pro_25_prediction'].values\n",
    "        \n",
    "        # Flash metrics - Compare Flash predictions with ground truth\n",
    "        flash_metrics = self._calculate_model_metrics(y_true, flash_pred, \"Flash\")\n",
    "        \n",
    "        # 2.5 Pro metrics - Compare 2.5 Pro predictions with ground truth  \n",
    "        pro_metrics = self._calculate_model_metrics(y_true, pro_pred, \"Gemini 2.5 Pro\")\n",
    "        \n",
    "        # Agreement analysis\n",
    "        flash_vs_ground_truth_agreement = (flash_pred == y_true).mean()\n",
    "        pro_vs_ground_truth_agreement = (pro_pred == y_true).mean()\n",
    "        judge_agreement_rate = df['pro_judge_agreement'].mean()\n",
    "        \n",
    "        # Cross-analysis: When do they agree/disagree with each other and ground truth?\n",
    "        both_correct = ((flash_pred == y_true) & (pro_pred == y_true)).sum()\n",
    "        both_wrong = ((flash_pred != y_true) & (pro_pred != y_true)).sum()\n",
    "        flash_right_pro_wrong = ((flash_pred == y_true) & (pro_pred != y_true)).sum()\n",
    "        pro_right_flash_wrong = ((flash_pred != y_true) & (pro_pred == y_true)).sum()\n",
    "        \n",
    "        cross_analysis = {\n",
    "            'both_correct': int(both_correct),\n",
    "            'both_wrong': int(both_wrong), \n",
    "            'flash_right_pro_wrong': int(flash_right_pro_wrong),\n",
    "            'pro_right_flash_wrong': int(pro_right_flash_wrong),\n",
    "            'total': len(df)\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Metrics calculated:\")\n",
    "        print(f\"   Flash F1: {flash_metrics['f1_score']:.3f}\")\n",
    "        print(f\"   2.5 Pro F1: {pro_metrics['f1_score']:.3f}\")\n",
    "        print(f\"   Judge agreement: {judge_agreement_rate:.1%}\")\n",
    "        \n",
    "        return {\n",
    "            'dataset_info': {\n",
    "                'total_samples': len(df),\n",
    "                'positive_samples': int((y_true == 1).sum()),\n",
    "                'negative_samples': int((y_true == 0).sum()),\n",
    "                'class_balance': float((y_true == 1).mean())\n",
    "            },\n",
    "            'flash_metrics': flash_metrics,\n",
    "            'pro_25_metrics': pro_metrics,\n",
    "            'agreement_analysis': {\n",
    "                'flash_vs_ground_truth': flash_vs_ground_truth_agreement,\n",
    "                'pro_vs_ground_truth': pro_vs_ground_truth_agreement,\n",
    "                'pro_judge_agreement_with_flash': judge_agreement_rate,\n",
    "                'cross_analysis': cross_analysis\n",
    "            },\n",
    "            'model_comparison': {\n",
    "                'f1_difference': pro_metrics['f1_score'] - flash_metrics['f1_score'],\n",
    "                'accuracy_difference': pro_metrics['accuracy'] - flash_metrics['accuracy'],\n",
    "                'precision_difference': pro_metrics['precision'] - flash_metrics['precision'],\n",
    "                'recall_difference': pro_metrics['recall'] - flash_metrics['recall'],\n",
    "                'better_f1': 'Gemini 2.5 Pro' if pro_metrics['f1_score'] > flash_metrics['f1_score'] else 'Flash'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _calculate_model_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, model_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate detailed metrics for a single model\"\"\"\n",
    "        \n",
    "        # Ensure binary values\n",
    "        y_true = np.array(y_true).astype(int)\n",
    "        y_pred = np.array(y_pred).astype(int)\n",
    "        \n",
    "        # Validate binary values\n",
    "        if not (set(np.unique(y_true)) <= {0, 1} and set(np.unique(y_pred)) <= {0, 1}):\n",
    "            print(f\"‚ö†Ô∏è Warning: Non-binary values detected in {model_name}\")\n",
    "            print(f\"   y_true unique: {np.unique(y_true)}\")\n",
    "            print(f\"   y_pred unique: {np.unique(y_pred)}\")\n",
    "            # Force to binary\n",
    "            y_true = np.clip(y_true, 0, 1)\n",
    "            y_pred = np.clip(y_pred, 0, 1)\n",
    "        \n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        # Handle edge cases where only one class is predicted\n",
    "        unique_pred = np.unique(y_pred)\n",
    "        unique_true = np.unique(y_true)\n",
    "        \n",
    "        if len(unique_pred) == 1 or len(unique_true) == 1:\n",
    "            # Use macro average for edge cases\n",
    "            precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        else:\n",
    "            precision = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "        \n",
    "        # Confusion matrix - ensure we get 2x2 matrix\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "        \n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        elif cm.shape == (1, 1):\n",
    "            # Handle case where only one class exists\n",
    "            if unique_true[0] == 0 and unique_pred[0] == 0:\n",
    "                tn, fp, fn, tp = cm[0,0], 0, 0, 0\n",
    "            elif unique_true[0] == 1 and unique_pred[0] == 1:\n",
    "                tn, fp, fn, tp = 0, 0, 0, cm[0,0]\n",
    "            else:\n",
    "                tn, fp, fn, tp = 0, cm[0,0], 0, 0\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Unexpected confusion matrix shape for {model_name}: {cm.shape}\")\n",
    "            tn = fp = fn = tp = 0\n",
    "        \n",
    "        # Additional metrics\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # Same as recall\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # Negative Predictive Value\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Positive Predictive Value (same as precision)\n",
    "        \n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # False Negative Rate\n",
    "        \n",
    "        balanced_accuracy = (sensitivity + specificity) / 2\n",
    "        \n",
    "        # Matthews Correlation Coefficient\n",
    "        mcc_num = (tp * tn) - (fp * fn)\n",
    "        mcc_den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "        mcc = mcc_num / mcc_den if mcc_den > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'specificity': specificity,\n",
    "            'sensitivity': sensitivity,\n",
    "            'balanced_accuracy': balanced_accuracy,\n",
    "            'negative_predictive_value': npv,\n",
    "            'positive_predictive_value': ppv,\n",
    "            'false_positive_rate': fpr,\n",
    "            'false_negative_rate': fnr,\n",
    "            'matthews_correlation': mcc,\n",
    "            'confusion_matrix': {\n",
    "                'true_positives': int(tp),\n",
    "                'false_positives': int(fp), \n",
    "                'true_negatives': int(tn),\n",
    "                'false_negatives': int(fn)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def create_classification_dashboard(self, metrics: Dict[str, Any], df: pd.DataFrame) -> str:\n",
    "        \"\"\"Create comprehensive classification metrics dashboard\"\"\"\n",
    "        \n",
    "        if not metrics:\n",
    "            return \"<html><body><h1>No valid metrics to display</h1></body></html>\"\n",
    "        \n",
    "        flash_metrics = metrics['flash_metrics']\n",
    "        pro_metrics = metrics['pro_25_metrics']\n",
    "        dataset_info = metrics['dataset_info']\n",
    "        agreement = metrics['agreement_analysis']\n",
    "        comparison = metrics['model_comparison']\n",
    "        \n",
    "        flash_cm = flash_metrics['confusion_matrix']\n",
    "        pro_cm = pro_metrics['confusion_matrix']\n",
    "        \n",
    "        # Performance assessment\n",
    "        if pro_metrics['f1_score'] >= 0.8:\n",
    "            pro_assessment = \"üü¢ EXCELLENT\"\n",
    "            pro_color = \"#10B981\"\n",
    "        elif pro_metrics['f1_score'] >= 0.7:\n",
    "            pro_assessment = \"üü° GOOD\" \n",
    "            pro_color = \"#F59E0B\"\n",
    "        elif pro_metrics['f1_score'] >= 0.6:\n",
    "            pro_assessment = \"üü† MODERATE\"\n",
    "            pro_color = \"#FF6B35\"\n",
    "        else:\n",
    "            pro_assessment = \"üî¥ POOR\"\n",
    "            pro_color = \"#EF4444\"\n",
    "        \n",
    "        html_content = f\"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Gemini 2.5 Pro Classification Metrics Dashboard</title>\n",
    "    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/Chart.js/4.4.0/chart.min.js\"></script>\n",
    "    <style>\n",
    "        * {{ margin: 0; padding: 0; box-sizing: border-box; }}\n",
    "        body {{ font-family: 'Inter', sans-serif; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: #333; line-height: 1.6; min-height: 100vh; }}\n",
    "        .dashboard {{ max-width: 1800px; margin: 0 auto; padding: 20px; }}\n",
    "        .header {{ background: rgba(255,255,255,0.95); padding: 40px; border-radius: 25px; text-align: center; margin-bottom: 30px; backdrop-filter: blur(15px); box-shadow: 0 20px 60px rgba(0,0,0,0.1); }}\n",
    "        .header h1 {{ font-size: 3rem; margin-bottom: 15px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; }}\n",
    "        .header h2 {{ font-size: 1.5rem; color: #666; margin-bottom: 20px; }}\n",
    "        .model-badge {{ display: inline-block; padding: 10px 25px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border-radius: 30px; font-weight: bold; font-size: 1.1rem; margin: 5px; }}\n",
    "        .metrics-overview {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(180px, 1fr)); gap: 20px; margin-bottom: 40px; }}\n",
    "        .metric-card {{ background: rgba(255,255,255,0.95); padding: 25px; border-radius: 20px; text-align: center; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); transition: transform 0.3s ease; }}\n",
    "        .metric-card:hover {{ transform: translateY(-5px); }}\n",
    "        .metric-value {{ font-size: 2.5rem; font-weight: bold; margin-bottom: 10px; }}\n",
    "        .metric-label {{ font-size: 1rem; color: #666; font-weight: 600; margin-bottom: 5px; }}\n",
    "        .metric-description {{ font-size: 0.85rem; color: #888; }}\n",
    "        .excellent {{ color: #10B981; }}\n",
    "        .good {{ color: #F59E0B; }}\n",
    "        .moderate {{ color: #FF6B35; }}\n",
    "        .poor {{ color: #EF4444; }}\n",
    "        .dashboard-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(450px, 1fr)); gap: 25px; margin-bottom: 40px; }}\n",
    "        .chart-section {{ background: rgba(255,255,255,0.95); padding: 30px; border-radius: 20px; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); }}\n",
    "        .chart-title {{ font-size: 1.3rem; font-weight: bold; margin-bottom: 20px; color: #333; text-align: center; }}\n",
    "        .chart-container {{ position: relative; height: 300px; margin-bottom: 15px; }}\n",
    "        .confusion-matrix {{ display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px; margin: 20px 0; text-align: center; font-size: 0.9rem; }}\n",
    "        .cm-cell {{ padding: 15px; border-radius: 12px; font-weight: bold; display: flex; flex-direction: column; justify-content: center; align-items: center; }}\n",
    "        .cm-header {{ background: linear-gradient(135deg, #f8f9fa, #e9ecef); color: #495057; }}\n",
    "        .cm-tp {{ background: linear-gradient(135deg, #d4edda, #c3e6cb); color: #155724; font-size: 1.8rem; }}\n",
    "        .cm-fp {{ background: linear-gradient(135deg, #f8d7da, #f1b0b7); color: #721c24; font-size: 1.8rem; }}\n",
    "        .cm-fn {{ background: linear-gradient(135deg, #fff3cd, #fce4a6); color: #856404; font-size: 1.8rem; }}\n",
    "        .cm-tn {{ background: linear-gradient(135deg, #d1ecf1, #bee5eb); color: #0c5460; font-size: 1.8rem; }}\n",
    "        .cm-label {{ font-size: 0.75rem; margin-top: 5px; opacity: 0.8; }}\n",
    "        .performance-summary {{ background: linear-gradient(135deg, #e3f2fd, #bbdefb); padding: 30px; border-radius: 20px; margin: 30px 0; text-align: center; }}\n",
    "        .performance-score {{ font-size: 3.5rem; font-weight: bold; color: {pro_color}; margin-bottom: 10px; }}\n",
    "        .performance-label {{ font-size: 1.3rem; color: #37474f; margin-bottom: 8px; }}\n",
    "        .performance-description {{ font-size: 1rem; color: #546e7a; }}\n",
    "        .comparison-section {{ background: rgba(255,255,255,0.95); padding: 30px; border-radius: 20px; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); margin-bottom: 30px; }}\n",
    "        .comparison-grid {{ display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin-top: 20px; }}\n",
    "        .model-column {{ padding: 20px; border-radius: 15px; }}\n",
    "        .flash-column {{ background: linear-gradient(135deg, #e8f5e8, #d4edda); }}\n",
    "        .pro-column {{ background: linear-gradient(135deg, #e3f2fd, #bbdefb); }}\n",
    "        .model-title {{ font-size: 1.5rem; font-weight: bold; text-align: center; margin-bottom: 20px; }}\n",
    "        .metric-row {{ display: flex; justify-content: space-between; margin: 8px 0; padding: 8px; background: rgba(255,255,255,0.5); border-radius: 8px; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"dashboard\">\n",
    "        <div class=\"header\">\n",
    "            <h1>üéØ Classification Metrics Dashboard</h1>\n",
    "            <h2>Gemini Models vs Meta Ground Truth</h2>\n",
    "            <div>\n",
    "                <span class=\"model-badge\">Flash Model</span>\n",
    "                <span class=\"model-badge\">Gemini 2.5 Pro</span>\n",
    "            </div>\n",
    "            <p style=\"margin-top: 20px; color: #666;\">\n",
    "                Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | \n",
    "                Samples: {dataset_info['total_samples']:,} | \n",
    "                Positive: {dataset_info['positive_samples']} | Negative: {dataset_info['negative_samples']}\n",
    "            </p>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"performance-summary\">\n",
    "            <div class=\"performance-score\">{pro_metrics['f1_score']:.3f}</div>\n",
    "            <div class=\"performance-label\">{pro_assessment} F1-Score (Gemini 2.5 Pro)</div>\n",
    "            <div class=\"performance-description\">\n",
    "                2.5 Pro achieves {pro_metrics['accuracy']:.1%} accuracy with {pro_metrics['precision']:.1%} precision\n",
    "                <br>Judge Agreement with Flash: {agreement['pro_judge_agreement_with_flash']:.1%}\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"metrics-overview\">\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if pro_metrics['f1_score'] >= 0.8 else 'good' if pro_metrics['f1_score'] >= 0.7 else 'moderate' if pro_metrics['f1_score'] >= 0.6 else 'poor'}\">{pro_metrics['f1_score']:.3f}</div>\n",
    "                <div class=\"metric-label\">2.5 Pro F1-Score</div>\n",
    "                <div class=\"metric-description\">Harmonic Mean P&R</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if pro_metrics['recall'] >= 0.8 else 'good' if pro_metrics['recall'] >= 0.7 else 'moderate' if pro_metrics['recall'] >= 0.6 else 'poor'}\">{pro_metrics['recall']:.3f}</div>\n",
    "                <div class=\"metric-label\">2.5 Pro TPR</div>\n",
    "                <div class=\"metric-description\">True Positive Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if pro_metrics['specificity'] >= 0.8 else 'good' if pro_metrics['specificity'] >= 0.7 else 'moderate' if pro_metrics['specificity'] >= 0.6 else 'poor'}\">{pro_metrics['specificity']:.3f}</div>\n",
    "                <div class=\"metric-label\">2.5 Pro TNR</div>\n",
    "                <div class=\"metric-description\">True Negative Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if pro_metrics['false_positive_rate'] <= 0.2 else 'good' if pro_metrics['false_positive_rate'] <= 0.3 else 'moderate' if pro_metrics['false_positive_rate'] <= 0.4 else 'poor'}\">{pro_metrics['false_positive_rate']:.3f}</div>\n",
    "                <div class=\"metric-label\">2.5 Pro FPR</div>\n",
    "                <div class=\"metric-description\">False Positive Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if pro_metrics['false_negative_rate'] <= 0.2 else 'good' if pro_metrics['false_negative_rate'] <= 0.3 else 'moderate' if pro_metrics['false_negative_rate'] <= 0.4 else 'poor'}\">{pro_metrics['false_negative_rate']:.3f}</div>\n",
    "                <div class=\"metric-label\">2.5 Pro FNR</div>\n",
    "                <div class=\"metric-description\">False Negative Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if pro_metrics['accuracy'] >= 0.9 else 'good' if pro_metrics['accuracy'] >= 0.8 else 'moderate' if pro_metrics['accuracy'] >= 0.7 else 'poor'}\">{pro_metrics['accuracy']:.3f}</div>\n",
    "                <div class=\"metric-label\">2.5 Pro Accuracy</div>\n",
    "                <div class=\"metric-description\">Overall Correctness</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if pro_metrics['precision'] >= 0.8 else 'good' if pro_metrics['precision'] >= 0.7 else 'moderate' if pro_metrics['precision'] >= 0.6 else 'poor'}\">{pro_metrics['precision']:.3f}</div>\n",
    "                <div class=\"metric-label\">2.5 Pro Precision</div>\n",
    "                <div class=\"metric-description\">Positive Predictive Value</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if agreement['pro_judge_agreement_with_flash'] >= 0.8 else 'good' if agreement['pro_judge_agreement_with_flash'] >= 0.7 else 'moderate' if agreement['pro_judge_agreement_with_flash'] >= 0.6 else 'poor'}\">{agreement['pro_judge_agreement_with_flash']:.1%}</div>\n",
    "                <div class=\"metric-label\">Judge Agreement</div>\n",
    "                <div class=\"metric-description\">2.5 Pro agrees with Flash</div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"comparison-section\">\n",
    "            <div class=\"chart-title\">üìä Gemini 2.5 Pro Performance Metrics</div>\n",
    "            <div style=\"max-width: 600px; margin: 0 auto;\">\n",
    "                <div class=\"model-column pro-column\">\n",
    "                    <div class=\"model-title\">üß† Gemini 2.5 Pro vs Ground Truth</div>\n",
    "                    <div class=\"metric-row\"><span>F1-Score:</span><span class=\"{'excellent' if pro_metrics['f1_score'] >= 0.8 else 'good' if pro_metrics['f1_score'] >= 0.7 else 'moderate' if pro_metrics['f1_score'] >= 0.6 else 'poor'}\">{pro_metrics['f1_score']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Accuracy:</span><span>{pro_metrics['accuracy']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Precision:</span><span>{pro_metrics['precision']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Recall (TPR):</span><span>{pro_metrics['recall']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Specificity (TNR):</span><span>{pro_metrics['specificity']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>False Positive Rate:</span><span>{pro_metrics['false_positive_rate']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>False Negative Rate:</span><span>{pro_metrics['false_negative_rate']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Judge Agreement with Flash:</span><span>{agreement['pro_judge_agreement_with_flash']:.1%}</span></div>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"dashboard-grid\">\n",
    "            <div class=\"chart-section\">\n",
    "                <div class=\"chart-title\">üß† Gemini 2.5 Pro Confusion Matrix</div>\n",
    "                <div class=\"confusion-matrix\">\n",
    "                    <div class=\"cm-cell cm-header\"></div>\n",
    "                    <div class=\"cm-cell cm-header\">Ground Truth: 0</div>\n",
    "                    <div class=\"cm-cell cm-header\">Ground Truth: 1</div>\n",
    "                    \n",
    "                    <div class=\"cm-cell cm-header\">2.5 Pro: 0</div>\n",
    "                    <div class=\"cm-cell cm-tn\">\n",
    "                        {pro_cm['true_negatives']}\n",
    "                        <div class=\"cm-label\">True Negatives</div>\n",
    "                    </div>\n",
    "                    <div class=\"cm-cell cm-fn\">\n",
    "                        {pro_cm['false_negatives']}\n",
    "                        <div class=\"cm-label\">False Negatives</div>\n",
    "                    </div>\n",
    "                    \n",
    "                    <div class=\"cm-cell cm-header\">2.5 Pro: 1</div>\n",
    "                    <div class=\"cm-cell cm-fp\">\n",
    "                        {pro_cm['false_positives']}\n",
    "                        <div class=\"cm-label\">False Positives</div>\n",
    "                    </div>\n",
    "                    <div class=\"cm-cell cm-tp\">\n",
    "                        {pro_cm['true_positives']}\n",
    "                        <div class=\"cm-label\">True Positives</div>\n",
    "                    </div>\n",
    "                </div>\n",
    "                <div style=\"text-align: center; margin-top: 15px;\">\n",
    "                    <p><strong>TP:</strong> {pro_cm['true_positives']} | <strong>FP:</strong> {pro_cm['false_positives']} | <strong>TN:</strong> {pro_cm['true_negatives']} | <strong>FN:</strong> {pro_cm['false_negatives']}</p>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        window.addEventListener('load', function() {{\n",
    "            console.log('Gemini 2.5 Pro Classification Dashboard Loaded');\n",
    "        }});\n",
    "    </script>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "        \n",
    "        return html_content\n",
    "    \n",
    "    def generate_classification_dashboard(self):\n",
    "        \"\"\"Main function to generate the classification dashboard\"\"\"\n",
    "        print(\"üöÄ STARTING CLASSIFICATION METRICS DASHBOARD GENERATION\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        try:\n",
    "            # Load results with ground truth\n",
    "            df = self.load_results_with_ground_truth()\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"‚ùå No data available for dashboard\")\n",
    "                return None\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = self.calculate_classification_metrics(df)\n",
    "            \n",
    "            if not metrics:\n",
    "                print(\"‚ùå Could not calculate metrics\")\n",
    "                return None\n",
    "            \n",
    "            # Create dashboard\n",
    "            print(\"\\nüìä Creating Classification Dashboard...\")\n",
    "            dashboard_html = self.create_classification_dashboard(metrics, df)\n",
    "            \n",
    "            # Save files\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            dashboard_filename = f\"gemini_classification_dashboard_{timestamp}.html\"\n",
    "            metrics_filename = f\"classification_metrics_{timestamp}.json\"\n",
    "            \n",
    "            with open(dashboard_filename, \"w\", encoding='utf-8') as f:\n",
    "                f.write(dashboard_html)\n",
    "            \n",
    "            with open(metrics_filename, \"w\") as f:\n",
    "                json.dump(metrics, f, indent=2, default=str)\n",
    "            \n",
    "            print(f\"‚úÖ Dashboard saved: {dashboard_filename}\")\n",
    "            print(f\"‚úÖ Metrics saved: {metrics_filename}\")\n",
    "            \n",
    "            # Summary\n",
    "            flash_metrics = metrics['flash_metrics']\n",
    "            pro_metrics = metrics['pro_25_metrics']\n",
    "            agreement = metrics['agreement_analysis']\n",
    "            \n",
    "            print(f\"\\nüìä CLASSIFICATION METRICS SUMMARY:\")\n",
    "            print(f\"   üìä Flash Model:\")\n",
    "            print(f\"      F1-Score: {flash_metrics['f1_score']:.3f}\")\n",
    "            print(f\"      Precision: {flash_metrics['precision']:.3f}\")\n",
    "            print(f\"      Recall: {flash_metrics['recall']:.3f}\")\n",
    "            print(f\"      Accuracy: {flash_metrics['accuracy']:.3f}\")\n",
    "            print(f\"      TP: {flash_metrics['confusion_matrix']['true_positives']}, FP: {flash_metrics['confusion_matrix']['false_positives']}\")\n",
    "            print(f\"      TN: {flash_metrics['confusion_matrix']['true_negatives']}, FN: {flash_metrics['confusion_matrix']['false_negatives']}\")\n",
    "            \n",
    "            print(f\"\\n   üß† Gemini 2.5 Pro:\")\n",
    "            print(f\"      F1-Score: {pro_metrics['f1_score']:.3f}\")\n",
    "            print(f\"      Precision: {pro_metrics['precision']:.3f}\")\n",
    "            print(f\"      Recall: {pro_metrics['recall']:.3f}\") \n",
    "            print(f\"      Accuracy: {pro_metrics['accuracy']:.3f}\")\n",
    "            print(f\"      TP: {pro_metrics['confusion_matrix']['true_positives']}, FP: {pro_metrics['confusion_matrix']['false_positives']}\")\n",
    "            print(f\"      TN: {pro_metrics['confusion_matrix']['true_negatives']}, FN: {pro_metrics['confusion_matrix']['false_negatives']}\")\n",
    "            \n",
    "            print(f\"\\n   ‚öñÔ∏è Agreement Rates:\")\n",
    "            print(f\"      Flash vs Ground Truth: {agreement['flash_vs_ground_truth']:.1%}\")\n",
    "            print(f\"      2.5 Pro vs Ground Truth: {agreement['pro_vs_ground_truth']:.1%}\")\n",
    "            print(f\"      2.5 Pro Judge Agreement: {agreement['pro_judge_agreement_with_flash']:.1%}\")\n",
    "            \n",
    "            return dashboard_filename, metrics_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Dashboard generation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the classification dashboard generation\"\"\"\n",
    "    dashboard_generator = Gemini25ProClassificationDashboard()\n",
    "    result = dashboard_generator.generate_classification_dashboard()\n",
    "    \n",
    "    if result:\n",
    "        dashboard_file, metrics_file = result\n",
    "        print(f\"\\nüéâ SUCCESS!\")\n",
    "        print(f\"üìÅ Files generated:\")\n",
    "        print(f\"   1. {dashboard_file}\")\n",
    "        print(f\"   2. {metrics_file}\")\n",
    "        print(f\"\\nüí° Open {dashboard_file} in your browser to view the dashboard!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Dashboard generation failed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af779d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING GEMINI 1.5 PRO CLASSIFICATION DASHBOARD GENERATION\n",
      "======================================================================\n",
      "üì• Loading Gemini 1.5 Pro results with ground truth...\n",
      "üìä Loading Gemini 1.5 Pro results data...\n",
      "   Found 299 result records\n",
      "üìä Loading ground truth data...\n",
      "   Found 299 ground truth records\n",
      "   Ground truth distribution from Meta table: {100: 187, 0: 112}\n",
      "‚úÖ Merged dataset: 299 records\n",
      "   Sample artifact_ids from results: ['meta:1174568814711709', 'meta:1192660369325206', 'meta:1216373337197968', 'meta:122131923230709520', 'meta:1795720740984128']\n",
      "   Sample artifact_ids from meta: ['meta:122140809938790694', 'meta:4050755845195196', 'meta:1173810271458869', 'meta:1297387721748313', 'meta:1138085658345605']\n",
      "   Sample flash_classification: [0, 100, 100, 100, 0]\n",
      "   Sample ground_truth after merge: [0, 100, 100, 100, 0]\n",
      "   Flash vs Ground Truth identical BEFORE conversion: 299/299\n",
      "   üö® CRITICAL ERROR: flash_classification is identical to ground_truth!\n",
      "   This means your ground truth data IS the Flash predictions, not human annotations.\n",
      "   Check your data pipeline - Meta table might contain Flash results instead of human labels.\n",
      "   üìã Sample records to verify:\n",
      "      Record 1: artifact_id=meta:1174568814711709, flash=0, ground_truth=0\n",
      "      Record 2: artifact_id=meta:1192660369325206, flash=100, ground_truth=100\n",
      "      Record 3: artifact_id=meta:1216373337197968, flash=100, ground_truth=100\n",
      "üìä Data distribution:\n",
      "   Ground truth: {1: 187, 0: 112}\n",
      "   Flash predictions: {1: 187, 0: 112}\n",
      "   1.5 Pro predictions: {0: 186, 1: 113}\n",
      "   Judge agreement rate: 55.2%\n",
      "üìä Final clean dataset: 299 records\n",
      "\n",
      "üîç DEBUG - Sample comparisons (first 5 records):\n",
      "   Columns available: ['artifact_id', 'flash_classification', 'flash_reasoning', 'model_prompt', 'pro_judge_agreement', 'pro_verdict', 'pro_confidence', 'pro_would_reach_same_conclusion', 'pro_reasoning', 'flash_vs_pro_analysis', 'improvements', 'api_call_time', 'model_used', 'error_message', 'ground_truth', 'ground_truth_reasoning', 'source', 'ground_truth_binary', 'flash_binary', 'pro_15_prediction']\n",
      "   Record 1:\n",
      "      artifact_id: meta:1174568814711709\n",
      "      ground_truth (from Meta): 0 -> binary: 0\n",
      "      flash_classification: 0 -> binary: 0\n",
      "      pro_would_reach_same_conclusion: True\n",
      "      pro_15_prediction: 0\n",
      "      Flash vs Ground Truth: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 2:\n",
      "      artifact_id: meta:1192660369325206\n",
      "      ground_truth (from Meta): 100 -> binary: 1\n",
      "      flash_classification: 100 -> binary: 1\n",
      "      pro_would_reach_same_conclusion: False\n",
      "      pro_15_prediction: 0\n",
      "      Flash vs Ground Truth: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 3:\n",
      "      artifact_id: meta:1216373337197968\n",
      "      ground_truth (from Meta): 100 -> binary: 1\n",
      "      flash_classification: 100 -> binary: 1\n",
      "      pro_would_reach_same_conclusion: False\n",
      "      pro_15_prediction: 0\n",
      "      Flash vs Ground Truth: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 4:\n",
      "      artifact_id: meta:122131923230709520\n",
      "      ground_truth (from Meta): 100 -> binary: 1\n",
      "      flash_classification: 100 -> binary: 1\n",
      "      pro_would_reach_same_conclusion: False\n",
      "      pro_15_prediction: 0\n",
      "      Flash vs Ground Truth: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 5:\n",
      "      artifact_id: meta:1795720740984128\n",
      "      ground_truth (from Meta): 0 -> binary: 0\n",
      "      flash_classification: 0 -> binary: 0\n",
      "      pro_would_reach_same_conclusion: False\n",
      "      pro_15_prediction: 1\n",
      "      Flash vs Ground Truth: ‚úÖ MATCH\n",
      "      ---\n",
      "\n",
      "üö® CRITICAL CHECK:\n",
      "   Flash predictions matching ground truth: 299/299 (100.0%)\n",
      "   üö® ERROR: Flash predictions are 100% identical to ground truth!\n",
      "   This suggests the ground truth data is actually Flash's predictions, not human annotations.\n",
      "   Check your data pipeline - the 'ground_truth' field might be populated with Flash results.\n",
      "üìä Calculating classification metrics...\n",
      "‚úÖ Metrics calculated:\n",
      "   1.5 Pro F1: 0.553\n",
      "   Judge agreement: 55.2%\n",
      "\n",
      "üìä Creating Gemini 1.5 Pro Classification Dashboard...\n",
      "‚úÖ Dashboard saved: gemini_15_pro_classification_dashboard_20250725_135737.html\n",
      "‚úÖ Metrics saved: gemini_15_pro_classification_metrics_20250725_135737.json\n",
      "\n",
      "üìä GEMINI 1.5 PRO CLASSIFICATION METRICS SUMMARY:\n",
      "   üß† Gemini 1.5 Pro:\n",
      "      F1-Score: 0.553\n",
      "      Precision: 0.735\n",
      "      Recall: 0.444\n",
      "      Accuracy: 0.552\n",
      "      TP: 83, FP: 30\n",
      "      TN: 82, FN: 104\n",
      "\n",
      "   ‚öñÔ∏è Agreement Rates:\n",
      "      1.5 Pro vs Ground Truth: 55.2%\n",
      "      1.5 Pro Judge Agreement: 55.2%\n",
      "\n",
      "üéâ SUCCESS!\n",
      "üìÅ Files generated:\n",
      "   1. gemini_15_pro_classification_dashboard_20250725_135737.html\n",
      "   2. gemini_15_pro_classification_metrics_20250725_135737.json\n",
      "\n",
      "üí° Open gemini_15_pro_classification_dashboard_20250725_135737.html in your browser to view the dashboard!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Gemini 1.5 Pro Classification Metrics Dashboard - Real Results vs Ground Truth\n",
    "Calculates traditional ML metrics (F1, Precision, Recall, Confusion Matrix) for 1.5 Pro decisions\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "PROJECT_ID = \"scope3-dev\"\n",
    "DATASET_ID = \"research_bs_monitoring\"\n",
    "META_TABLE = \"BA_Meta_Ground_Truth\"\n",
    "RESULTS_TABLE = \"BA_Meta_Gemini_Pro_Judge_Results\"  # Gemini 1.5 Pro results table\n",
    "\n",
    "class Gemini15ProClassificationDashboard:\n",
    "    def __init__(self):\n",
    "        self.client = bigquery.Client(project=PROJECT_ID)\n",
    "        \n",
    "    def load_results_with_ground_truth(self) -> pd.DataFrame:\n",
    "        \"\"\"Load 1.5 Pro results with real ground truth from Meta table\"\"\"\n",
    "        print(\"üì• Loading Gemini 1.5 Pro results with ground truth...\")\n",
    "        \n",
    "        # First, let's get the results data\n",
    "        results_query = f\"\"\"\n",
    "        SELECT \n",
    "            artifact_id,\n",
    "            flash_classification,\n",
    "            flash_reasoning,\n",
    "            model_prompt,\n",
    "            pro_judge_agreement,\n",
    "            pro_verdict,\n",
    "            pro_confidence,\n",
    "            pro_would_reach_same_conclusion,\n",
    "            pro_reasoning,\n",
    "            flash_vs_pro_analysis,\n",
    "            improvements,\n",
    "            api_call_time,\n",
    "            model_used,\n",
    "            error_message\n",
    "        FROM `{PROJECT_ID}.{DATASET_ID}.{RESULTS_TABLE}`\n",
    "        WHERE pro_judge_agreement IS NOT NULL\n",
    "        AND pro_would_reach_same_conclusion IS NOT NULL\n",
    "        AND (error_message IS NULL OR error_message = '')\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get real ground truth from Meta table\n",
    "        meta_query = f\"\"\"\n",
    "        SELECT \n",
    "            artifact_id,\n",
    "            correct_classification as ground_truth,\n",
    "            correct_reasoning as ground_truth_reasoning,\n",
    "            source\n",
    "        FROM `{PROJECT_ID}.{DATASET_ID}.{META_TABLE}`\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(\"üìä Loading Gemini 1.5 Pro results data...\")\n",
    "            results_df = self.client.query(results_query).to_dataframe()\n",
    "            print(f\"   Found {len(results_df)} result records\")\n",
    "            \n",
    "            print(\"üìä Loading ground truth data...\")\n",
    "            meta_df = self.client.query(meta_query).to_dataframe()\n",
    "            print(f\"   Found {len(meta_df)} ground truth records\")\n",
    "            \n",
    "            # Debug: Check ground truth distribution\n",
    "            if len(meta_df) > 0:\n",
    "                gt_dist = meta_df['ground_truth'].value_counts().to_dict()\n",
    "                print(f\"   Ground truth distribution from Meta table: {gt_dist}\")\n",
    "            \n",
    "            # Join results with ground truth\n",
    "            df = results_df.merge(meta_df, on='artifact_id', how='inner')\n",
    "            print(f\"‚úÖ Merged dataset: {len(df)} records\")\n",
    "            \n",
    "            if len(df) == 0:\n",
    "                print(\"‚ùå No matching records found after merge\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Debug: Check what we actually got from the merge\n",
    "            print(f\"   Sample artifact_ids from results: {results_df['artifact_id'].head().tolist()}\")\n",
    "            print(f\"   Sample artifact_ids from meta: {meta_df['artifact_id'].head().tolist()}\")\n",
    "            print(f\"   Sample flash_classification: {results_df['flash_classification'].head().tolist()}\")\n",
    "            print(f\"   Sample ground_truth after merge: {df['ground_truth'].head().tolist()}\")\n",
    "            \n",
    "            # Check if flash_classification and ground_truth are identical BEFORE conversion\n",
    "            identical_before_conversion = (df['flash_classification'] == df['ground_truth']).sum()\n",
    "            print(f\"   Flash vs Ground Truth identical BEFORE conversion: {identical_before_conversion}/{len(df)}\")\n",
    "            \n",
    "            if identical_before_conversion == len(df):\n",
    "                print(\"   üö® CRITICAL ERROR: flash_classification is identical to ground_truth!\")\n",
    "                print(\"   This means your ground truth data IS the Flash predictions, not human annotations.\")\n",
    "                print(\"   Check your data pipeline - Meta table might contain Flash results instead of human labels.\")\n",
    "                \n",
    "                # Let's check a few individual records to confirm\n",
    "                print(\"   üìã Sample records to verify:\")\n",
    "                for i in range(min(3, len(df))):\n",
    "                    row = df.iloc[i]\n",
    "                    print(f\"      Record {i+1}: artifact_id={row['artifact_id']}, flash={row['flash_classification']}, ground_truth={row['ground_truth']}\")\n",
    "            \n",
    "            # Convert to binary format\n",
    "            df['ground_truth_binary'] = (df['ground_truth'] == 100).astype(int)\n",
    "            df['flash_binary'] = (df['flash_classification'] == 100).astype(int)\n",
    "            \n",
    "            # Convert 1.5 Pro \"would reach same conclusion\" to predictions\n",
    "            df['pro_15_prediction'] = np.where(\n",
    "                df['pro_would_reach_same_conclusion'] == True,\n",
    "                df['flash_binary'],  # Same as Flash\n",
    "                1 - df['flash_binary']  # Opposite of Flash\n",
    "            ).astype(int)\n",
    "            \n",
    "            print(f\"üìä Data distribution:\")\n",
    "            print(f\"   Ground truth: {df['ground_truth_binary'].value_counts().to_dict()}\")\n",
    "            print(f\"   Flash predictions: {df['flash_binary'].value_counts().to_dict()}\")\n",
    "            print(f\"   1.5 Pro predictions: {df['pro_15_prediction'].value_counts().to_dict()}\")\n",
    "            print(f\"   Judge agreement rate: {df['pro_judge_agreement'].mean():.1%}\")\n",
    "            \n",
    "            # Validate binary data\n",
    "            for col, name in [('ground_truth_binary', 'Ground truth'), \n",
    "                            ('flash_binary', 'Flash'), \n",
    "                            ('pro_15_prediction', '1.5 Pro')]:\n",
    "                if not all(df[col].isin([0, 1])):\n",
    "                    print(f\"‚ö†Ô∏è Warning: {name} contains non-binary values\")\n",
    "                    df = df[df[col].isin([0, 1])]\n",
    "            \n",
    "            print(f\"üìä Final clean dataset: {len(df)} records\")\n",
    "            \n",
    "            # Debug sample comparisons\n",
    "            print(f\"\\nüîç DEBUG - Sample comparisons (first 5 records):\")\n",
    "            print(f\"   Columns available: {df.columns.tolist()}\")\n",
    "            for i in range(min(5, len(df))):\n",
    "                row = df.iloc[i]\n",
    "                print(f\"   Record {i+1}:\")\n",
    "                print(f\"      artifact_id: {row['artifact_id']}\")\n",
    "                print(f\"      ground_truth (from Meta): {row['ground_truth']} -> binary: {row['ground_truth_binary']}\")\n",
    "                print(f\"      flash_classification: {row['flash_classification']} -> binary: {row['flash_binary']}\")\n",
    "                print(f\"      pro_would_reach_same_conclusion: {row['pro_would_reach_same_conclusion']}\")\n",
    "                print(f\"      pro_15_prediction: {row['pro_15_prediction']}\")\n",
    "                print(f\"      Flash vs Ground Truth: {'‚úÖ MATCH' if row['flash_binary'] == row['ground_truth_binary'] else '‚ùå DIFFER'}\")\n",
    "                print(f\"      ---\")\n",
    "            \n",
    "            # Check if Flash predictions are identical to ground truth\n",
    "            flash_matches_gt = (df['flash_binary'] == df['ground_truth_binary']).sum()\n",
    "            print(f\"\\nüö® CRITICAL CHECK:\")\n",
    "            print(f\"   Flash predictions matching ground truth: {flash_matches_gt}/{len(df)} ({flash_matches_gt/len(df):.1%})\")\n",
    "            \n",
    "            if flash_matches_gt == len(df):\n",
    "                print(\"   üö® ERROR: Flash predictions are 100% identical to ground truth!\")\n",
    "                print(\"   This suggests the ground truth data is actually Flash's predictions, not human annotations.\")\n",
    "                print(\"   Check your data pipeline - the 'ground_truth' field might be populated with Flash results.\")\n",
    "            elif flash_matches_gt > len(df) * 0.95:\n",
    "                print(\"   ‚ö†Ô∏è WARNING: Flash predictions are suspiciously similar to ground truth (>95% match)\")\n",
    "                print(\"   This might indicate data contamination.\")\n",
    "            else:\n",
    "                print(\"   ‚úÖ Good: Flash predictions differ from ground truth as expected.\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading data: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def calculate_classification_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate comprehensive classification metrics for 1.5 Pro\"\"\"\n",
    "        print(\"üìä Calculating classification metrics...\")\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            return {}\n",
    "        \n",
    "        y_true = df['ground_truth_binary'].values\n",
    "        pro_pred = df['pro_15_prediction'].values\n",
    "        \n",
    "        # 1.5 Pro metrics - Compare 1.5 Pro predictions with ground truth  \n",
    "        pro_metrics = self._calculate_model_metrics(y_true, pro_pred, \"Gemini 1.5 Pro\")\n",
    "        \n",
    "        # Agreement analysis\n",
    "        pro_vs_ground_truth_agreement = (pro_pred == y_true).mean()\n",
    "        judge_agreement_rate = df['pro_judge_agreement'].mean()\n",
    "        \n",
    "        print(f\"‚úÖ Metrics calculated:\")\n",
    "        print(f\"   1.5 Pro F1: {pro_metrics['f1_score']:.3f}\")\n",
    "        print(f\"   Judge agreement: {judge_agreement_rate:.1%}\")\n",
    "        \n",
    "        return {\n",
    "            'dataset_info': {\n",
    "                'total_samples': len(df),\n",
    "                'positive_samples': int((y_true == 1).sum()),\n",
    "                'negative_samples': int((y_true == 0).sum()),\n",
    "                'class_balance': float((y_true == 1).mean())\n",
    "            },\n",
    "            'pro_15_metrics': pro_metrics,\n",
    "            'agreement_analysis': {\n",
    "                'pro_vs_ground_truth': pro_vs_ground_truth_agreement,\n",
    "                'pro_judge_agreement_with_flash': judge_agreement_rate\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _calculate_model_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, model_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate detailed metrics for a single model\"\"\"\n",
    "        \n",
    "        # Ensure binary values\n",
    "        y_true = np.array(y_true).astype(int)\n",
    "        y_pred = np.array(y_pred).astype(int)\n",
    "        \n",
    "        # Validate binary values\n",
    "        if not (set(np.unique(y_true)) <= {0, 1} and set(np.unique(y_pred)) <= {0, 1}):\n",
    "            print(f\"‚ö†Ô∏è Warning: Non-binary values detected in {model_name}\")\n",
    "            print(f\"   y_true unique: {np.unique(y_true)}\")\n",
    "            print(f\"   y_pred unique: {np.unique(y_pred)}\")\n",
    "            # Force to binary\n",
    "            y_true = np.clip(y_true, 0, 1)\n",
    "            y_pred = np.clip(y_pred, 0, 1)\n",
    "        \n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        # Handle edge cases where only one class is predicted\n",
    "        unique_pred = np.unique(y_pred)\n",
    "        unique_true = np.unique(y_true)\n",
    "        \n",
    "        if len(unique_pred) == 1 or len(unique_true) == 1:\n",
    "            # Use macro average for edge cases\n",
    "            precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        else:\n",
    "            precision = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "        \n",
    "        # Confusion matrix - ensure we get 2x2 matrix\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "        \n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        elif cm.shape == (1, 1):\n",
    "            # Handle case where only one class exists\n",
    "            if unique_true[0] == 0 and unique_pred[0] == 0:\n",
    "                tn, fp, fn, tp = cm[0,0], 0, 0, 0\n",
    "            elif unique_true[0] == 1 and unique_pred[0] == 1:\n",
    "                tn, fp, fn, tp = 0, 0, 0, cm[0,0]\n",
    "            else:\n",
    "                tn, fp, fn, tp = 0, cm[0,0], 0, 0\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Unexpected confusion matrix shape for {model_name}: {cm.shape}\")\n",
    "            tn = fp = fn = tp = 0\n",
    "        \n",
    "        # Additional metrics\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # Same as recall\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # Negative Predictive Value\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Positive Predictive Value (same as precision)\n",
    "        \n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # False Negative Rate\n",
    "        \n",
    "        balanced_accuracy = (sensitivity + specificity) / 2\n",
    "        \n",
    "        # Matthews Correlation Coefficient\n",
    "        mcc_num = (tp * tn) - (fp * fn)\n",
    "        mcc_den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "        mcc = mcc_num / mcc_den if mcc_den > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'specificity': specificity,\n",
    "            'sensitivity': sensitivity,\n",
    "            'balanced_accuracy': balanced_accuracy,\n",
    "            'negative_predictive_value': npv,\n",
    "            'positive_predictive_value': ppv,\n",
    "            'false_positive_rate': fpr,\n",
    "            'false_negative_rate': fnr,\n",
    "            'matthews_correlation': mcc,\n",
    "            'confusion_matrix': {\n",
    "                'true_positives': int(tp),\n",
    "                'false_positives': int(fp), \n",
    "                'true_negatives': int(tn),\n",
    "                'false_negatives': int(fn)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def create_classification_dashboard(self, metrics: Dict[str, Any], df: pd.DataFrame) -> str:\n",
    "        \"\"\"Create comprehensive classification metrics dashboard\"\"\"\n",
    "        \n",
    "        if not metrics:\n",
    "            return \"<html><body><h1>No valid metrics to display</h1></body></html>\"\n",
    "        \n",
    "        pro_metrics = metrics['pro_15_metrics']\n",
    "        dataset_info = metrics['dataset_info']\n",
    "        agreement = metrics['agreement_analysis']\n",
    "        \n",
    "        pro_cm = pro_metrics['confusion_matrix']\n",
    "        \n",
    "        # Performance assessment\n",
    "        if pro_metrics['f1_score'] >= 0.8:\n",
    "            pro_assessment = \"üü¢ EXCELLENT\"\n",
    "            pro_color = \"#10B981\"\n",
    "        elif pro_metrics['f1_score'] >= 0.7:\n",
    "            pro_assessment = \"üü° GOOD\" \n",
    "            pro_color = \"#F59E0B\"\n",
    "        elif pro_metrics['f1_score'] >= 0.6:\n",
    "            pro_assessment = \"üü† MODERATE\"\n",
    "            pro_color = \"#FF6B35\"\n",
    "        else:\n",
    "            pro_assessment = \"üî¥ POOR\"\n",
    "            pro_color = \"#EF4444\"\n",
    "        \n",
    "        html_content = f\"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Gemini 1.5 Pro Classification Metrics Dashboard</title>\n",
    "    <style>\n",
    "        * {{ margin: 0; padding: 0; box-sizing: border-box; }}\n",
    "        body {{ font-family: 'Inter', sans-serif; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: #333; line-height: 1.6; min-height: 100vh; }}\n",
    "        .dashboard {{ max-width: 1800px; margin: 0 auto; padding: 20px; }}\n",
    "        .header {{ background: rgba(255,255,255,0.95); padding: 40px; border-radius: 25px; text-align: center; margin-bottom: 30px; backdrop-filter: blur(15px); box-shadow: 0 20px 60px rgba(0,0,0,0.1); }}\n",
    "        .header h1 {{ font-size: 3rem; margin-bottom: 15px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; }}\n",
    "        .header h2 {{ font-size: 1.5rem; color: #666; margin-bottom: 20px; }}\n",
    "        .model-badge {{ display: inline-block; padding: 10px 25px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border-radius: 30px; font-weight: bold; font-size: 1.1rem; margin: 5px; }}\n",
    "        .metrics-overview {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(180px, 1fr)); gap: 20px; margin-bottom: 40px; }}\n",
    "        .metric-card {{ background: rgba(255,255,255,0.95); padding: 25px; border-radius: 20px; text-align: center; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); transition: transform 0.3s ease; }}\n",
    "        .metric-card:hover {{ transform: translateY(-5px); }}\n",
    "        .metric-value {{ font-size: 2.5rem; font-weight: bold; margin-bottom: 10px; }}\n",
    "        .metric-label {{ font-size: 1rem; color: #666; font-weight: 600; margin-bottom: 5px; }}\n",
    "        .metric-description {{ font-size: 0.85rem; color: #888; }}\n",
    "        .excellent {{ color: #10B981; }}\n",
    "        .good {{ color: #F59E0B; }}\n",
    "        .moderate {{ color: #FF6B35; }}\n",
    "        .poor {{ color: #EF4444; }}\n",
    "        .dashboard-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(450px, 1fr)); gap: 25px; margin-bottom: 40px; }}\n",
    "        .chart-section {{ background: rgba(255,255,255,0.95); padding: 30px; border-radius: 20px; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); }}\n",
    "        .chart-title {{ font-size: 1.3rem; font-weight: bold; margin-bottom: 20px; color: #333; text-align: center; }}\n",
    "        .confusion-matrix {{ display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px; margin: 20px 0; text-align: center; font-size: 0.9rem; }}\n",
    "        .cm-cell {{ padding: 15px; border-radius: 12px; font-weight: bold; display: flex; flex-direction: column; justify-content: center; align-items: center; }}\n",
    "        .cm-header {{ background: linear-gradient(135deg, #f8f9fa, #e9ecef); color: #495057; }}\n",
    "        .cm-tp {{ background: linear-gradient(135deg, #d4edda, #c3e6cb); color: #155724; font-size: 1.8rem; }}\n",
    "        .cm-fp {{ background: linear-gradient(135deg, #f8d7da, #f1b0b7); color: #721c24; font-size: 1.8rem; }}\n",
    "        .cm-fn {{ background: linear-gradient(135deg, #fff3cd, #fce4a6); color: #856404; font-size: 1.8rem; }}\n",
    "        .cm-tn {{ background: linear-gradient(135deg, #d1ecf1, #bee5eb); color: #0c5460; font-size: 1.8rem; }}\n",
    "        .cm-label {{ font-size: 0.75rem; margin-top: 5px; opacity: 0.8; }}\n",
    "        .performance-summary {{ background: linear-gradient(135deg, #e3f2fd, #bbdefb); padding: 30px; border-radius: 20px; margin: 30px 0; text-align: center; }}\n",
    "        .performance-score {{ font-size: 3.5rem; font-weight: bold; color: {pro_color}; margin-bottom: 10px; }}\n",
    "        .performance-label {{ font-size: 1.3rem; color: #37474f; margin-bottom: 8px; }}\n",
    "        .performance-description {{ font-size: 1rem; color: #546e7a; }}\n",
    "        .comparison-section {{ background: rgba(255,255,255,0.95); padding: 30px; border-radius: 20px; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); margin-bottom: 30px; }}\n",
    "        .model-column {{ padding: 20px; border-radius: 15px; }}\n",
    "        .pro-column {{ background: linear-gradient(135deg, #e3f2fd, #bbdefb); }}\n",
    "        .model-title {{ font-size: 1.5rem; font-weight: bold; text-align: center; margin-bottom: 20px; }}\n",
    "        .metric-row {{ display: flex; justify-content: space-between; margin: 8px 0; padding: 8px; background: rgba(255,255,255,0.5); border-radius: 8px; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"dashboard\">\n",
    "        <div class=\"header\">\n",
    "            <h1>üéØ Gemini 1.5 Pro Classification Metrics</h1>\n",
    "            <h2>Performance vs Meta Ground Truth</h2>\n",
    "            <div>\n",
    "                <span class=\"model-badge\">Gemini 1.5 Pro</span>\n",
    "            </div>\n",
    "            <p style=\"margin-top: 20px; color: #666;\">\n",
    "                Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | \n",
    "                Samples: {dataset_info['total_samples']:,} | \n",
    "                Positive: {dataset_info['positive_samples']} | Negative: {dataset_info['negative_samples']}\n",
    "            </p>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"performance-summary\">\n",
    "            <div class=\"performance-score\">{pro_metrics['f1_score']:.3f}</div>\n",
    "            <div class=\"performance-label\">{pro_assessment} F1-Score (Gemini 1.5 Pro)</div>\n",
    "            <div class=\"performance-description\">\n",
    "                1.5 Pro achieves {pro_metrics['accuracy']:.1%} accuracy with {pro_metrics['precision']:.1%} precision\n",
    "                <br>Judge Agreement with Flash: {agreement['pro_judge_agreement_with_flash']:.1%}\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"metrics-overview\">\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if pro_metrics['f1_score'] >= 0.8 else 'good' if pro_metrics['f1_score'] >= 0.7 else 'moderate' if pro_metrics['f1_score'] >= 0.6 else 'poor'}\">{pro_metrics['f1_score']:.3f}</div>\n",
    "                <div class=\"metric-label\">1.5 Pro F1-Score</div>\n",
    "                <div class=\"metric-description\">Harmonic Mean P&R</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if pro_metrics['recall'] >= 0.8 else 'good' if pro_metrics['recall'] >= 0.7 else 'moderate' if pro_metrics['recall'] >= 0.6 else 'poor'}\">{pro_metrics['recall']:.3f}</div>\n",
    "                <div class=\"metric-label\">1.5 Pro TPR</div>\n",
    "                <div class=\"metric-description\">True Positive Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if pro_metrics['specificity'] >= 0.8 else 'good' if pro_metrics['specificity'] >= 0.7 else 'moderate' if pro_metrics['specificity'] >= 0.6 else 'poor'}\">{pro_metrics['specificity']:.3f}</div>\n",
    "                <div class=\"metric-label\">1.5 Pro TNR</div>\n",
    "                <div class=\"metric-description\">True Negative Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if pro_metrics['false_positive_rate'] <= 0.2 else 'good' if pro_metrics['false_positive_rate'] <= 0.3 else 'moderate' if pro_metrics['false_positive_rate'] <= 0.4 else 'poor'}\">{pro_metrics['false_positive_rate']:.3f}</div>\n",
    "                <div class=\"metric-label\">1.5 Pro FPR</div>\n",
    "                <div class=\"metric-description\">False Positive Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if pro_metrics['false_negative_rate'] <= 0.2 else 'good' if pro_metrics['false_negative_rate'] <= 0.3 else 'moderate' if pro_metrics['false_negative_rate'] <= 0.4 else 'poor'}\">{pro_metrics['false_negative_rate']:.3f}</div>\n",
    "                <div class=\"metric-label\">1.5 Pro FNR</div>\n",
    "                <div class=\"metric-description\">False Negative Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if pro_metrics['accuracy'] >= 0.9 else 'good' if pro_metrics['accuracy'] >= 0.8 else 'moderate' if pro_metrics['accuracy'] >= 0.7 else 'poor'}\">{pro_metrics['accuracy']:.3f}</div>\n",
    "                <div class=\"metric-label\">1.5 Pro Accuracy</div>\n",
    "                <div class=\"metric-description\">Overall Correctness</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if pro_metrics['precision'] >= 0.8 else 'good' if pro_metrics['precision'] >= 0.7 else 'moderate' if pro_metrics['precision'] >= 0.6 else 'poor'}\">{pro_metrics['precision']:.3f}</div>\n",
    "                <div class=\"metric-label\">1.5 Pro Precision</div>\n",
    "                <div class=\"metric-description\">Positive Predictive Value</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if agreement['pro_judge_agreement_with_flash'] >= 0.8 else 'good' if agreement['pro_judge_agreement_with_flash'] >= 0.7 else 'moderate' if agreement['pro_judge_agreement_with_flash'] >= 0.6 else 'poor'}\">{agreement['pro_judge_agreement_with_flash']:.1%}</div>\n",
    "                <div class=\"metric-label\">Judge Agreement</div>\n",
    "                <div class=\"metric-description\">1.5 Pro agrees with Flash</div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"comparison-section\">\n",
    "            <div class=\"chart-title\">üìä Gemini 1.5 Pro Performance Metrics</div>\n",
    "            <div style=\"max-width: 600px; margin: 0 auto;\">\n",
    "                <div class=\"model-column pro-column\">\n",
    "                    <div class=\"model-title\">üß† Gemini 1.5 Pro vs Ground Truth</div>\n",
    "                    <div class=\"metric-row\"><span>F1-Score:</span><span class=\"{'excellent' if pro_metrics['f1_score'] >= 0.8 else 'good' if pro_metrics['f1_score'] >= 0.7 else 'moderate' if pro_metrics['f1_score'] >= 0.6 else 'poor'}\">{pro_metrics['f1_score']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Accuracy:</span><span>{pro_metrics['accuracy']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Precision:</span><span>{pro_metrics['precision']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Recall (TPR):</span><span>{pro_metrics['recall']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Specificity (TNR):</span><span>{pro_metrics['specificity']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>False Positive Rate:</span><span>{pro_metrics['false_positive_rate']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>False Negative Rate:</span><span>{pro_metrics['false_negative_rate']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Judge Agreement with Flash:</span><span>{agreement['pro_judge_agreement_with_flash']:.1%}</span></div>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"dashboard-grid\">\n",
    "            <div class=\"chart-section\">\n",
    "                <div class=\"chart-title\">üß† Gemini 1.5 Pro Confusion Matrix</div>\n",
    "                <div class=\"confusion-matrix\">\n",
    "                    <div class=\"cm-cell cm-header\"></div>\n",
    "                    <div class=\"cm-cell cm-header\">Ground Truth: 0</div>\n",
    "                    <div class=\"cm-cell cm-header\">Ground Truth: 1</div>\n",
    "                    \n",
    "                    <div class=\"cm-cell cm-header\">1.5 Pro: 0</div>\n",
    "                    <div class=\"cm-cell cm-tn\">\n",
    "                        {pro_cm['true_negatives']}\n",
    "                        <div class=\"cm-label\">True Negatives</div>\n",
    "                    </div>\n",
    "                    <div class=\"cm-cell cm-fn\">\n",
    "                        {pro_cm['false_negatives']}\n",
    "                        <div class=\"cm-label\">False Negatives</div>\n",
    "                    </div>\n",
    "                    \n",
    "                    <div class=\"cm-cell cm-header\">1.5 Pro: 1</div>\n",
    "                    <div class=\"cm-cell cm-fp\">\n",
    "                        {pro_cm['false_positives']}\n",
    "                        <div class=\"cm-label\">False Positives</div>\n",
    "                    </div>\n",
    "                    <div class=\"cm-cell cm-tp\">\n",
    "                        {pro_cm['true_positives']}\n",
    "                        <div class=\"cm-label\">True Positives</div>\n",
    "                    </div>\n",
    "                </div>\n",
    "                <div style=\"text-align: center; margin-top: 15px;\">\n",
    "                    <p><strong>TP:</strong> {pro_cm['true_positives']} | <strong>FP:</strong> {pro_cm['false_positives']} | <strong>TN:</strong> {pro_cm['true_negatives']} | <strong>FN:</strong> {pro_cm['false_negatives']}</p>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        window.addEventListener('load', function() {{\n",
    "            console.log('Gemini 1.5 Pro Classification Dashboard Loaded');\n",
    "        }});\n",
    "    </script>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "        \n",
    "        return html_content\n",
    "    \n",
    "    def generate_classification_dashboard(self):\n",
    "        \"\"\"Main function to generate the classification dashboard\"\"\"\n",
    "        print(\"üöÄ STARTING GEMINI 1.5 PRO CLASSIFICATION DASHBOARD GENERATION\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        try:\n",
    "            # Load results with ground truth\n",
    "            df = self.load_results_with_ground_truth()\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"‚ùå No data available for dashboard\")\n",
    "                return None\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = self.calculate_classification_metrics(df)\n",
    "            \n",
    "            if not metrics:\n",
    "                print(\"‚ùå Could not calculate metrics\")\n",
    "                return None\n",
    "            \n",
    "            # Create dashboard\n",
    "            print(\"\\nüìä Creating Gemini 1.5 Pro Classification Dashboard...\")\n",
    "            dashboard_html = self.create_classification_dashboard(metrics, df)\n",
    "            \n",
    "            # Save files\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            dashboard_filename = f\"gemini_15_pro_classification_dashboard_{timestamp}.html\"\n",
    "            metrics_filename = f\"gemini_15_pro_classification_metrics_{timestamp}.json\"\n",
    "            \n",
    "            with open(dashboard_filename, \"w\", encoding='utf-8') as f:\n",
    "                f.write(dashboard_html)\n",
    "            \n",
    "            with open(metrics_filename, \"w\") as f:\n",
    "                json.dump(metrics, f, indent=2, default=str)\n",
    "            \n",
    "            print(f\"‚úÖ Dashboard saved: {dashboard_filename}\")\n",
    "            print(f\"‚úÖ Metrics saved: {metrics_filename}\")\n",
    "            \n",
    "            # Summary\n",
    "            pro_metrics = metrics['pro_15_metrics']\n",
    "            agreement = metrics['agreement_analysis']\n",
    "            \n",
    "            print(f\"\\nüìä GEMINI 1.5 PRO CLASSIFICATION METRICS SUMMARY:\")\n",
    "            print(f\"   üß† Gemini 1.5 Pro:\")\n",
    "            print(f\"      F1-Score: {pro_metrics['f1_score']:.3f}\")\n",
    "            print(f\"      Precision: {pro_metrics['precision']:.3f}\")\n",
    "            print(f\"      Recall: {pro_metrics['recall']:.3f}\") \n",
    "            print(f\"      Accuracy: {pro_metrics['accuracy']:.3f}\")\n",
    "            print(f\"      TP: {pro_metrics['confusion_matrix']['true_positives']}, FP: {pro_metrics['confusion_matrix']['false_positives']}\")\n",
    "            print(f\"      TN: {pro_metrics['confusion_matrix']['true_negatives']}, FN: {pro_metrics['confusion_matrix']['false_negatives']}\")\n",
    "            \n",
    "            print(f\"\\n   ‚öñÔ∏è Agreement Rates:\")\n",
    "            print(f\"      1.5 Pro vs Ground Truth: {agreement['pro_vs_ground_truth']:.1%}\")\n",
    "            print(f\"      1.5 Pro Judge Agreement: {agreement['pro_judge_agreement_with_flash']:.1%}\")\n",
    "            \n",
    "            return dashboard_filename, metrics_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Dashboard generation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the Gemini 1.5 Pro classification dashboard generation\"\"\"\n",
    "    dashboard_generator = Gemini15ProClassificationDashboard()\n",
    "    result = dashboard_generator.generate_classification_dashboard()\n",
    "    \n",
    "    if result:\n",
    "        dashboard_file, metrics_file = result\n",
    "        print(f\"\\nüéâ SUCCESS!\")\n",
    "        print(f\"üìÅ Files generated:\")\n",
    "        print(f\"   1. {dashboard_file}\")\n",
    "        print(f\"   2. {metrics_file}\")\n",
    "        print(f\"\\nüí° Open {dashboard_file} in your browser to view the dashboard!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Dashboard generation failed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b13f236d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING WEB GEMINI PRO CLASSIFICATION DASHBOARD GENERATION\n",
      "======================================================================\n",
      "üì• Loading Web Gemini Pro results with ground truth...\n",
      "üìä Loading Web Gemini Pro results data...\n",
      "   Found 294 result records\n",
      "üìä Loading Web ground truth data...\n",
      "   Found 294 web ground truth records\n",
      "   Web ground truth distribution: {100: 252, 0: 42}\n",
      "‚úÖ Merged dataset: 294 records\n",
      "   Sample artifact_ids from results: ['news.de/gesundheit/855915949/corona-zahlen-kreisfreie-stadt-solingen-heute-aktuell-27-06-2025-coronavirus-news-zu-rki-fallzahlen-tote-in-nordrhein-westfalen-intensivbetten-auslastung-und-neue-covid-19-variante-nimbus/1', 'medicinenet.com/how_do_i_know_if_i_have_damaged_my_rotator_cuff/article.htm', 'michigansthumb.com/news/article/Gagetown-Elementary-hosts-wax-museum-7356734.php', 'linuxiac.com/incus-6-13-container-and-virtual-machine-manager-released', 'liveworksheets.com/node/6694649']\n",
      "   Sample artifact_ids from web GT: ['news.de/gesundheit/855915949/corona-zahlen-kreisfreie-stadt-solingen-heute-aktuell-27-06-2025-coronavirus-news-zu-rki-fallzahlen-tote-in-nordrhein-westfalen-intensivbetten-auslastung-und-neue-covid-19-variante-nimbus/1', 'bizjournals.com/sanfrancisco/news/2025/06/27/genentech-ulcerative-colitis-vixarelimab-ibd-osmr.html', 'scmp.com/lifestyle/food-drink/article/3315878/health-benefits-superfood-prickly-pear-cactus-spiking-popularity-japan', 'plejada.pl/newsy/bruce-willis-zniknal-z-powodu-choroby-poczatkowo-zostal-zle-zdiagnozowany/2cdfxfw', 'askamanager.org/2018/09/my-coworker-keeps-making-me-do-my-old-job-manager-doesnt-believe-im-sick-and-more.html']\n",
      "   Sample flash_classification: [0, 100, 100, 100, 100]\n",
      "   Sample web ground_truth after merge: [0, 100, 100, 100, 100]\n",
      "   Flash vs Web Ground Truth identical BEFORE conversion: 294/294\n",
      "   üö® CRITICAL ERROR: flash_classification is identical to web ground_truth!\n",
      "   This means your web ground truth data IS the Flash predictions, not human annotations.\n",
      "   Check your data pipeline - Web GT table might contain Flash results instead of human labels.\n",
      "   üìã Sample records to verify:\n",
      "      Record 1: artifact_id=news.de/gesundheit/855915949/corona-zahlen-kreisfreie-stadt-solingen-heute-aktuell-27-06-2025-coronavirus-news-zu-rki-fallzahlen-tote-in-nordrhein-westfalen-intensivbetten-auslastung-und-neue-covid-19-variante-nimbus/1, flash=0, web_gt=0\n",
      "      Record 2: artifact_id=medicinenet.com/how_do_i_know_if_i_have_damaged_my_rotator_cuff/article.htm, flash=100, web_gt=100\n",
      "      Record 3: artifact_id=michigansthumb.com/news/article/Gagetown-Elementary-hosts-wax-museum-7356734.php, flash=100, web_gt=100\n",
      "üìä Data distribution:\n",
      "   Web ground truth: {1: 252, 0: 42}\n",
      "   Flash predictions: {1: 252, 0: 42}\n",
      "   Web Gemini 1.5 predictions: {1: 216, 0: 78}\n",
      "   Judge agreement rate: 80.3%\n",
      "üìä Final clean dataset: 294 records\n",
      "\n",
      "üîç DEBUG - Sample comparisons (first 5 records):\n",
      "   Columns available: ['artifact_id', 'flash_classification', 'flash_reasoning', 'model_prompt', 'pro_judge_agreement', 'pro_verdict', 'pro_confidence', 'pro_would_reach_same_conclusion', 'pro_reasoning', 'flash_vs_pro_analysis', 'improvements', 'api_call_time', 'model_used', 'error_message', 'ground_truth', 'ground_truth_reasoning', 'source', 'ground_truth_binary', 'flash_binary', 'gemini_15_web_prediction']\n",
      "   Record 1:\n",
      "      artifact_id: news.de/gesundheit/855915949/corona-zahlen-kreisfreie-stadt-solingen-heute-aktuell-27-06-2025-coronavirus-news-zu-rki-fallzahlen-tote-in-nordrhein-westfalen-intensivbetten-auslastung-und-neue-covid-19-variante-nimbus/1\n",
      "      web_ground_truth (from Web GT): 0 -> binary: 0\n",
      "      flash_classification: 0 -> binary: 0\n",
      "      pro_would_reach_same_conclusion: False\n",
      "      gemini_15_web_prediction: 1\n",
      "      Flash vs Web GT: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 2:\n",
      "      artifact_id: medicinenet.com/how_do_i_know_if_i_have_damaged_my_rotator_cuff/article.htm\n",
      "      web_ground_truth (from Web GT): 100 -> binary: 1\n",
      "      flash_classification: 100 -> binary: 1\n",
      "      pro_would_reach_same_conclusion: True\n",
      "      gemini_15_web_prediction: 1\n",
      "      Flash vs Web GT: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 3:\n",
      "      artifact_id: michigansthumb.com/news/article/Gagetown-Elementary-hosts-wax-museum-7356734.php\n",
      "      web_ground_truth (from Web GT): 100 -> binary: 1\n",
      "      flash_classification: 100 -> binary: 1\n",
      "      pro_would_reach_same_conclusion: False\n",
      "      gemini_15_web_prediction: 0\n",
      "      Flash vs Web GT: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 4:\n",
      "      artifact_id: linuxiac.com/incus-6-13-container-and-virtual-machine-manager-released\n",
      "      web_ground_truth (from Web GT): 100 -> binary: 1\n",
      "      flash_classification: 100 -> binary: 1\n",
      "      pro_would_reach_same_conclusion: True\n",
      "      gemini_15_web_prediction: 1\n",
      "      Flash vs Web GT: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 5:\n",
      "      artifact_id: liveworksheets.com/node/6694649\n",
      "      web_ground_truth (from Web GT): 100 -> binary: 1\n",
      "      flash_classification: 100 -> binary: 1\n",
      "      pro_would_reach_same_conclusion: True\n",
      "      gemini_15_web_prediction: 1\n",
      "      Flash vs Web GT: ‚úÖ MATCH\n",
      "      ---\n",
      "\n",
      "üö® CRITICAL CHECK:\n",
      "   Flash predictions matching web ground truth: 294/294 (100.0%)\n",
      "   üö® ERROR: Flash predictions are 100% identical to web ground truth!\n",
      "   This suggests the web ground truth data is actually Flash's predictions, not human annotations.\n",
      "   Check your data pipeline - the 'ground_truth' field might be populated with Flash results.\n",
      "üìä Calculating classification metrics...\n",
      "‚úÖ Metrics calculated:\n",
      "   Web Gemini 1.5 F1: 0.876\n",
      "   Judge agreement: 80.3%\n",
      "\n",
      "üìä Creating Web Gemini Pro Classification Dashboard...\n",
      "‚úÖ Dashboard saved: web_gemini_pro_classification_dashboard_20250725_135740.html\n",
      "‚úÖ Metrics saved: web_gemini_pro_classification_metrics_20250725_135740.json\n",
      "\n",
      "üìä WEB GEMINI 1.5 CLASSIFICATION METRICS SUMMARY:\n",
      "   üåê Web Gemini 1.5:\n",
      "      F1-Score: 0.876\n",
      "      Precision: 0.949\n",
      "      Recall: 0.813\n",
      "      Accuracy: 0.803\n",
      "      TP: 205, FP: 11\n",
      "      TN: 31, FN: 47\n",
      "\n",
      "   ‚öñÔ∏è Agreement Rates:\n",
      "      Web 1.5 vs Web Ground Truth: 80.3%\n",
      "      Web 1.5 Judge Agreement: 80.3%\n",
      "\n",
      "üéâ SUCCESS!\n",
      "üìÅ Files generated:\n",
      "   1. web_gemini_pro_classification_dashboard_20250725_135740.html\n",
      "   2. web_gemini_pro_classification_metrics_20250725_135740.json\n",
      "\n",
      "üí° Open web_gemini_pro_classification_dashboard_20250725_135740.html in your browser to view the dashboard!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Web Gemini Pro Classification Metrics Dashboard - Real Results vs Web Ground Truth\n",
    "Calculates traditional ML metrics (F1, Precision, Recall, Confusion Matrix) for Gemini Pro decisions on Web content\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "PROJECT_ID = \"scope3-dev\"\n",
    "DATASET_ID = \"research_bs_monitoring\"\n",
    "WEB_GROUND_TRUTH_TABLE = \"BA_Web_Ground_Truth\"  # Web ground truth table\n",
    "WEB_RESULTS_TABLE = \"BA_Web_Gemini_Pro_Judge_Results\"  # Web Gemini Pro results table\n",
    "\n",
    "class WebGeminiProClassificationDashboard:\n",
    "    def __init__(self):\n",
    "        self.client = bigquery.Client(project=PROJECT_ID)\n",
    "        \n",
    "    def load_results_with_ground_truth(self) -> pd.DataFrame:\n",
    "        \"\"\"Load Web Gemini Pro results with real ground truth from Web Ground Truth table\"\"\"\n",
    "        print(\"üì• Loading Web Gemini Pro results with ground truth...\")\n",
    "        \n",
    "        # First, let's get the results data\n",
    "        results_query = f\"\"\"\n",
    "        SELECT \n",
    "            artifact_id,\n",
    "            flash_classification,\n",
    "            flash_reasoning,\n",
    "            model_prompt,\n",
    "            pro_judge_agreement,\n",
    "            pro_verdict,\n",
    "            pro_confidence,\n",
    "            pro_would_reach_same_conclusion,\n",
    "            pro_reasoning,\n",
    "            flash_vs_pro_analysis,\n",
    "            improvements,\n",
    "            api_call_time,\n",
    "            model_used,\n",
    "            error_message\n",
    "        FROM `{PROJECT_ID}.{DATASET_ID}.{WEB_RESULTS_TABLE}`\n",
    "        WHERE pro_judge_agreement IS NOT NULL\n",
    "        AND pro_would_reach_same_conclusion IS NOT NULL\n",
    "        AND (error_message IS NULL OR error_message = '')\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get real ground truth from Web Ground Truth table\n",
    "        web_gt_query = f\"\"\"\n",
    "        SELECT \n",
    "            artifact_id,\n",
    "            correct_classification as ground_truth,\n",
    "            correct_reasoning as ground_truth_reasoning,\n",
    "            source\n",
    "        FROM `{PROJECT_ID}.{DATASET_ID}.{WEB_GROUND_TRUTH_TABLE}`\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(\"üìä Loading Web Gemini Pro results data...\")\n",
    "            results_df = self.client.query(results_query).to_dataframe()\n",
    "            print(f\"   Found {len(results_df)} result records\")\n",
    "            \n",
    "            print(\"üìä Loading Web ground truth data...\")\n",
    "            web_gt_df = self.client.query(web_gt_query).to_dataframe()\n",
    "            print(f\"   Found {len(web_gt_df)} web ground truth records\")\n",
    "            \n",
    "            # Debug: Check ground truth distribution\n",
    "            if len(web_gt_df) > 0:\n",
    "                gt_dist = web_gt_df['ground_truth'].value_counts().to_dict()\n",
    "                print(f\"   Web ground truth distribution: {gt_dist}\")\n",
    "            \n",
    "            # Join results with ground truth\n",
    "            df = results_df.merge(web_gt_df, on='artifact_id', how='inner')\n",
    "            print(f\"‚úÖ Merged dataset: {len(df)} records\")\n",
    "            \n",
    "            if len(df) == 0:\n",
    "                print(\"‚ùå No matching records found after merge\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Debug: Check what we actually got from the merge\n",
    "            print(f\"   Sample artifact_ids from results: {results_df['artifact_id'].head().tolist()}\")\n",
    "            print(f\"   Sample artifact_ids from web GT: {web_gt_df['artifact_id'].head().tolist()}\")\n",
    "            print(f\"   Sample flash_classification: {results_df['flash_classification'].head().tolist()}\")\n",
    "            print(f\"   Sample web ground_truth after merge: {df['ground_truth'].head().tolist()}\")\n",
    "            \n",
    "            # Check if flash_classification and ground_truth are identical BEFORE conversion\n",
    "            identical_before_conversion = (df['flash_classification'] == df['ground_truth']).sum()\n",
    "            print(f\"   Flash vs Web Ground Truth identical BEFORE conversion: {identical_before_conversion}/{len(df)}\")\n",
    "            \n",
    "            if identical_before_conversion == len(df):\n",
    "                print(\"   üö® CRITICAL ERROR: flash_classification is identical to web ground_truth!\")\n",
    "                print(\"   This means your web ground truth data IS the Flash predictions, not human annotations.\")\n",
    "                print(\"   Check your data pipeline - Web GT table might contain Flash results instead of human labels.\")\n",
    "                \n",
    "                # Let's check a few individual records to confirm\n",
    "                print(\"   üìã Sample records to verify:\")\n",
    "                for i in range(min(3, len(df))):\n",
    "                    row = df.iloc[i]\n",
    "                    print(f\"      Record {i+1}: artifact_id={row['artifact_id']}, flash={row['flash_classification']}, web_gt={row['ground_truth']}\")\n",
    "            \n",
    "            # Convert to binary format\n",
    "            df['ground_truth_binary'] = (df['ground_truth'] == 100).astype(int)\n",
    "            df['flash_binary'] = (df['flash_classification'] == 100).astype(int)\n",
    "            \n",
    "            # Convert Gemini 1.5 \"would reach same conclusion\" to predictions\n",
    "            df['gemini_15_web_prediction'] = np.where(\n",
    "                df['pro_would_reach_same_conclusion'] == True,\n",
    "                df['flash_binary'],  # Same as Flash\n",
    "                1 - df['flash_binary']  # Opposite of Flash\n",
    "            ).astype(int)\n",
    "            \n",
    "            print(f\"üìä Data distribution:\")\n",
    "            print(f\"   Web ground truth: {df['ground_truth_binary'].value_counts().to_dict()}\")\n",
    "            print(f\"   Flash predictions: {df['flash_binary'].value_counts().to_dict()}\")\n",
    "            print(f\"   Web Gemini 1.5 predictions: {df['gemini_15_web_prediction'].value_counts().to_dict()}\")\n",
    "            print(f\"   Judge agreement rate: {df['pro_judge_agreement'].mean():.1%}\")\n",
    "            \n",
    "            # Validate binary data\n",
    "            for col, name in [('ground_truth_binary', 'Web ground truth'), \n",
    "                            ('flash_binary', 'Flash'), \n",
    "                            ('gemini_15_web_prediction', 'Web Gemini 1.5')]:\n",
    "                if not all(df[col].isin([0, 1])):\n",
    "                    print(f\"‚ö†Ô∏è Warning: {name} contains non-binary values\")\n",
    "                    df = df[df[col].isin([0, 1])]\n",
    "            \n",
    "            print(f\"üìä Final clean dataset: {len(df)} records\")\n",
    "            \n",
    "            # Debug sample comparisons\n",
    "            print(f\"\\nüîç DEBUG - Sample comparisons (first 5 records):\")\n",
    "            print(f\"   Columns available: {df.columns.tolist()}\")\n",
    "            for i in range(min(5, len(df))):\n",
    "                row = df.iloc[i]\n",
    "                print(f\"   Record {i+1}:\")\n",
    "                print(f\"      artifact_id: {row['artifact_id']}\")\n",
    "                print(f\"      web_ground_truth (from Web GT): {row['ground_truth']} -> binary: {row['ground_truth_binary']}\")\n",
    "                print(f\"      flash_classification: {row['flash_classification']} -> binary: {row['flash_binary']}\")\n",
    "                print(f\"      pro_would_reach_same_conclusion: {row['pro_would_reach_same_conclusion']}\")\n",
    "                print(f\"      gemini_15_web_prediction: {row['gemini_15_web_prediction']}\")\n",
    "                print(f\"      Flash vs Web GT: {'‚úÖ MATCH' if row['flash_binary'] == row['ground_truth_binary'] else '‚ùå DIFFER'}\")\n",
    "                print(f\"      ---\")\n",
    "            \n",
    "            # Check if Flash predictions are identical to ground truth\n",
    "            flash_matches_gt = (df['flash_binary'] == df['ground_truth_binary']).sum()\n",
    "            print(f\"\\nüö® CRITICAL CHECK:\")\n",
    "            print(f\"   Flash predictions matching web ground truth: {flash_matches_gt}/{len(df)} ({flash_matches_gt/len(df):.1%})\")\n",
    "            \n",
    "            if flash_matches_gt == len(df):\n",
    "                print(\"   üö® ERROR: Flash predictions are 100% identical to web ground truth!\")\n",
    "                print(\"   This suggests the web ground truth data is actually Flash's predictions, not human annotations.\")\n",
    "                print(\"   Check your data pipeline - the 'ground_truth' field might be populated with Flash results.\")\n",
    "            elif flash_matches_gt > len(df) * 0.95:\n",
    "                print(\"   ‚ö†Ô∏è WARNING: Flash predictions are suspiciously similar to web ground truth (>95% match)\")\n",
    "                print(\"   This might indicate data contamination.\")\n",
    "            else:\n",
    "                print(\"   ‚úÖ Good: Flash predictions differ from web ground truth as expected.\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading data: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def calculate_classification_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate comprehensive classification metrics for Web Gemini Pro\"\"\"\n",
    "        print(\"üìä Calculating classification metrics...\")\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            return {}\n",
    "        \n",
    "        y_true = df['ground_truth_binary'].values\n",
    "        gemini_15_pred = df['gemini_15_web_prediction'].values\n",
    "        \n",
    "        # Web Gemini 1.5 metrics - Compare Web Gemini 1.5 predictions with web ground truth  \n",
    "        gemini_15_metrics = self._calculate_model_metrics(y_true, gemini_15_pred, \"Web Gemini 1.5\")\n",
    "        \n",
    "        # Agreement analysis\n",
    "        gemini_15_vs_ground_truth_agreement = (gemini_15_pred == y_true).mean()\n",
    "        judge_agreement_rate = df['pro_judge_agreement'].mean()\n",
    "        \n",
    "        print(f\"‚úÖ Metrics calculated:\")\n",
    "        print(f\"   Web Gemini 1.5 F1: {gemini_15_metrics['f1_score']:.3f}\")\n",
    "        print(f\"   Judge agreement: {judge_agreement_rate:.1%}\")\n",
    "        \n",
    "        return {\n",
    "            'dataset_info': {\n",
    "                'total_samples': len(df),\n",
    "                'positive_samples': int((y_true == 1).sum()),\n",
    "                'negative_samples': int((y_true == 0).sum()),\n",
    "                'class_balance': float((y_true == 1).mean())\n",
    "            },\n",
    "            'gemini_15_web_metrics': gemini_15_metrics,\n",
    "            'agreement_analysis': {\n",
    "                'gemini_15_vs_ground_truth': gemini_15_vs_ground_truth_agreement,\n",
    "                'gemini_15_judge_agreement_with_flash': judge_agreement_rate\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _calculate_model_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, model_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate detailed metrics for a single model\"\"\"\n",
    "        \n",
    "        # Ensure binary values\n",
    "        y_true = np.array(y_true).astype(int)\n",
    "        y_pred = np.array(y_pred).astype(int)\n",
    "        \n",
    "        # Validate binary values\n",
    "        if not (set(np.unique(y_true)) <= {0, 1} and set(np.unique(y_pred)) <= {0, 1}):\n",
    "            print(f\"‚ö†Ô∏è Warning: Non-binary values detected in {model_name}\")\n",
    "            print(f\"   y_true unique: {np.unique(y_true)}\")\n",
    "            print(f\"   y_pred unique: {np.unique(y_pred)}\")\n",
    "            # Force to binary\n",
    "            y_true = np.clip(y_true, 0, 1)\n",
    "            y_pred = np.clip(y_pred, 0, 1)\n",
    "        \n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        # Handle edge cases where only one class is predicted\n",
    "        unique_pred = np.unique(y_pred)\n",
    "        unique_true = np.unique(y_true)\n",
    "        \n",
    "        if len(unique_pred) == 1 or len(unique_true) == 1:\n",
    "            # Use macro average for edge cases\n",
    "            precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        else:\n",
    "            precision = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "        \n",
    "        # Confusion matrix - ensure we get 2x2 matrix\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "        \n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        elif cm.shape == (1, 1):\n",
    "            # Handle case where only one class exists\n",
    "            if unique_true[0] == 0 and unique_pred[0] == 0:\n",
    "                tn, fp, fn, tp = cm[0,0], 0, 0, 0\n",
    "            elif unique_true[0] == 1 and unique_pred[0] == 1:\n",
    "                tn, fp, fn, tp = 0, 0, 0, cm[0,0]\n",
    "            else:\n",
    "                tn, fp, fn, tp = 0, cm[0,0], 0, 0\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Unexpected confusion matrix shape for {model_name}: {cm.shape}\")\n",
    "            tn = fp = fn = tp = 0\n",
    "        \n",
    "        # Additional metrics\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # Same as recall\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # Negative Predictive Value\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Positive Predictive Value (same as precision)\n",
    "        \n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # False Negative Rate\n",
    "        \n",
    "        balanced_accuracy = (sensitivity + specificity) / 2\n",
    "        \n",
    "        # Matthews Correlation Coefficient\n",
    "        mcc_num = (tp * tn) - (fp * fn)\n",
    "        mcc_den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "        mcc = mcc_num / mcc_den if mcc_den > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'specificity': specificity,\n",
    "            'sensitivity': sensitivity,\n",
    "            'balanced_accuracy': balanced_accuracy,\n",
    "            'negative_predictive_value': npv,\n",
    "            'positive_predictive_value': ppv,\n",
    "            'false_positive_rate': fpr,\n",
    "            'false_negative_rate': fnr,\n",
    "            'matthews_correlation': mcc,\n",
    "            'confusion_matrix': {\n",
    "                'true_positives': int(tp),\n",
    "                'false_positives': int(fp), \n",
    "                'true_negatives': int(tn),\n",
    "                'false_negatives': int(fn)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def create_classification_dashboard(self, metrics: Dict[str, Any], df: pd.DataFrame) -> str:\n",
    "        \"\"\"Create comprehensive classification metrics dashboard\"\"\"\n",
    "        \n",
    "        if not metrics:\n",
    "            return \"<html><body><h1>No valid metrics to display</h1></body></html>\"\n",
    "        \n",
    "        gemini_15_metrics = metrics['gemini_15_web_metrics']\n",
    "        dataset_info = metrics['dataset_info']\n",
    "        agreement = metrics['agreement_analysis']\n",
    "        \n",
    "        gemini_15_cm = gemini_15_metrics['confusion_matrix']\n",
    "        \n",
    "        # Performance assessment\n",
    "        if gemini_15_metrics['f1_score'] >= 0.8:\n",
    "            gemini_15_assessment = \"üü¢ EXCELLENT\"\n",
    "            gemini_15_color = \"#10B981\"\n",
    "        elif gemini_15_metrics['f1_score'] >= 0.7:\n",
    "            gemini_15_assessment = \"üü° GOOD\" \n",
    "            gemini_15_color = \"#F59E0B\"\n",
    "        elif gemini_15_metrics['f1_score'] >= 0.6:\n",
    "            gemini_15_assessment = \"üü† MODERATE\"\n",
    "            gemini_15_color = \"#FF6B35\"\n",
    "        else:\n",
    "            gemini_15_assessment = \"üî¥ POOR\"\n",
    "            gemini_15_color = \"#EF4444\"\n",
    "        \n",
    "        html_content = f\"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Web Gemini 1.5 Classification Metrics Dashboard</title>\n",
    "    <style>\n",
    "        * {{ margin: 0; padding: 0; box-sizing: border-box; }}\n",
    "        body {{ font-family: 'Inter', sans-serif; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: #333; line-height: 1.6; min-height: 100vh; }}\n",
    "        .dashboard {{ max-width: 1800px; margin: 0 auto; padding: 20px; }}\n",
    "        .header {{ background: rgba(255,255,255,0.95); padding: 40px; border-radius: 25px; text-align: center; margin-bottom: 30px; backdrop-filter: blur(15px); box-shadow: 0 20px 60px rgba(0,0,0,0.1); }}\n",
    "        .header h1 {{ font-size: 3rem; margin-bottom: 15px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; }}\n",
    "        .header h2 {{ font-size: 1.5rem; color: #666; margin-bottom: 20px; }}\n",
    "        .model-badge {{ display: inline-block; padding: 10px 25px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border-radius: 30px; font-weight: bold; font-size: 1.1rem; margin: 5px; }}\n",
    "        .metrics-overview {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(180px, 1fr)); gap: 20px; margin-bottom: 40px; }}\n",
    "        .metric-card {{ background: rgba(255,255,255,0.95); padding: 25px; border-radius: 20px; text-align: center; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); transition: transform 0.3s ease; }}\n",
    "        .metric-card:hover {{ transform: translateY(-5px); }}\n",
    "        .metric-value {{ font-size: 2.5rem; font-weight: bold; margin-bottom: 10px; }}\n",
    "        .metric-label {{ font-size: 1rem; color: #666; font-weight: 600; margin-bottom: 5px; }}\n",
    "        .metric-description {{ font-size: 0.85rem; color: #888; }}\n",
    "        .excellent {{ color: #10B981; }}\n",
    "        .good {{ color: #F59E0B; }}\n",
    "        .moderate {{ color: #FF6B35; }}\n",
    "        .poor {{ color: #EF4444; }}\n",
    "        .dashboard-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(450px, 1fr)); gap: 25px; margin-bottom: 40px; }}\n",
    "        .chart-section {{ background: rgba(255,255,255,0.95); padding: 30px; border-radius: 20px; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); }}\n",
    "        .chart-title {{ font-size: 1.3rem; font-weight: bold; margin-bottom: 20px; color: #333; text-align: center; }}\n",
    "        .confusion-matrix {{ display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px; margin: 20px 0; text-align: center; font-size: 0.9rem; }}\n",
    "        .cm-cell {{ padding: 15px; border-radius: 12px; font-weight: bold; display: flex; flex-direction: column; justify-content: center; align-items: center; }}\n",
    "        .cm-header {{ background: linear-gradient(135deg, #f8f9fa, #e9ecef); color: #495057; }}\n",
    "        .cm-tp {{ background: linear-gradient(135deg, #d4edda, #c3e6cb); color: #155724; font-size: 1.8rem; }}\n",
    "        .cm-fp {{ background: linear-gradient(135deg, #f8d7da, #f1b0b7); color: #721c24; font-size: 1.8rem; }}\n",
    "        .cm-fn {{ background: linear-gradient(135deg, #fff3cd, #fce4a6); color: #856404; font-size: 1.8rem; }}\n",
    "        .cm-tn {{ background: linear-gradient(135deg, #d1ecf1, #bee5eb); color: #0c5460; font-size: 1.8rem; }}\n",
    "        .cm-label {{ font-size: 0.75rem; margin-top: 5px; opacity: 0.8; }}\n",
    "        .performance-summary {{ background: linear-gradient(135deg, #e3f2fd, #bbdefb); padding: 30px; border-radius: 20px; margin: 30px 0; text-align: center; }}\n",
    "        .performance-score {{ font-size: 3.5rem; font-weight: bold; color: {gemini_15_color}; margin-bottom: 10px; }}\n",
    "        .performance-label {{ font-size: 1.3rem; color: #37474f; margin-bottom: 8px; }}\n",
    "        .performance-description {{ font-size: 1rem; color: #546e7a; }}\n",
    "        .comparison-section {{ background: rgba(255,255,255,0.95); padding: 30px; border-radius: 20px; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); margin-bottom: 30px; }}\n",
    "        .model-column {{ padding: 20px; border-radius: 15px; }}\n",
    "        .pro-column {{ background: linear-gradient(135deg, #e3f2fd, #bbdefb); }}\n",
    "        .model-title {{ font-size: 1.5rem; font-weight: bold; text-align: center; margin-bottom: 20px; }}\n",
    "        .metric-row {{ display: flex; justify-content: space-between; margin: 8px 0; padding: 8px; background: rgba(255,255,255,0.5); border-radius: 8px; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"dashboard\">\n",
    "        <div class=\"header\">\n",
    "            <h1>üåê Web Gemini 1.5 Classification Metrics</h1>\n",
    "            <h2>Performance vs Web Ground Truth</h2>\n",
    "            <div>\n",
    "                <span class=\"model-badge\">Web Gemini 1.5</span>\n",
    "            </div>\n",
    "            <p style=\"margin-top: 20px; color: #666;\">\n",
    "                Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | \n",
    "                Samples: {dataset_info['total_samples']:,} | \n",
    "                Positive: {dataset_info['positive_samples']} | Negative: {dataset_info['negative_samples']}\n",
    "            </p>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"performance-summary\">\n",
    "            <div class=\"performance-score\">{gemini_15_metrics['f1_score']:.3f}</div>\n",
    "            <div class=\"performance-label\">{gemini_15_assessment} F1-Score (Web Gemini 1.5)</div>\n",
    "            <div class=\"performance-description\">\n",
    "                Web Gemini 1.5 achieves {gemini_15_metrics['accuracy']:.1%} accuracy with {gemini_15_metrics['precision']:.1%} precision\n",
    "                <br>Judge Agreement with Flash: {agreement['gemini_15_judge_agreement_with_flash']:.1%}\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"metrics-overview\">\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if gemini_15_metrics['f1_score'] >= 0.8 else 'good' if gemini_15_metrics['f1_score'] >= 0.7 else 'moderate' if gemini_15_metrics['f1_score'] >= 0.6 else 'poor'}\">{gemini_15_metrics['f1_score']:.3f}</div>\n",
    "                <div class=\"metric-label\">Web 1.5 F1-Score</div>\n",
    "                <div class=\"metric-description\">Harmonic Mean P&R</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if gemini_15_metrics['recall'] >= 0.8 else 'good' if gemini_15_metrics['recall'] >= 0.7 else 'moderate' if gemini_15_metrics['recall'] >= 0.6 else 'poor'}\">{gemini_15_metrics['recall']:.3f}</div>\n",
    "                <div class=\"metric-label\">Web 1.5 TPR</div>\n",
    "                <div class=\"metric-description\">True Positive Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if gemini_15_metrics['specificity'] >= 0.8 else 'good' if gemini_15_metrics['specificity'] >= 0.7 else 'moderate' if gemini_15_metrics['specificity'] >= 0.6 else 'poor'}\">{gemini_15_metrics['specificity']:.3f}</div>\n",
    "                <div class=\"metric-label\">Web 1.5 TNR</div>\n",
    "                <div class=\"metric-description\">True Negative Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if gemini_15_metrics['false_positive_rate'] <= 0.2 else 'good' if gemini_15_metrics['false_positive_rate'] <= 0.3 else 'moderate' if gemini_15_metrics['false_positive_rate'] <= 0.4 else 'poor'}\">{gemini_15_metrics['false_positive_rate']:.3f}</div>\n",
    "                <div class=\"metric-label\">Web 1.5 FPR</div>\n",
    "                <div class=\"metric-description\">False Positive Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if gemini_15_metrics['false_negative_rate'] <= 0.2 else 'good' if gemini_15_metrics['false_negative_rate'] <= 0.3 else 'moderate' if gemini_15_metrics['false_negative_rate'] <= 0.4 else 'poor'}\">{gemini_15_metrics['false_negative_rate']:.3f}</div>\n",
    "                <div class=\"metric-label\">Web 1.5 FNR</div>\n",
    "                <div class=\"metric-description\">False Negative Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if gemini_15_metrics['accuracy'] >= 0.9 else 'good' if gemini_15_metrics['accuracy'] >= 0.8 else 'moderate' if gemini_15_metrics['accuracy'] >= 0.7 else 'poor'}\">{gemini_15_metrics['accuracy']:.3f}</div>\n",
    "                <div class=\"metric-label\">Web 1.5 Accuracy</div>\n",
    "                <div class=\"metric-description\">Overall Correctness</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if gemini_15_metrics['precision'] >= 0.8 else 'good' if gemini_15_metrics['precision'] >= 0.7 else 'moderate' if gemini_15_metrics['precision'] >= 0.6 else 'poor'}\">{gemini_15_metrics['precision']:.3f}</div>\n",
    "                <div class=\"metric-label\">Web 1.5 Precision</div>\n",
    "                <div class=\"metric-description\">Positive Predictive Value</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if agreement['gemini_15_judge_agreement_with_flash'] >= 0.8 else 'good' if agreement['gemini_15_judge_agreement_with_flash'] >= 0.7 else 'moderate' if agreement['gemini_15_judge_agreement_with_flash'] >= 0.6 else 'poor'}\">{agreement['gemini_15_judge_agreement_with_flash']:.1%}</div>\n",
    "                <div class=\"metric-label\">Judge Agreement</div>\n",
    "                <div class=\"metric-description\">Web 1.5 agrees with Flash</div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"comparison-section\">\n",
    "            <div class=\"chart-title\">üìä Web Gemini 1.5 Performance Metrics</div>\n",
    "            <div style=\"max-width: 600px; margin: 0 auto;\">\n",
    "                <div class=\"model-column pro-column\">\n",
    "                    <div class=\"model-title\">üåê Web Gemini 1.5 vs Web Ground Truth</div>\n",
    "                    <div class=\"metric-row\"><span>F1-Score:</span><span class=\"{'excellent' if gemini_15_metrics['f1_score'] >= 0.8 else 'good' if gemini_15_metrics['f1_score'] >= 0.7 else 'moderate' if gemini_15_metrics['f1_score'] >= 0.6 else 'poor'}\">{gemini_15_metrics['f1_score']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Accuracy:</span><span>{gemini_15_metrics['accuracy']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Precision:</span><span>{gemini_15_metrics['precision']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Recall (TPR):</span><span>{gemini_15_metrics['recall']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Specificity (TNR):</span><span>{gemini_15_metrics['specificity']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>False Positive Rate:</span><span>{gemini_15_metrics['false_positive_rate']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>False Negative Rate:</span><span>{gemini_15_metrics['false_negative_rate']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Judge Agreement with Flash:</span><span>{agreement['gemini_15_judge_agreement_with_flash']:.1%}</span></div>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"dashboard-grid\">\n",
    "            <div class=\"chart-section\">\n",
    "                <div class=\"chart-title\">üåê Web Gemini 1.5 Confusion Matrix</div>\n",
    "                <div class=\"confusion-matrix\">\n",
    "                    <div class=\"cm-cell cm-header\"></div>\n",
    "                    <div class=\"cm-cell cm-header\">Web GT: 0</div>\n",
    "                    <div class=\"cm-cell cm-header\">Web GT: 1</div>\n",
    "                    \n",
    "                    <div class=\"cm-cell cm-header\">Web 1.5: 0</div>\n",
    "                    <div class=\"cm-cell cm-tn\">\n",
    "                        {gemini_15_cm['true_negatives']}\n",
    "                        <div class=\"cm-label\">True Negatives</div>\n",
    "                    </div>\n",
    "                    <div class=\"cm-cell cm-fn\">\n",
    "                        {gemini_15_cm['false_negatives']}\n",
    "                        <div class=\"cm-label\">False Negatives</div>\n",
    "                    </div>\n",
    "                    \n",
    "                    <div class=\"cm-cell cm-header\">Web 1.5: 1</div>\n",
    "                    <div class=\"cm-cell cm-fp\">\n",
    "                        {gemini_15_cm['false_positives']}\n",
    "                        <div class=\"cm-label\">False Positives</div>\n",
    "                    </div>\n",
    "                    <div class=\"cm-cell cm-tp\">\n",
    "                        {gemini_15_cm['true_positives']}\n",
    "                        <div class=\"cm-label\">True Positives</div>\n",
    "                    </div>\n",
    "                </div>\n",
    "                <div style=\"text-align: center; margin-top: 15px;\">\n",
    "                    <p><strong>TP:</strong> {gemini_15_cm['true_positives']} | <strong>FP:</strong> {gemini_15_cm['false_positives']} | <strong>TN:</strong> {gemini_15_cm['true_negatives']} | <strong>FN:</strong> {gemini_15_cm['false_negatives']}</p>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        window.addEventListener('load', function() {{\n",
    "            console.log('Web Gemini 1.5 Classification Dashboard Loaded');\n",
    "        }});\n",
    "    </script>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "        \n",
    "        return html_content\n",
    "    \n",
    "    def generate_classification_dashboard(self):\n",
    "        \"\"\"Main function to generate the classification dashboard\"\"\"\n",
    "        print(\"üöÄ STARTING WEB GEMINI PRO CLASSIFICATION DASHBOARD GENERATION\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        try:\n",
    "            # Load results with ground truth\n",
    "            df = self.load_results_with_ground_truth()\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"‚ùå No data available for dashboard\")\n",
    "                return None\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = self.calculate_classification_metrics(df)\n",
    "            \n",
    "            if not metrics:\n",
    "                print(\"‚ùå Could not calculate metrics\")\n",
    "                return None\n",
    "            \n",
    "            # Create dashboard\n",
    "            print(\"\\nüìä Creating Web Gemini Pro Classification Dashboard...\")\n",
    "            dashboard_html = self.create_classification_dashboard(metrics, df)\n",
    "            \n",
    "            # Save files\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            dashboard_filename = f\"web_gemini_pro_classification_dashboard_{timestamp}.html\"\n",
    "            metrics_filename = f\"web_gemini_pro_classification_metrics_{timestamp}.json\"\n",
    "            \n",
    "            with open(dashboard_filename, \"w\", encoding='utf-8') as f:\n",
    "                f.write(dashboard_html)\n",
    "            \n",
    "            with open(metrics_filename, \"w\") as f:\n",
    "                json.dump(metrics, f, indent=2, default=str)\n",
    "            \n",
    "            print(f\"‚úÖ Dashboard saved: {dashboard_filename}\")\n",
    "            print(f\"‚úÖ Metrics saved: {metrics_filename}\")\n",
    "            \n",
    "            # Summary\n",
    "            gemini_15_metrics = metrics['gemini_15_web_metrics']\n",
    "            agreement = metrics['agreement_analysis']\n",
    "            \n",
    "            print(f\"\\nüìä WEB GEMINI 1.5 CLASSIFICATION METRICS SUMMARY:\")\n",
    "            print(f\"   üåê Web Gemini 1.5:\")\n",
    "            print(f\"      F1-Score: {gemini_15_metrics['f1_score']:.3f}\")\n",
    "            print(f\"      Precision: {gemini_15_metrics['precision']:.3f}\")\n",
    "            print(f\"      Recall: {gemini_15_metrics['recall']:.3f}\") \n",
    "            print(f\"      Accuracy: {gemini_15_metrics['accuracy']:.3f}\")\n",
    "            print(f\"      TP: {gemini_15_metrics['confusion_matrix']['true_positives']}, FP: {gemini_15_metrics['confusion_matrix']['false_positives']}\")\n",
    "            print(f\"      TN: {gemini_15_metrics['confusion_matrix']['true_negatives']}, FN: {gemini_15_metrics['confusion_matrix']['false_negatives']}\")\n",
    "            \n",
    "            print(f\"\\n   ‚öñÔ∏è Agreement Rates:\")\n",
    "            print(f\"      Web 1.5 vs Web Ground Truth: {agreement['gemini_15_vs_ground_truth']:.1%}\")\n",
    "            print(f\"      Web 1.5 Judge Agreement: {agreement['gemini_15_judge_agreement_with_flash']:.1%}\")\n",
    "            \n",
    "            return dashboard_filename, metrics_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Dashboard generation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the Web Gemini Pro classification dashboard generation\"\"\"\n",
    "    dashboard_generator = WebGeminiProClassificationDashboard()\n",
    "    result = dashboard_generator.generate_classification_dashboard()\n",
    "    \n",
    "    if result:\n",
    "        dashboard_file, metrics_file = result\n",
    "        print(f\"\\nüéâ SUCCESS!\")\n",
    "        print(f\"üìÅ Files generated:\")\n",
    "        print(f\"   1. {dashboard_file}\")\n",
    "        print(f\"   2. {metrics_file}\")\n",
    "        print(f\"\\nüí° Open {dashboard_file} in your browser to view the dashboard!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Dashboard generation failed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5acb308a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING WEB GEMINI 2.5 PRO CLASSIFICATION DASHBOARD GENERATION\n",
      "======================================================================\n",
      "üì• Loading Web Gemini 2.5 Pro results with ground truth...\n",
      "üìä Loading Web Gemini 2.5 Pro results data...\n",
      "   Found 293 result records\n",
      "üìä Loading Web ground truth data...\n",
      "   Found 294 web ground truth records\n",
      "   Web ground truth distribution: {100: 252, 0: 42}\n",
      "‚úÖ Merged dataset: 293 records\n",
      "   Sample artifact_ids from results: ['comicsands.com/message-wrong-person-reddit', 'cracked.com/article_22086_5-self-righteous-critics-who-were-total-hypocrites.html', 'daringgourmet.com/best-buttermilk-biscuits/comment-page-2', 'dictionary.com/e/acronyms/fw', 'deltiasgaming.com/dead-rails-a-beginners-guide']\n",
      "   Sample artifact_ids from web GT: ['news.de/gesundheit/855915949/corona-zahlen-kreisfreie-stadt-solingen-heute-aktuell-27-06-2025-coronavirus-news-zu-rki-fallzahlen-tote-in-nordrhein-westfalen-intensivbetten-auslastung-und-neue-covid-19-variante-nimbus/1', 'bizjournals.com/sanfrancisco/news/2025/06/27/genentech-ulcerative-colitis-vixarelimab-ibd-osmr.html', 'scmp.com/lifestyle/food-drink/article/3315878/health-benefits-superfood-prickly-pear-cactus-spiking-popularity-japan', 'plejada.pl/newsy/bruce-willis-zniknal-z-powodu-choroby-poczatkowo-zostal-zle-zdiagnozowany/2cdfxfw', 'askamanager.org/2018/09/my-coworker-keeps-making-me-do-my-old-job-manager-doesnt-believe-im-sick-and-more.html']\n",
      "   Sample flash_classification: [100, 0, 100, 100, 100]\n",
      "   Sample web ground_truth after merge: [100, 0, 100, 100, 100]\n",
      "   Flash vs Web Ground Truth identical BEFORE conversion: 293/293\n",
      "   üö® CRITICAL ERROR: flash_classification is identical to web ground_truth!\n",
      "   This means your web ground truth data IS the Flash predictions, not human annotations.\n",
      "   Check your data pipeline - Web GT table might contain Flash results instead of human labels.\n",
      "   üìã Sample records to verify:\n",
      "      Record 1: artifact_id=comicsands.com/message-wrong-person-reddit, flash=100, web_gt=100\n",
      "      Record 2: artifact_id=cracked.com/article_22086_5-self-righteous-critics-who-were-total-hypocrites.html, flash=0, web_gt=0\n",
      "      Record 3: artifact_id=daringgourmet.com/best-buttermilk-biscuits/comment-page-2, flash=100, web_gt=100\n",
      "üìä Data distribution:\n",
      "   Web ground truth: {1: 251, 0: 42}\n",
      "   Flash predictions: {1: 251, 0: 42}\n",
      "   Web Gemini 2.5 Pro predictions: {1: 223, 0: 70}\n",
      "   Judge agreement rate: 80.9%\n",
      "üìä Final clean dataset: 293 records\n",
      "\n",
      "üîç DEBUG - Sample comparisons (first 5 records):\n",
      "   Columns available: ['artifact_id', 'flash_classification', 'flash_reasoning', 'model_prompt', 'pro_judge_agreement', 'pro_verdict', 'pro_confidence', 'pro_would_reach_same_conclusion', 'pro_reasoning', 'flash_vs_25pro_analysis', 'model_version_insights', 'improvements', 'api_call_time', 'model_used', 'error_message', 'ground_truth', 'ground_truth_reasoning', 'source', 'ground_truth_binary', 'flash_binary', 'gemini_25_web_prediction']\n",
      "   Record 1:\n",
      "      artifact_id: comicsands.com/message-wrong-person-reddit\n",
      "      web_ground_truth (from Web GT): 100 -> binary: 1\n",
      "      flash_classification: 100 -> binary: 1\n",
      "      pro_would_reach_same_conclusion: False\n",
      "      gemini_25_web_prediction: 0\n",
      "      Flash vs Web GT: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 2:\n",
      "      artifact_id: cracked.com/article_22086_5-self-righteous-critics-who-were-total-hypocrites.html\n",
      "      web_ground_truth (from Web GT): 0 -> binary: 0\n",
      "      flash_classification: 0 -> binary: 0\n",
      "      pro_would_reach_same_conclusion: False\n",
      "      gemini_25_web_prediction: 1\n",
      "      Flash vs Web GT: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 3:\n",
      "      artifact_id: daringgourmet.com/best-buttermilk-biscuits/comment-page-2\n",
      "      web_ground_truth (from Web GT): 100 -> binary: 1\n",
      "      flash_classification: 100 -> binary: 1\n",
      "      pro_would_reach_same_conclusion: False\n",
      "      gemini_25_web_prediction: 0\n",
      "      Flash vs Web GT: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 4:\n",
      "      artifact_id: dictionary.com/e/acronyms/fw\n",
      "      web_ground_truth (from Web GT): 100 -> binary: 1\n",
      "      flash_classification: 100 -> binary: 1\n",
      "      pro_would_reach_same_conclusion: False\n",
      "      gemini_25_web_prediction: 0\n",
      "      Flash vs Web GT: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 5:\n",
      "      artifact_id: deltiasgaming.com/dead-rails-a-beginners-guide\n",
      "      web_ground_truth (from Web GT): 100 -> binary: 1\n",
      "      flash_classification: 100 -> binary: 1\n",
      "      pro_would_reach_same_conclusion: False\n",
      "      gemini_25_web_prediction: 0\n",
      "      Flash vs Web GT: ‚úÖ MATCH\n",
      "      ---\n",
      "\n",
      "üö® CRITICAL CHECK:\n",
      "   Flash predictions matching web ground truth: 293/293 (100.0%)\n",
      "   üö® ERROR: Flash predictions are 100% identical to web ground truth!\n",
      "   This suggests the web ground truth data is actually Flash's predictions, not human annotations.\n",
      "   Check your data pipeline - the 'ground_truth' field might be populated with Flash results.\n",
      "üìä Calculating classification metrics...\n",
      "‚úÖ Metrics calculated:\n",
      "   Web Gemini 2.5 Pro F1: 0.878\n",
      "   Judge agreement: 80.9%\n",
      "\n",
      "üìä Creating Web Gemini 2.5 Pro Classification Dashboard...\n",
      "‚úÖ Dashboard saved: web_gemini_25_pro_classification_dashboard_20250725_135745.html\n",
      "‚úÖ Metrics saved: web_gemini_25_pro_classification_metrics_20250725_135745.json\n",
      "\n",
      "üìä WEB GEMINI 2.5 PRO CLASSIFICATION METRICS SUMMARY:\n",
      "   üåê Web Gemini 2.5 Pro:\n",
      "      F1-Score: 0.878\n",
      "      Precision: 0.933\n",
      "      Recall: 0.829\n",
      "      Accuracy: 0.802\n",
      "      TP: 208, FP: 15\n",
      "      TN: 27, FN: 43\n",
      "\n",
      "   ‚öñÔ∏è Agreement Rates:\n",
      "      Web 2.5 Pro vs Web Ground Truth: 80.2%\n",
      "      Web 2.5 Pro Judge Agreement: 80.9%\n",
      "\n",
      "üéâ SUCCESS!\n",
      "üìÅ Files generated:\n",
      "   1. web_gemini_25_pro_classification_dashboard_20250725_135745.html\n",
      "   2. web_gemini_25_pro_classification_metrics_20250725_135745.json\n",
      "\n",
      "üí° Open web_gemini_25_pro_classification_dashboard_20250725_135745.html in your browser to view the dashboard!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Web Gemini 2.5 Pro Classification Metrics Dashboard - Real Results vs Web Ground Truth\n",
    "Calculates traditional ML metrics (F1, Precision, Recall, Confusion Matrix) for Gemini 2.5 Pro decisions on Web content\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "PROJECT_ID = \"scope3-dev\"\n",
    "DATASET_ID = \"research_bs_monitoring\"\n",
    "WEB_GROUND_TRUTH_TABLE = \"BA_Web_Ground_Truth\"  # Web ground truth table\n",
    "WEB_RESULTS_TABLE = \"BA_Web_Gemini_25_Pro_Judge_Results\"  # Web Gemini 2.5 Pro results table\n",
    "\n",
    "class WebGemini25ProClassificationDashboard:\n",
    "    def __init__(self):\n",
    "        self.client = bigquery.Client(project=PROJECT_ID)\n",
    "        \n",
    "    def load_results_with_ground_truth(self) -> pd.DataFrame:\n",
    "        \"\"\"Load Web Gemini 2.5 Pro results with real ground truth from Web Ground Truth table\"\"\"\n",
    "        print(\"üì• Loading Web Gemini 2.5 Pro results with ground truth...\")\n",
    "        \n",
    "        # First, let's get the results data\n",
    "        results_query = f\"\"\"\n",
    "        SELECT \n",
    "            artifact_id,\n",
    "            flash_classification,\n",
    "            flash_reasoning,\n",
    "            model_prompt,\n",
    "            pro_judge_agreement,\n",
    "            pro_verdict,\n",
    "            pro_confidence,\n",
    "            pro_would_reach_same_conclusion,\n",
    "            pro_reasoning,\n",
    "            flash_vs_25pro_analysis,\n",
    "            model_version_insights,\n",
    "            improvements,\n",
    "            api_call_time,\n",
    "            model_used,\n",
    "            error_message\n",
    "        FROM `{PROJECT_ID}.{DATASET_ID}.{WEB_RESULTS_TABLE}`\n",
    "        WHERE pro_judge_agreement IS NOT NULL\n",
    "        AND pro_would_reach_same_conclusion IS NOT NULL\n",
    "        AND (error_message IS NULL OR error_message = '')\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get real ground truth from Web Ground Truth table\n",
    "        web_gt_query = f\"\"\"\n",
    "        SELECT \n",
    "            artifact_id,\n",
    "            correct_classification as ground_truth,\n",
    "            correct_reasoning as ground_truth_reasoning,\n",
    "            source\n",
    "        FROM `{PROJECT_ID}.{DATASET_ID}.{WEB_GROUND_TRUTH_TABLE}`\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(\"üìä Loading Web Gemini 2.5 Pro results data...\")\n",
    "            results_df = self.client.query(results_query).to_dataframe()\n",
    "            print(f\"   Found {len(results_df)} result records\")\n",
    "            \n",
    "            print(\"üìä Loading Web ground truth data...\")\n",
    "            web_gt_df = self.client.query(web_gt_query).to_dataframe()\n",
    "            print(f\"   Found {len(web_gt_df)} web ground truth records\")\n",
    "            \n",
    "            # Debug: Check ground truth distribution\n",
    "            if len(web_gt_df) > 0:\n",
    "                gt_dist = web_gt_df['ground_truth'].value_counts().to_dict()\n",
    "                print(f\"   Web ground truth distribution: {gt_dist}\")\n",
    "            \n",
    "            # Join results with ground truth\n",
    "            df = results_df.merge(web_gt_df, on='artifact_id', how='inner')\n",
    "            print(f\"‚úÖ Merged dataset: {len(df)} records\")\n",
    "            \n",
    "            if len(df) == 0:\n",
    "                print(\"‚ùå No matching records found after merge\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Debug: Check what we actually got from the merge\n",
    "            print(f\"   Sample artifact_ids from results: {results_df['artifact_id'].head().tolist()}\")\n",
    "            print(f\"   Sample artifact_ids from web GT: {web_gt_df['artifact_id'].head().tolist()}\")\n",
    "            print(f\"   Sample flash_classification: {results_df['flash_classification'].head().tolist()}\")\n",
    "            print(f\"   Sample web ground_truth after merge: {df['ground_truth'].head().tolist()}\")\n",
    "            \n",
    "            # Check if flash_classification and ground_truth are identical BEFORE conversion\n",
    "            identical_before_conversion = (df['flash_classification'] == df['ground_truth']).sum()\n",
    "            print(f\"   Flash vs Web Ground Truth identical BEFORE conversion: {identical_before_conversion}/{len(df)}\")\n",
    "            \n",
    "            if identical_before_conversion == len(df):\n",
    "                print(\"   üö® CRITICAL ERROR: flash_classification is identical to web ground_truth!\")\n",
    "                print(\"   This means your web ground truth data IS the Flash predictions, not human annotations.\")\n",
    "                print(\"   Check your data pipeline - Web GT table might contain Flash results instead of human labels.\")\n",
    "                \n",
    "                # Let's check a few individual records to confirm\n",
    "                print(\"   üìã Sample records to verify:\")\n",
    "                for i in range(min(3, len(df))):\n",
    "                    row = df.iloc[i]\n",
    "                    print(f\"      Record {i+1}: artifact_id={row['artifact_id']}, flash={row['flash_classification']}, web_gt={row['ground_truth']}\")\n",
    "            \n",
    "            # Convert to binary format\n",
    "            df['ground_truth_binary'] = (df['ground_truth'] == 100).astype(int)\n",
    "            df['flash_binary'] = (df['flash_classification'] == 100).astype(int)\n",
    "            \n",
    "            # Convert Gemini 2.5 Pro \"would reach same conclusion\" to predictions\n",
    "            df['gemini_25_web_prediction'] = np.where(\n",
    "                df['pro_would_reach_same_conclusion'] == True,\n",
    "                df['flash_binary'],  # Same as Flash\n",
    "                1 - df['flash_binary']  # Opposite of Flash\n",
    "            ).astype(int)\n",
    "            \n",
    "            print(f\"üìä Data distribution:\")\n",
    "            print(f\"   Web ground truth: {df['ground_truth_binary'].value_counts().to_dict()}\")\n",
    "            print(f\"   Flash predictions: {df['flash_binary'].value_counts().to_dict()}\")\n",
    "            print(f\"   Web Gemini 2.5 Pro predictions: {df['gemini_25_web_prediction'].value_counts().to_dict()}\")\n",
    "            print(f\"   Judge agreement rate: {df['pro_judge_agreement'].mean():.1%}\")\n",
    "            \n",
    "            # Validate binary data\n",
    "            for col, name in [('ground_truth_binary', 'Web ground truth'), \n",
    "                            ('flash_binary', 'Flash'), \n",
    "                            ('gemini_25_web_prediction', 'Web Gemini 2.5 Pro')]:\n",
    "                if not all(df[col].isin([0, 1])):\n",
    "                    print(f\"‚ö†Ô∏è Warning: {name} contains non-binary values\")\n",
    "                    df = df[df[col].isin([0, 1])]\n",
    "            \n",
    "            print(f\"üìä Final clean dataset: {len(df)} records\")\n",
    "            \n",
    "            # Debug sample comparisons\n",
    "            print(f\"\\nüîç DEBUG - Sample comparisons (first 5 records):\")\n",
    "            print(f\"   Columns available: {df.columns.tolist()}\")\n",
    "            for i in range(min(5, len(df))):\n",
    "                row = df.iloc[i]\n",
    "                print(f\"   Record {i+1}:\")\n",
    "                print(f\"      artifact_id: {row['artifact_id']}\")\n",
    "                print(f\"      web_ground_truth (from Web GT): {row['ground_truth']} -> binary: {row['ground_truth_binary']}\")\n",
    "                print(f\"      flash_classification: {row['flash_classification']} -> binary: {row['flash_binary']}\")\n",
    "                print(f\"      pro_would_reach_same_conclusion: {row['pro_would_reach_same_conclusion']}\")\n",
    "                print(f\"      gemini_25_web_prediction: {row['gemini_25_web_prediction']}\")\n",
    "                print(f\"      Flash vs Web GT: {'‚úÖ MATCH' if row['flash_binary'] == row['ground_truth_binary'] else '‚ùå DIFFER'}\")\n",
    "                print(f\"      ---\")\n",
    "            \n",
    "            # Check if Flash predictions are identical to ground truth\n",
    "            flash_matches_gt = (df['flash_binary'] == df['ground_truth_binary']).sum()\n",
    "            print(f\"\\nüö® CRITICAL CHECK:\")\n",
    "            print(f\"   Flash predictions matching web ground truth: {flash_matches_gt}/{len(df)} ({flash_matches_gt/len(df):.1%})\")\n",
    "            \n",
    "            if flash_matches_gt == len(df):\n",
    "                print(\"   üö® ERROR: Flash predictions are 100% identical to web ground truth!\")\n",
    "                print(\"   This suggests the web ground truth data is actually Flash's predictions, not human annotations.\")\n",
    "                print(\"   Check your data pipeline - the 'ground_truth' field might be populated with Flash results.\")\n",
    "            elif flash_matches_gt > len(df) * 0.95:\n",
    "                print(\"   ‚ö†Ô∏è WARNING: Flash predictions are suspiciously similar to web ground truth (>95% match)\")\n",
    "                print(\"   This might indicate data contamination.\")\n",
    "            else:\n",
    "                print(\"   ‚úÖ Good: Flash predictions differ from web ground truth as expected.\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading data: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def calculate_classification_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate comprehensive classification metrics for Web Gemini 2.5 Pro\"\"\"\n",
    "        print(\"üìä Calculating classification metrics...\")\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            return {}\n",
    "        \n",
    "        y_true = df['ground_truth_binary'].values\n",
    "        gemini_25_pred = df['gemini_25_web_prediction'].values\n",
    "        \n",
    "        # Web Gemini 2.5 Pro metrics - Compare Web Gemini 2.5 Pro predictions with web ground truth  \n",
    "        gemini_25_metrics = self._calculate_model_metrics(y_true, gemini_25_pred, \"Web Gemini 2.5 Pro\")\n",
    "        \n",
    "        # Agreement analysis\n",
    "        gemini_25_vs_ground_truth_agreement = (gemini_25_pred == y_true).mean()\n",
    "        judge_agreement_rate = df['pro_judge_agreement'].mean()\n",
    "        \n",
    "        print(f\"‚úÖ Metrics calculated:\")\n",
    "        print(f\"   Web Gemini 2.5 Pro F1: {gemini_25_metrics['f1_score']:.3f}\")\n",
    "        print(f\"   Judge agreement: {judge_agreement_rate:.1%}\")\n",
    "        \n",
    "        return {\n",
    "            'dataset_info': {\n",
    "                'total_samples': len(df),\n",
    "                'positive_samples': int((y_true == 1).sum()),\n",
    "                'negative_samples': int((y_true == 0).sum()),\n",
    "                'class_balance': float((y_true == 1).mean())\n",
    "            },\n",
    "            'gemini_25_web_metrics': gemini_25_metrics,\n",
    "            'agreement_analysis': {\n",
    "                'gemini_25_vs_ground_truth': gemini_25_vs_ground_truth_agreement,\n",
    "                'gemini_25_judge_agreement_with_flash': judge_agreement_rate\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _calculate_model_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, model_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate detailed metrics for a single model\"\"\"\n",
    "        \n",
    "        # Ensure binary values\n",
    "        y_true = np.array(y_true).astype(int)\n",
    "        y_pred = np.array(y_pred).astype(int)\n",
    "        \n",
    "        # Validate binary values\n",
    "        if not (set(np.unique(y_true)) <= {0, 1} and set(np.unique(y_pred)) <= {0, 1}):\n",
    "            print(f\"‚ö†Ô∏è Warning: Non-binary values detected in {model_name}\")\n",
    "            print(f\"   y_true unique: {np.unique(y_true)}\")\n",
    "            print(f\"   y_pred unique: {np.unique(y_pred)}\")\n",
    "            # Force to binary\n",
    "            y_true = np.clip(y_true, 0, 1)\n",
    "            y_pred = np.clip(y_pred, 0, 1)\n",
    "        \n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        # Handle edge cases where only one class is predicted\n",
    "        unique_pred = np.unique(y_pred)\n",
    "        unique_true = np.unique(y_true)\n",
    "        \n",
    "        if len(unique_pred) == 1 or len(unique_true) == 1:\n",
    "            # Use macro average for edge cases\n",
    "            precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        else:\n",
    "            precision = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "        \n",
    "        # Confusion matrix - ensure we get 2x2 matrix\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "        \n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        elif cm.shape == (1, 1):\n",
    "            # Handle case where only one class exists\n",
    "            if unique_true[0] == 0 and unique_pred[0] == 0:\n",
    "                tn, fp, fn, tp = cm[0,0], 0, 0, 0\n",
    "            elif unique_true[0] == 1 and unique_pred[0] == 1:\n",
    "                tn, fp, fn, tp = 0, 0, 0, cm[0,0]\n",
    "            else:\n",
    "                tn, fp, fn, tp = 0, cm[0,0], 0, 0\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Unexpected confusion matrix shape for {model_name}: {cm.shape}\")\n",
    "            tn = fp = fn = tp = 0\n",
    "        \n",
    "        # Additional metrics\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # Same as recall\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # Negative Predictive Value\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Positive Predictive Value (same as precision)\n",
    "        \n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # False Negative Rate\n",
    "        \n",
    "        balanced_accuracy = (sensitivity + specificity) / 2\n",
    "        \n",
    "        # Matthews Correlation Coefficient\n",
    "        mcc_num = (tp * tn) - (fp * fn)\n",
    "        mcc_den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "        mcc = mcc_num / mcc_den if mcc_den > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'specificity': specificity,\n",
    "            'sensitivity': sensitivity,\n",
    "            'balanced_accuracy': balanced_accuracy,\n",
    "            'negative_predictive_value': npv,\n",
    "            'positive_predictive_value': ppv,\n",
    "            'false_positive_rate': fpr,\n",
    "            'false_negative_rate': fnr,\n",
    "            'matthews_correlation': mcc,\n",
    "            'confusion_matrix': {\n",
    "                'true_positives': int(tp),\n",
    "                'false_positives': int(fp), \n",
    "                'true_negatives': int(tn),\n",
    "                'false_negatives': int(fn)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def create_classification_dashboard(self, metrics: Dict[str, Any], df: pd.DataFrame) -> str:\n",
    "        \"\"\"Create comprehensive classification metrics dashboard\"\"\"\n",
    "        \n",
    "        if not metrics:\n",
    "            return \"<html><body><h1>No valid metrics to display</h1></body></html>\"\n",
    "        \n",
    "        gemini_25_metrics = metrics['gemini_25_web_metrics']\n",
    "        dataset_info = metrics['dataset_info']\n",
    "        agreement = metrics['agreement_analysis']\n",
    "        \n",
    "        gemini_25_cm = gemini_25_metrics['confusion_matrix']\n",
    "        \n",
    "        # Performance assessment\n",
    "        if gemini_25_metrics['f1_score'] >= 0.8:\n",
    "            gemini_25_assessment = \"üü¢ EXCELLENT\"\n",
    "            gemini_25_color = \"#10B981\"\n",
    "        elif gemini_25_metrics['f1_score'] >= 0.7:\n",
    "            gemini_25_assessment = \"üü° GOOD\" \n",
    "            gemini_25_color = \"#F59E0B\"\n",
    "        elif gemini_25_metrics['f1_score'] >= 0.6:\n",
    "            gemini_25_assessment = \"üü† MODERATE\"\n",
    "            gemini_25_color = \"#FF6B35\"\n",
    "        else:\n",
    "            gemini_25_assessment = \"üî¥ POOR\"\n",
    "            gemini_25_color = \"#EF4444\"\n",
    "        \n",
    "        html_content = f\"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Web Gemini 2.5 Pro Classification Metrics Dashboard</title>\n",
    "    <style>\n",
    "        * {{ margin: 0; padding: 0; box-sizing: border-box; }}\n",
    "        body {{ font-family: 'Inter', sans-serif; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: #333; line-height: 1.6; min-height: 100vh; }}\n",
    "        .dashboard {{ max-width: 1800px; margin: 0 auto; padding: 20px; }}\n",
    "        .header {{ background: rgba(255,255,255,0.95); padding: 40px; border-radius: 25px; text-align: center; margin-bottom: 30px; backdrop-filter: blur(15px); box-shadow: 0 20px 60px rgba(0,0,0,0.1); }}\n",
    "        .header h1 {{ font-size: 3rem; margin-bottom: 15px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; }}\n",
    "        .header h2 {{ font-size: 1.5rem; color: #666; margin-bottom: 20px; }}\n",
    "        .model-badge {{ display: inline-block; padding: 10px 25px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border-radius: 30px; font-weight: bold; font-size: 1.1rem; margin: 5px; }}\n",
    "        .metrics-overview {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(180px, 1fr)); gap: 20px; margin-bottom: 40px; }}\n",
    "        .metric-card {{ background: rgba(255,255,255,0.95); padding: 25px; border-radius: 20px; text-align: center; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); transition: transform 0.3s ease; }}\n",
    "        .metric-card:hover {{ transform: translateY(-5px); }}\n",
    "        .metric-value {{ font-size: 2.5rem; font-weight: bold; margin-bottom: 10px; }}\n",
    "        .metric-label {{ font-size: 1rem; color: #666; font-weight: 600; margin-bottom: 5px; }}\n",
    "        .metric-description {{ font-size: 0.85rem; color: #888; }}\n",
    "        .excellent {{ color: #10B981; }}\n",
    "        .good {{ color: #F59E0B; }}\n",
    "        .moderate {{ color: #FF6B35; }}\n",
    "        .poor {{ color: #EF4444; }}\n",
    "        .dashboard-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(450px, 1fr)); gap: 25px; margin-bottom: 40px; }}\n",
    "        .chart-section {{ background: rgba(255,255,255,0.95); padding: 30px; border-radius: 20px; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); }}\n",
    "        .chart-title {{ font-size: 1.3rem; font-weight: bold; margin-bottom: 20px; color: #333; text-align: center; }}\n",
    "        .confusion-matrix {{ display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px; margin: 20px 0; text-align: center; font-size: 0.9rem; }}\n",
    "        .cm-cell {{ padding: 15px; border-radius: 12px; font-weight: bold; display: flex; flex-direction: column; justify-content: center; align-items: center; }}\n",
    "        .cm-header {{ background: linear-gradient(135deg, #f8f9fa, #e9ecef); color: #495057; }}\n",
    "        .cm-tp {{ background: linear-gradient(135deg, #d4edda, #c3e6cb); color: #155724; font-size: 1.8rem; }}\n",
    "        .cm-fp {{ background: linear-gradient(135deg, #f8d7da, #f1b0b7); color: #721c24; font-size: 1.8rem; }}\n",
    "        .cm-fn {{ background: linear-gradient(135deg, #fff3cd, #fce4a6); color: #856404; font-size: 1.8rem; }}\n",
    "        .cm-tn {{ background: linear-gradient(135deg, #d1ecf1, #bee5eb); color: #0c5460; font-size: 1.8rem; }}\n",
    "        .cm-label {{ font-size: 0.75rem; margin-top: 5px; opacity: 0.8; }}\n",
    "        .performance-summary {{ background: linear-gradient(135deg, #e3f2fd, #bbdefb); padding: 30px; border-radius: 20px; margin: 30px 0; text-align: center; }}\n",
    "        .performance-score {{ font-size: 3.5rem; font-weight: bold; color: {gemini_25_color}; margin-bottom: 10px; }}\n",
    "        .performance-label {{ font-size: 1.3rem; color: #37474f; margin-bottom: 8px; }}\n",
    "        .performance-description {{ font-size: 1rem; color: #546e7a; }}\n",
    "        .comparison-section {{ background: rgba(255,255,255,0.95); padding: 30px; border-radius: 20px; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); margin-bottom: 30px; }}\n",
    "        .model-column {{ padding: 20px; border-radius: 15px; }}\n",
    "        .pro-column {{ background: linear-gradient(135deg, #e3f2fd, #bbdefb); }}\n",
    "        .model-title {{ font-size: 1.5rem; font-weight: bold; text-align: center; margin-bottom: 20px; }}\n",
    "        .metric-row {{ display: flex; justify-content: space-between; margin: 8px 0; padding: 8px; background: rgba(255,255,255,0.5); border-radius: 8px; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"dashboard\">\n",
    "        <div class=\"header\">\n",
    "            <h1>üåê Web Gemini 2.5 Pro Classification Metrics</h1>\n",
    "            <h2>Performance vs Web Ground Truth</h2>\n",
    "            <div>\n",
    "                <span class=\"model-badge\">Web Gemini 2.5 Pro</span>\n",
    "            </div>\n",
    "            <p style=\"margin-top: 20px; color: #666;\">\n",
    "                Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | \n",
    "                Samples: {dataset_info['total_samples']:,} | \n",
    "                Positive: {dataset_info['positive_samples']} | Negative: {dataset_info['negative_samples']}\n",
    "            </p>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"performance-summary\">\n",
    "            <div class=\"performance-score\">{gemini_25_metrics['f1_score']:.3f}</div>\n",
    "            <div class=\"performance-label\">{gemini_25_assessment} F1-Score (Web Gemini 2.5 Pro)</div>\n",
    "            <div class=\"performance-description\">\n",
    "                Web Gemini 2.5 Pro achieves {gemini_25_metrics['accuracy']:.1%} accuracy with {gemini_25_metrics['precision']:.1%} precision\n",
    "                <br>Judge Agreement with Flash: {agreement['gemini_25_judge_agreement_with_flash']:.1%}\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"metrics-overview\">\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if gemini_25_metrics['f1_score'] >= 0.8 else 'good' if gemini_25_metrics['f1_score'] >= 0.7 else 'moderate' if gemini_25_metrics['f1_score'] >= 0.6 else 'poor'}\">{gemini_25_metrics['f1_score']:.3f}</div>\n",
    "                <div class=\"metric-label\">Web 2.5 Pro F1-Score</div>\n",
    "                <div class=\"metric-description\">Harmonic Mean P&R</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if gemini_25_metrics['recall'] >= 0.8 else 'good' if gemini_25_metrics['recall'] >= 0.7 else 'moderate' if gemini_25_metrics['recall'] >= 0.6 else 'poor'}\">{gemini_25_metrics['recall']:.3f}</div>\n",
    "                <div class=\"metric-label\">Web 2.5 Pro TPR</div>\n",
    "                <div class=\"metric-description\">True Positive Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if gemini_25_metrics['specificity'] >= 0.8 else 'good' if gemini_25_metrics['specificity'] >= 0.7 else 'moderate' if gemini_25_metrics['specificity'] >= 0.6 else 'poor'}\">{gemini_25_metrics['specificity']:.3f}</div>\n",
    "                <div class=\"metric-label\">Web 2.5 Pro TNR</div>\n",
    "                <div class=\"metric-description\">True Negative Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if gemini_25_metrics['false_positive_rate'] <= 0.2 else 'good' if gemini_25_metrics['false_positive_rate'] <= 0.3 else 'moderate' if gemini_25_metrics['false_positive_rate'] <= 0.4 else 'poor'}\">{gemini_25_metrics['false_positive_rate']:.3f}</div>\n",
    "                <div class=\"metric-label\">Web 2.5 Pro FPR</div>\n",
    "                <div class=\"metric-description\">False Positive Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if gemini_25_metrics['false_negative_rate'] <= 0.2 else 'good' if gemini_25_metrics['false_negative_rate'] <= 0.3 else 'moderate' if gemini_25_metrics['false_negative_rate'] <= 0.4 else 'poor'}\">{gemini_25_metrics['false_negative_rate']:.3f}</div>\n",
    "                <div class=\"metric-label\">Web 2.5 Pro FNR</div>\n",
    "                <div class=\"metric-description\">False Negative Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if gemini_25_metrics['accuracy'] >= 0.9 else 'good' if gemini_25_metrics['accuracy'] >= 0.8 else 'moderate' if gemini_25_metrics['accuracy'] >= 0.7 else 'poor'}\">{gemini_25_metrics['accuracy']:.3f}</div>\n",
    "                <div class=\"metric-label\">Web 2.5 Pro Accuracy</div>\n",
    "                <div class=\"metric-description\">Overall Correctness</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if gemini_25_metrics['precision'] >= 0.8 else 'good' if gemini_25_metrics['precision'] >= 0.7 else 'moderate' if gemini_25_metrics['precision'] >= 0.6 else 'poor'}\">{gemini_25_metrics['precision']:.3f}</div>\n",
    "                <div class=\"metric-label\">Web 2.5 Pro Precision</div>\n",
    "                <div class=\"metric-description\">Positive Predictive Value</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if agreement['gemini_25_judge_agreement_with_flash'] >= 0.8 else 'good' if agreement['gemini_25_judge_agreement_with_flash'] >= 0.7 else 'moderate' if agreement['gemini_25_judge_agreement_with_flash'] >= 0.6 else 'poor'}\">{agreement['gemini_25_judge_agreement_with_flash']:.1%}</div>\n",
    "                <div class=\"metric-label\">Judge Agreement</div>\n",
    "                <div class=\"metric-description\">Web 2.5 Pro agrees with Flash</div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"comparison-section\">\n",
    "            <div class=\"chart-title\">üìä Web Gemini 2.5 Pro Performance Metrics</div>\n",
    "            <div style=\"max-width: 600px; margin: 0 auto;\">\n",
    "                <div class=\"model-column pro-column\">\n",
    "                    <div class=\"model-title\">üåê Web Gemini 2.5 Pro vs Web Ground Truth</div>\n",
    "                    <div class=\"metric-row\"><span>F1-Score:</span><span class=\"{'excellent' if gemini_25_metrics['f1_score'] >= 0.8 else 'good' if gemini_25_metrics['f1_score'] >= 0.7 else 'moderate' if gemini_25_metrics['f1_score'] >= 0.6 else 'poor'}\">{gemini_25_metrics['f1_score']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Accuracy:</span><span>{gemini_25_metrics['accuracy']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Precision:</span><span>{gemini_25_metrics['precision']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Recall (TPR):</span><span>{gemini_25_metrics['recall']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Specificity (TNR):</span><span>{gemini_25_metrics['specificity']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>False Positive Rate:</span><span>{gemini_25_metrics['false_positive_rate']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>False Negative Rate:</span><span>{gemini_25_metrics['false_negative_rate']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Judge Agreement with Flash:</span><span>{agreement['gemini_25_judge_agreement_with_flash']:.1%}</span></div>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"dashboard-grid\">\n",
    "            <div class=\"chart-section\">\n",
    "                <div class=\"chart-title\">üåê Web Gemini 2.5 Pro Confusion Matrix</div>\n",
    "                <div class=\"confusion-matrix\">\n",
    "                    <div class=\"cm-cell cm-header\"></div>\n",
    "                    <div class=\"cm-cell cm-header\">Web GT: 0</div>\n",
    "                    <div class=\"cm-cell cm-header\">Web GT: 1</div>\n",
    "                    \n",
    "                    <div class=\"cm-cell cm-header\">Web 2.5 Pro: 0</div>\n",
    "                    <div class=\"cm-cell cm-tn\">\n",
    "                        {gemini_25_cm['true_negatives']}\n",
    "                        <div class=\"cm-label\">True Negatives</div>\n",
    "                    </div>\n",
    "                    <div class=\"cm-cell cm-fn\">\n",
    "                        {gemini_25_cm['false_negatives']}\n",
    "                        <div class=\"cm-label\">False Negatives</div>\n",
    "                    </div>\n",
    "                    \n",
    "                    <div class=\"cm-cell cm-header\">Web 2.5 Pro: 1</div>\n",
    "                    <div class=\"cm-cell cm-fp\">\n",
    "                        {gemini_25_cm['false_positives']}\n",
    "                        <div class=\"cm-label\">False Positives</div>\n",
    "                    </div>\n",
    "                    <div class=\"cm-cell cm-tp\">\n",
    "                        {gemini_25_cm['true_positives']}\n",
    "                        <div class=\"cm-label\">True Positives</div>\n",
    "                    </div>\n",
    "                </div>\n",
    "                <div style=\"text-align: center; margin-top: 15px;\">\n",
    "                    <p><strong>TP:</strong> {gemini_25_cm['true_positives']} | <strong>FP:</strong> {gemini_25_cm['false_positives']} | <strong>TN:</strong> {gemini_25_cm['true_negatives']} | <strong>FN:</strong> {gemini_25_cm['false_negatives']}</p>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        window.addEventListener('load', function() {{\n",
    "            console.log('Web Gemini 2.5 Pro Classification Dashboard Loaded');\n",
    "        }});\n",
    "    </script>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "        \n",
    "        return html_content\n",
    "    \n",
    "    def generate_classification_dashboard(self):\n",
    "        \"\"\"Main function to generate the classification dashboard\"\"\"\n",
    "        print(\"üöÄ STARTING WEB GEMINI 2.5 PRO CLASSIFICATION DASHBOARD GENERATION\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        try:\n",
    "            # Load results with ground truth\n",
    "            df = self.load_results_with_ground_truth()\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"‚ùå No data available for dashboard\")\n",
    "                return None\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = self.calculate_classification_metrics(df)\n",
    "            \n",
    "            if not metrics:\n",
    "                print(\"‚ùå Could not calculate metrics\")\n",
    "                return None\n",
    "            \n",
    "            # Create dashboard\n",
    "            print(\"\\nüìä Creating Web Gemini 2.5 Pro Classification Dashboard...\")\n",
    "            dashboard_html = self.create_classification_dashboard(metrics, df)\n",
    "            \n",
    "            # Save files\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            dashboard_filename = f\"web_gemini_25_pro_classification_dashboard_{timestamp}.html\"\n",
    "            metrics_filename = f\"web_gemini_25_pro_classification_metrics_{timestamp}.json\"\n",
    "            \n",
    "            with open(dashboard_filename, \"w\", encoding='utf-8') as f:\n",
    "                f.write(dashboard_html)\n",
    "            \n",
    "            with open(metrics_filename, \"w\") as f:\n",
    "                json.dump(metrics, f, indent=2, default=str)\n",
    "            \n",
    "            print(f\"‚úÖ Dashboard saved: {dashboard_filename}\")\n",
    "            print(f\"‚úÖ Metrics saved: {metrics_filename}\")\n",
    "            \n",
    "            # Summary\n",
    "            gemini_25_metrics = metrics['gemini_25_web_metrics']\n",
    "            agreement = metrics['agreement_analysis']\n",
    "            \n",
    "            print(f\"\\nüìä WEB GEMINI 2.5 PRO CLASSIFICATION METRICS SUMMARY:\")\n",
    "            print(f\"   üåê Web Gemini 2.5 Pro:\")\n",
    "            print(f\"      F1-Score: {gemini_25_metrics['f1_score']:.3f}\")\n",
    "            print(f\"      Precision: {gemini_25_metrics['precision']:.3f}\")\n",
    "            print(f\"      Recall: {gemini_25_metrics['recall']:.3f}\") \n",
    "            print(f\"      Accuracy: {gemini_25_metrics['accuracy']:.3f}\")\n",
    "            print(f\"      TP: {gemini_25_metrics['confusion_matrix']['true_positives']}, FP: {gemini_25_metrics['confusion_matrix']['false_positives']}\")\n",
    "            print(f\"      TN: {gemini_25_metrics['confusion_matrix']['true_negatives']}, FN: {gemini_25_metrics['confusion_matrix']['false_negatives']}\")\n",
    "            \n",
    "            print(f\"\\n   ‚öñÔ∏è Agreement Rates:\")\n",
    "            print(f\"      Web 2.5 Pro vs Web Ground Truth: {agreement['gemini_25_vs_ground_truth']:.1%}\")\n",
    "            print(f\"      Web 2.5 Pro Judge Agreement: {agreement['gemini_25_judge_agreement_with_flash']:.1%}\")\n",
    "            \n",
    "            return dashboard_filename, metrics_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Dashboard generation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the Web Gemini 2.5 Pro classification dashboard generation\"\"\"\n",
    "    dashboard_generator = WebGemini25ProClassificationDashboard()\n",
    "    result = dashboard_generator.generate_classification_dashboard()\n",
    "    \n",
    "    if result:\n",
    "        dashboard_file, metrics_file = result\n",
    "        print(f\"\\nüéâ SUCCESS!\")\n",
    "        print(f\"üìÅ Files generated:\")\n",
    "        print(f\"   1. {dashboard_file}\")\n",
    "        print(f\"   2. {metrics_file}\")\n",
    "        print(f\"\\nüí° Open {dashboard_file} in your browser to view the dashboard!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Dashboard generation failed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3b5b7be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING FLASH META CLASSIFICATION DASHBOARD GENERATION\n",
      "======================================================================\n",
      "üì• Loading Flash Meta results with ground truth...\n",
      "üìä Loading Flash Meta results data...\n",
      "   Found 299 Flash result records\n",
      "üìä Loading Meta ground truth data...\n",
      "   Found 299 meta ground truth records\n",
      "   Meta ground truth distribution: {100: 187, 0: 112}\n",
      "‚úÖ Merged dataset: 299 records\n",
      "   Sample artifact_ids from results: ['meta:993741799587060', 'meta:9940080572776247', 'meta:996072466011266', 'meta:aWdfbWVkaWFfM3B2OjE3ODQ4Mjc1NzkyNDkyODM4', 'meta:aWdfbWVkaWFfM3B2OjE3ODkzMzQxMTE3MjUyODY4']\n",
      "   Sample artifact_ids from meta GT: ['meta:122140809938790694', 'meta:4050755845195196', 'meta:1173810271458869', 'meta:1297387721748313', 'meta:1138085658345605']\n",
      "   Sample flash_classification: [0, 100, 100, 100, 100]\n",
      "   Sample meta ground_truth after merge: [0, 100, 100, 100, 100]\n",
      "   Flash vs Meta Ground Truth identical BEFORE conversion: 299/299\n",
      "   üö® CRITICAL ERROR: flash_classification is identical to meta ground_truth!\n",
      "   This means your meta ground truth data IS the Flash predictions, not human annotations.\n",
      "   Check your data pipeline - Meta GT table might contain Flash results instead of human labels.\n",
      "   üìã Sample records to verify:\n",
      "      Record 1: artifact_id=meta:993741799587060, flash=0, meta_gt=0\n",
      "      Record 2: artifact_id=meta:9940080572776247, flash=100, meta_gt=100\n",
      "      Record 3: artifact_id=meta:996072466011266, flash=100, meta_gt=100\n",
      "üìä Data distribution:\n",
      "   Meta ground truth: {1: 187, 0: 112}\n",
      "   Flash predictions: {1: 187, 0: 112}\n",
      "üìä Final clean dataset: 299 records\n",
      "\n",
      "üîç DEBUG - Sample comparisons (first 5 records):\n",
      "   Columns available: ['artifact_id', 'flash_classification', 'flash_reasoning', 'model_prompt', 'source', 'data_source', 'ground_truth', 'ground_truth_reasoning', 'gt_source', 'ground_truth_binary', 'flash_binary']\n",
      "   Record 1:\n",
      "      artifact_id: meta:993741799587060\n",
      "      meta_ground_truth (from Meta GT): 0 -> binary: 0\n",
      "      flash_classification: 0 -> binary: 0\n",
      "      Flash vs Meta GT: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 2:\n",
      "      artifact_id: meta:9940080572776247\n",
      "      meta_ground_truth (from Meta GT): 100 -> binary: 1\n",
      "      flash_classification: 100 -> binary: 1\n",
      "      Flash vs Meta GT: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 3:\n",
      "      artifact_id: meta:996072466011266\n",
      "      meta_ground_truth (from Meta GT): 100 -> binary: 1\n",
      "      flash_classification: 100 -> binary: 1\n",
      "      Flash vs Meta GT: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 4:\n",
      "      artifact_id: meta:aWdfbWVkaWFfM3B2OjE3ODQ4Mjc1NzkyNDkyODM4\n",
      "      meta_ground_truth (from Meta GT): 100 -> binary: 1\n",
      "      flash_classification: 100 -> binary: 1\n",
      "      Flash vs Meta GT: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 5:\n",
      "      artifact_id: meta:aWdfbWVkaWFfM3B2OjE3ODkzMzQxMTE3MjUyODY4\n",
      "      meta_ground_truth (from Meta GT): 100 -> binary: 1\n",
      "      flash_classification: 100 -> binary: 1\n",
      "      Flash vs Meta GT: ‚úÖ MATCH\n",
      "      ---\n",
      "\n",
      "üö® CRITICAL CHECK:\n",
      "   Flash predictions matching meta ground truth: 299/299 (100.0%)\n",
      "   üö® ERROR: Flash predictions are 100% identical to meta ground truth!\n",
      "   This suggests the meta ground truth data is actually Flash's predictions, not human annotations.\n",
      "   Check your data pipeline - the 'ground_truth' field might be populated with Flash results.\n",
      "üìä Calculating Flash classification metrics...\n",
      "‚úÖ Metrics calculated:\n",
      "   Flash F1: 1.000\n",
      "   Flash vs GT agreement: 100.0%\n",
      "\n",
      "üìä Creating Flash Meta Classification Dashboard...\n",
      "‚úÖ Dashboard saved: flash_meta_classification_dashboard_20250725_135747.html\n",
      "‚úÖ Metrics saved: flash_meta_classification_metrics_20250725_135747.json\n",
      "\n",
      "üìä FLASH META CLASSIFICATION METRICS SUMMARY:\n",
      "   ‚ö° Flash Model:\n",
      "      F1-Score: 1.000\n",
      "      Precision: 1.000\n",
      "      Recall: 1.000\n",
      "      Accuracy: 1.000\n",
      "      TP: 187, FP: 0\n",
      "      TN: 112, FN: 0\n",
      "\n",
      "   ‚öñÔ∏è Agreement Rates:\n",
      "      Flash vs Meta Ground Truth: 100.0%\n",
      "\n",
      "üéâ SUCCESS!\n",
      "üìÅ Files generated:\n",
      "   1. flash_meta_classification_dashboard_20250725_135747.html\n",
      "   2. flash_meta_classification_metrics_20250725_135747.json\n",
      "\n",
      "üí° Open flash_meta_classification_dashboard_20250725_135747.html in your browser to view the dashboard!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Flash Meta Classification Metrics Dashboard - Flash Results vs Meta Ground Truth\n",
    "Calculates traditional ML metrics (F1, Precision, Recall, Confusion Matrix) for Flash decisions on Meta content\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "PROJECT_ID = \"scope3-dev\"\n",
    "DATASET_ID = \"research_bs_monitoring\"\n",
    "META_GROUND_TRUTH_TABLE = \"BA_Meta_Ground_Truth\"  # Meta ground truth table\n",
    "META_RESULTS_TABLE = \"BA_Meta_Gemini_Pro_Judge_Results\"  # Contains flash_classification\n",
    "\n",
    "class FlashMetaClassificationDashboard:\n",
    "    def __init__(self):\n",
    "        self.client = bigquery.Client(project=PROJECT_ID)\n",
    "        \n",
    "    def load_results_with_ground_truth(self) -> pd.DataFrame:\n",
    "        \"\"\"Load Flash results with real ground truth from Meta Ground Truth table\"\"\"\n",
    "        print(\"üì• Loading Flash Meta results with ground truth...\")\n",
    "        \n",
    "        # Get Flash classifications from the results table\n",
    "        results_query = f\"\"\"\n",
    "        SELECT \n",
    "            artifact_id,\n",
    "            flash_classification,\n",
    "            flash_reasoning,\n",
    "            model_prompt,\n",
    "            source,\n",
    "            data_source\n",
    "        FROM `{PROJECT_ID}.{DATASET_ID}.{META_RESULTS_TABLE}`\n",
    "        WHERE flash_classification IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get real ground truth from Meta Ground Truth table\n",
    "        meta_gt_query = f\"\"\"\n",
    "        SELECT \n",
    "            artifact_id,\n",
    "            correct_classification as ground_truth,\n",
    "            correct_reasoning as ground_truth_reasoning,\n",
    "            source as gt_source\n",
    "        FROM `{PROJECT_ID}.{DATASET_ID}.{META_GROUND_TRUTH_TABLE}`\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(\"üìä Loading Flash Meta results data...\")\n",
    "            results_df = self.client.query(results_query).to_dataframe()\n",
    "            print(f\"   Found {len(results_df)} Flash result records\")\n",
    "            \n",
    "            print(\"üìä Loading Meta ground truth data...\")\n",
    "            meta_gt_df = self.client.query(meta_gt_query).to_dataframe()\n",
    "            print(f\"   Found {len(meta_gt_df)} meta ground truth records\")\n",
    "            \n",
    "            # Debug: Check ground truth distribution\n",
    "            if len(meta_gt_df) > 0:\n",
    "                gt_dist = meta_gt_df['ground_truth'].value_counts().to_dict()\n",
    "                print(f\"   Meta ground truth distribution: {gt_dist}\")\n",
    "            \n",
    "            # Join results with ground truth\n",
    "            df = results_df.merge(meta_gt_df, on='artifact_id', how='inner')\n",
    "            print(f\"‚úÖ Merged dataset: {len(df)} records\")\n",
    "            \n",
    "            if len(df) == 0:\n",
    "                print(\"‚ùå No matching records found after merge\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Debug: Check what we actually got from the merge\n",
    "            print(f\"   Sample artifact_ids from results: {results_df['artifact_id'].head().tolist()}\")\n",
    "            print(f\"   Sample artifact_ids from meta GT: {meta_gt_df['artifact_id'].head().tolist()}\")\n",
    "            print(f\"   Sample flash_classification: {results_df['flash_classification'].head().tolist()}\")\n",
    "            print(f\"   Sample meta ground_truth after merge: {df['ground_truth'].head().tolist()}\")\n",
    "            \n",
    "            # Check if flash_classification and ground_truth are identical BEFORE conversion\n",
    "            identical_before_conversion = (df['flash_classification'] == df['ground_truth']).sum()\n",
    "            print(f\"   Flash vs Meta Ground Truth identical BEFORE conversion: {identical_before_conversion}/{len(df)}\")\n",
    "            \n",
    "            if identical_before_conversion == len(df):\n",
    "                print(\"   üö® CRITICAL ERROR: flash_classification is identical to meta ground_truth!\")\n",
    "                print(\"   This means your meta ground truth data IS the Flash predictions, not human annotations.\")\n",
    "                print(\"   Check your data pipeline - Meta GT table might contain Flash results instead of human labels.\")\n",
    "                \n",
    "                # Let's check a few individual records to confirm\n",
    "                print(\"   üìã Sample records to verify:\")\n",
    "                for i in range(min(3, len(df))):\n",
    "                    row = df.iloc[i]\n",
    "                    print(f\"      Record {i+1}: artifact_id={row['artifact_id']}, flash={row['flash_classification']}, meta_gt={row['ground_truth']}\")\n",
    "            \n",
    "            # Convert to binary format\n",
    "            df['ground_truth_binary'] = (df['ground_truth'] == 100).astype(int)\n",
    "            df['flash_binary'] = (df['flash_classification'] == 100).astype(int)\n",
    "            \n",
    "            print(f\"üìä Data distribution:\")\n",
    "            print(f\"   Meta ground truth: {df['ground_truth_binary'].value_counts().to_dict()}\")\n",
    "            print(f\"   Flash predictions: {df['flash_binary'].value_counts().to_dict()}\")\n",
    "            \n",
    "            # Validate binary data\n",
    "            for col, name in [('ground_truth_binary', 'Meta ground truth'), \n",
    "                            ('flash_binary', 'Flash')]:\n",
    "                if not all(df[col].isin([0, 1])):\n",
    "                    print(f\"‚ö†Ô∏è Warning: {name} contains non-binary values\")\n",
    "                    df = df[df[col].isin([0, 1])]\n",
    "            \n",
    "            print(f\"üìä Final clean dataset: {len(df)} records\")\n",
    "            \n",
    "            # Debug sample comparisons\n",
    "            print(f\"\\nüîç DEBUG - Sample comparisons (first 5 records):\")\n",
    "            print(f\"   Columns available: {df.columns.tolist()}\")\n",
    "            for i in range(min(5, len(df))):\n",
    "                row = df.iloc[i]\n",
    "                print(f\"   Record {i+1}:\")\n",
    "                print(f\"      artifact_id: {row['artifact_id']}\")\n",
    "                print(f\"      meta_ground_truth (from Meta GT): {row['ground_truth']} -> binary: {row['ground_truth_binary']}\")\n",
    "                print(f\"      flash_classification: {row['flash_classification']} -> binary: {row['flash_binary']}\")\n",
    "                print(f\"      Flash vs Meta GT: {'‚úÖ MATCH' if row['flash_binary'] == row['ground_truth_binary'] else '‚ùå DIFFER'}\")\n",
    "                print(f\"      ---\")\n",
    "            \n",
    "            # Check if Flash predictions are identical to ground truth\n",
    "            flash_matches_gt = (df['flash_binary'] == df['ground_truth_binary']).sum()\n",
    "            print(f\"\\nüö® CRITICAL CHECK:\")\n",
    "            print(f\"   Flash predictions matching meta ground truth: {flash_matches_gt}/{len(df)} ({flash_matches_gt/len(df):.1%})\")\n",
    "            \n",
    "            if flash_matches_gt == len(df):\n",
    "                print(\"   üö® ERROR: Flash predictions are 100% identical to meta ground truth!\")\n",
    "                print(\"   This suggests the meta ground truth data is actually Flash's predictions, not human annotations.\")\n",
    "                print(\"   Check your data pipeline - the 'ground_truth' field might be populated with Flash results.\")\n",
    "            elif flash_matches_gt > len(df) * 0.95:\n",
    "                print(\"   ‚ö†Ô∏è WARNING: Flash predictions are suspiciously similar to meta ground truth (>95% match)\")\n",
    "                print(\"   This might indicate data contamination.\")\n",
    "            else:\n",
    "                print(\"   ‚úÖ Good: Flash predictions differ from meta ground truth as expected.\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading data: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def calculate_classification_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate comprehensive classification metrics for Flash\"\"\"\n",
    "        print(\"üìä Calculating Flash classification metrics...\")\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            return {}\n",
    "        \n",
    "        y_true = df['ground_truth_binary'].values\n",
    "        flash_pred = df['flash_binary'].values\n",
    "        \n",
    "        # Flash metrics - Compare Flash predictions with meta ground truth  \n",
    "        flash_metrics = self._calculate_model_metrics(y_true, flash_pred, \"Flash\")\n",
    "        \n",
    "        # Flash agreement with ground truth\n",
    "        flash_vs_ground_truth_agreement = (flash_pred == y_true).mean()\n",
    "        \n",
    "        print(f\"‚úÖ Metrics calculated:\")\n",
    "        print(f\"   Flash F1: {flash_metrics['f1_score']:.3f}\")\n",
    "        print(f\"   Flash vs GT agreement: {flash_vs_ground_truth_agreement:.1%}\")\n",
    "        \n",
    "        return {\n",
    "            'dataset_info': {\n",
    "                'total_samples': len(df),\n",
    "                'positive_samples': int((y_true == 1).sum()),\n",
    "                'negative_samples': int((y_true == 0).sum()),\n",
    "                'class_balance': float((y_true == 1).mean())\n",
    "            },\n",
    "            'flash_meta_metrics': flash_metrics,\n",
    "            'agreement_analysis': {\n",
    "                'flash_vs_ground_truth': flash_vs_ground_truth_agreement\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _calculate_model_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, model_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate detailed metrics for a single model\"\"\"\n",
    "        \n",
    "        # Ensure binary values\n",
    "        y_true = np.array(y_true).astype(int)\n",
    "        y_pred = np.array(y_pred).astype(int)\n",
    "        \n",
    "        # Validate binary values\n",
    "        if not (set(np.unique(y_true)) <= {0, 1} and set(np.unique(y_pred)) <= {0, 1}):\n",
    "            print(f\"‚ö†Ô∏è Warning: Non-binary values detected in {model_name}\")\n",
    "            print(f\"   y_true unique: {np.unique(y_true)}\")\n",
    "            print(f\"   y_pred unique: {np.unique(y_pred)}\")\n",
    "            # Force to binary\n",
    "            y_true = np.clip(y_true, 0, 1)\n",
    "            y_pred = np.clip(y_pred, 0, 1)\n",
    "        \n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        # Handle edge cases where only one class is predicted\n",
    "        unique_pred = np.unique(y_pred)\n",
    "        unique_true = np.unique(y_true)\n",
    "        \n",
    "        if len(unique_pred) == 1 or len(unique_true) == 1:\n",
    "            # Use macro average for edge cases\n",
    "            precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        else:\n",
    "            precision = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "        \n",
    "        # Confusion matrix - ensure we get 2x2 matrix\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "        \n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        elif cm.shape == (1, 1):\n",
    "            # Handle case where only one class exists\n",
    "            if unique_true[0] == 0 and unique_pred[0] == 0:\n",
    "                tn, fp, fn, tp = cm[0,0], 0, 0, 0\n",
    "            elif unique_true[0] == 1 and unique_pred[0] == 1:\n",
    "                tn, fp, fn, tp = 0, 0, 0, cm[0,0]\n",
    "            else:\n",
    "                tn, fp, fn, tp = 0, cm[0,0], 0, 0\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Unexpected confusion matrix shape for {model_name}: {cm.shape}\")\n",
    "            tn = fp = fn = tp = 0\n",
    "        \n",
    "        # Additional metrics\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # Same as recall\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # Negative Predictive Value\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Positive Predictive Value (same as precision)\n",
    "        \n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # False Negative Rate\n",
    "        \n",
    "        balanced_accuracy = (sensitivity + specificity) / 2\n",
    "        \n",
    "        # Matthews Correlation Coefficient\n",
    "        mcc_num = (tp * tn) - (fp * fn)\n",
    "        mcc_den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "        mcc = mcc_num / mcc_den if mcc_den > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'specificity': specificity,\n",
    "            'sensitivity': sensitivity,\n",
    "            'balanced_accuracy': balanced_accuracy,\n",
    "            'negative_predictive_value': npv,\n",
    "            'positive_predictive_value': ppv,\n",
    "            'false_positive_rate': fpr,\n",
    "            'false_negative_rate': fnr,\n",
    "            'matthews_correlation': mcc,\n",
    "            'confusion_matrix': {\n",
    "                'true_positives': int(tp),\n",
    "                'false_positives': int(fp), \n",
    "                'true_negatives': int(tn),\n",
    "                'false_negatives': int(fn)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def create_classification_dashboard(self, metrics: Dict[str, Any], df: pd.DataFrame) -> str:\n",
    "        \"\"\"Create comprehensive classification metrics dashboard\"\"\"\n",
    "        \n",
    "        if not metrics:\n",
    "            return \"<html><body><h1>No valid metrics to display</h1></body></html>\"\n",
    "        \n",
    "        flash_metrics = metrics['flash_meta_metrics']\n",
    "        dataset_info = metrics['dataset_info']\n",
    "        agreement = metrics['agreement_analysis']\n",
    "        \n",
    "        flash_cm = flash_metrics['confusion_matrix']\n",
    "        \n",
    "        # Performance assessment\n",
    "        if flash_metrics['f1_score'] >= 0.8:\n",
    "            flash_assessment = \"üü¢ EXCELLENT\"\n",
    "            flash_color = \"#10B981\"\n",
    "        elif flash_metrics['f1_score'] >= 0.7:\n",
    "            flash_assessment = \"üü° GOOD\" \n",
    "            flash_color = \"#F59E0B\"\n",
    "        elif flash_metrics['f1_score'] >= 0.6:\n",
    "            flash_assessment = \"üü† MODERATE\"\n",
    "            flash_color = \"#FF6B35\"\n",
    "        else:\n",
    "            flash_assessment = \"üî¥ POOR\"\n",
    "            flash_color = \"#EF4444\"\n",
    "        \n",
    "        html_content = f\"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Flash Meta Classification Metrics Dashboard</title>\n",
    "    <style>\n",
    "        * {{ margin: 0; padding: 0; box-sizing: border-box; }}\n",
    "        body {{ font-family: 'Inter', sans-serif; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%); color: #333; line-height: 1.6; min-height: 100vh; }}\n",
    "        .dashboard {{ max-width: 1800px; margin: 0 auto; padding: 20px; }}\n",
    "        .header {{ background: rgba(255,255,255,0.95); padding: 40px; border-radius: 25px; text-align: center; margin-bottom: 30px; backdrop-filter: blur(15px); box-shadow: 0 20px 60px rgba(0,0,0,0.1); }}\n",
    "        .header h1 {{ font-size: 3rem; margin-bottom: 15px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; }}\n",
    "        .header h2 {{ font-size: 1.5rem; color: #666; margin-bottom: 20px; }}\n",
    "        .model-badge {{ display: inline-block; padding: 10px 25px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%); color: white; border-radius: 30px; font-weight: bold; font-size: 1.1rem; margin: 5px; }}\n",
    "        .metrics-overview {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(180px, 1fr)); gap: 20px; margin-bottom: 40px; }}\n",
    "        .metric-card {{ background: rgba(255,255,255,0.95); padding: 25px; border-radius: 20px; text-align: center; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); transition: transform 0.3s ease; }}\n",
    "        .metric-card:hover {{ transform: translateY(-5px); }}\n",
    "        .metric-value {{ font-size: 2.5rem; font-weight: bold; margin-bottom: 10px; }}\n",
    "        .metric-label {{ font-size: 1rem; color: #666; font-weight: 600; margin-bottom: 5px; }}\n",
    "        .metric-description {{ font-size: 0.85rem; color: #888; }}\n",
    "        .excellent {{ color: #10B981; }}\n",
    "        .good {{ color: #F59E0B; }}\n",
    "        .moderate {{ color: #FF6B35; }}\n",
    "        .poor {{ color: #EF4444; }}\n",
    "        .dashboard-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(450px, 1fr)); gap: 25px; margin-bottom: 40px; }}\n",
    "        .chart-section {{ background: rgba(255,255,255,0.95); padding: 30px; border-radius: 20px; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); }}\n",
    "        .chart-title {{ font-size: 1.3rem; font-weight: bold; margin-bottom: 20px; color: #333; text-align: center; }}\n",
    "        .confusion-matrix {{ display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px; margin: 20px 0; text-align: center; font-size: 0.9rem; }}\n",
    "        .cm-cell {{ padding: 15px; border-radius: 12px; font-weight: bold; display: flex; flex-direction: column; justify-content: center; align-items: center; }}\n",
    "        .cm-header {{ background: linear-gradient(135deg, #f8f9fa, #e9ecef); color: #495057; }}\n",
    "        .cm-tp {{ background: linear-gradient(135deg, #d4edda, #c3e6cb); color: #155724; font-size: 1.8rem; }}\n",
    "        .cm-fp {{ background: linear-gradient(135deg, #f8d7da, #f1b0b7); color: #721c24; font-size: 1.8rem; }}\n",
    "        .cm-fn {{ background: linear-gradient(135deg, #fff3cd, #fce4a6); color: #856404; font-size: 1.8rem; }}\n",
    "        .cm-tn {{ background: linear-gradient(135deg, #d1ecf1, #bee5eb); color: #0c5460; font-size: 1.8rem; }}\n",
    "        .cm-label {{ font-size: 0.75rem; margin-top: 5px; opacity: 0.8; }}\n",
    "        .performance-summary {{ background: linear-gradient(135deg, #ffe6e6, #ffcccc); padding: 30px; border-radius: 20px; margin: 30px 0; text-align: center; }}\n",
    "        .performance-score {{ font-size: 3.5rem; font-weight: bold; color: {flash_color}; margin-bottom: 10px; }}\n",
    "        .performance-label {{ font-size: 1.3rem; color: #37474f; margin-bottom: 8px; }}\n",
    "        .performance-description {{ font-size: 1rem; color: #546e7a; }}\n",
    "        .comparison-section {{ background: rgba(255,255,255,0.95); padding: 30px; border-radius: 20px; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); margin-bottom: 30px; }}\n",
    "        .model-column {{ padding: 20px; border-radius: 15px; }}\n",
    "        .flash-column {{ background: linear-gradient(135deg, #ffe6e6, #ffcccc); }}\n",
    "        .model-title {{ font-size: 1.5rem; font-weight: bold; text-align: center; margin-bottom: 20px; }}\n",
    "        .metric-row {{ display: flex; justify-content: space-between; margin: 8px 0; padding: 8px; background: rgba(255,255,255,0.5); border-radius: 8px; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"dashboard\">\n",
    "        <div class=\"header\">\n",
    "            <h1>‚ö° Flash Meta Classification Metrics</h1>\n",
    "            <h2>Performance vs Meta Ground Truth</h2>\n",
    "            <div>\n",
    "                <span class=\"model-badge\">Flash Model</span>\n",
    "            </div>\n",
    "            <p style=\"margin-top: 20px; color: #666;\">\n",
    "                Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | \n",
    "                Samples: {dataset_info['total_samples']:,} | \n",
    "                Positive: {dataset_info['positive_samples']} | Negative: {dataset_info['negative_samples']}\n",
    "            </p>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"performance-summary\">\n",
    "            <div class=\"performance-score\">{flash_metrics['f1_score']:.3f}</div>\n",
    "            <div class=\"performance-label\">{flash_assessment} F1-Score (Flash Model)</div>\n",
    "            <div class=\"performance-description\">\n",
    "                Flash achieves {flash_metrics['accuracy']:.1%} accuracy with {flash_metrics['precision']:.1%} precision\n",
    "                <br>Agreement with Meta Ground Truth: {agreement['flash_vs_ground_truth']:.1%}\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"metrics-overview\">\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if flash_metrics['f1_score'] >= 0.8 else 'good' if flash_metrics['f1_score'] >= 0.7 else 'moderate' if flash_metrics['f1_score'] >= 0.6 else 'poor'}\">{flash_metrics['f1_score']:.3f}</div>\n",
    "                <div class=\"metric-label\">Flash F1-Score</div>\n",
    "                <div class=\"metric-description\">Harmonic Mean P&R</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if flash_metrics['recall'] >= 0.8 else 'good' if flash_metrics['recall'] >= 0.7 else 'moderate' if flash_metrics['recall'] >= 0.6 else 'poor'}\">{flash_metrics['recall']:.3f}</div>\n",
    "                <div class=\"metric-label\">Flash TPR</div>\n",
    "                <div class=\"metric-description\">True Positive Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if flash_metrics['specificity'] >= 0.8 else 'good' if flash_metrics['specificity'] >= 0.7 else 'moderate' if flash_metrics['specificity'] >= 0.6 else 'poor'}\">{flash_metrics['specificity']:.3f}</div>\n",
    "                <div class=\"metric-label\">Flash TNR</div>\n",
    "                <div class=\"metric-description\">True Negative Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if flash_metrics['false_positive_rate'] <= 0.2 else 'good' if flash_metrics['false_positive_rate'] <= 0.3 else 'moderate' if flash_metrics['false_positive_rate'] <= 0.4 else 'poor'}\">{flash_metrics['false_positive_rate']:.3f}</div>\n",
    "                <div class=\"metric-label\">Flash FPR</div>\n",
    "                <div class=\"metric-description\">False Positive Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if flash_metrics['false_negative_rate'] <= 0.2 else 'good' if flash_metrics['false_negative_rate'] <= 0.3 else 'moderate' if flash_metrics['false_negative_rate'] <= 0.4 else 'poor'}\">{flash_metrics['false_negative_rate']:.3f}</div>\n",
    "                <div class=\"metric-label\">Flash FNR</div>\n",
    "                <div class=\"metric-description\">False Negative Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if flash_metrics['accuracy'] >= 0.9 else 'good' if flash_metrics['accuracy'] >= 0.8 else 'moderate' if flash_metrics['accuracy'] >= 0.7 else 'poor'}\">{flash_metrics['accuracy']:.3f}</div>\n",
    "                <div class=\"metric-label\">Flash Accuracy</div>\n",
    "                <div class=\"metric-description\">Overall Correctness</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if flash_metrics['precision'] >= 0.8 else 'good' if flash_metrics['precision'] >= 0.7 else 'moderate' if flash_metrics['precision'] >= 0.6 else 'poor'}\">{flash_metrics['precision']:.3f}</div>\n",
    "                <div class=\"metric-label\">Flash Precision</div>\n",
    "                <div class=\"metric-description\">Positive Predictive Value</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if agreement['flash_vs_ground_truth'] >= 0.8 else 'good' if agreement['flash_vs_ground_truth'] >= 0.7 else 'moderate' if agreement['flash_vs_ground_truth'] >= 0.6 else 'poor'}\">{agreement['flash_vs_ground_truth']:.1%}</div>\n",
    "                <div class=\"metric-label\">Ground Truth Agreement</div>\n",
    "                <div class=\"metric-description\">Flash matches human labels</div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"comparison-section\">\n",
    "            <div class=\"chart-title\">üìä Flash Model Performance Metrics</div>\n",
    "            <div style=\"max-width: 600px; margin: 0 auto;\">\n",
    "                <div class=\"model-column flash-column\">\n",
    "                    <div class=\"model-title\">‚ö° Flash vs Meta Ground Truth</div>\n",
    "                    <div class=\"metric-row\"><span>F1-Score:</span><span class=\"{'excellent' if flash_metrics['f1_score'] >= 0.8 else 'good' if flash_metrics['f1_score'] >= 0.7 else 'moderate' if flash_metrics['f1_score'] >= 0.6 else 'poor'}\">{flash_metrics['f1_score']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Accuracy:</span><span>{flash_metrics['accuracy']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Precision:</span><span>{flash_metrics['precision']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Recall (TPR):</span><span>{flash_metrics['recall']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Specificity (TNR):</span><span>{flash_metrics['specificity']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>False Positive Rate:</span><span>{flash_metrics['false_positive_rate']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>False Negative Rate:</span><span>{flash_metrics['false_negative_rate']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Ground Truth Agreement:</span><span>{agreement['flash_vs_ground_truth']:.1%}</span></div>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"dashboard-grid\">\n",
    "            <div class=\"chart-section\">\n",
    "                <div class=\"chart-title\">‚ö° Flash Confusion Matrix</div>\n",
    "                <div class=\"confusion-matrix\">\n",
    "                    <div class=\"cm-cell cm-header\"></div>\n",
    "                    <div class=\"cm-cell cm-header\">Meta GT: 0</div>\n",
    "                    <div class=\"cm-cell cm-header\">Meta GT: 1</div>\n",
    "                    \n",
    "                    <div class=\"cm-cell cm-header\">Flash: 0</div>\n",
    "                    <div class=\"cm-cell cm-tn\">\n",
    "                        {flash_cm['true_negatives']}\n",
    "                        <div class=\"cm-label\">True Negatives</div>\n",
    "                    </div>\n",
    "                    <div class=\"cm-cell cm-fn\">\n",
    "                        {flash_cm['false_negatives']}\n",
    "                        <div class=\"cm-label\">False Negatives</div>\n",
    "                    </div>\n",
    "                    \n",
    "                    <div class=\"cm-cell cm-header\">Flash: 1</div>\n",
    "                    <div class=\"cm-cell cm-fp\">\n",
    "                        {flash_cm['false_positives']}\n",
    "                        <div class=\"cm-label\">False Positives</div>\n",
    "                    </div>\n",
    "                    <div class=\"cm-cell cm-tp\">\n",
    "                        {flash_cm['true_positives']}\n",
    "                        <div class=\"cm-label\">True Positives</div>\n",
    "                    </div>\n",
    "                </div>\n",
    "                <div style=\"text-align: center; margin-top: 15px;\">\n",
    "                    <p><strong>TP:</strong> {flash_cm['true_positives']} | <strong>FP:</strong> {flash_cm['false_positives']} | <strong>TN:</strong> {flash_cm['true_negatives']} | <strong>FN:</strong> {flash_cm['false_negatives']}</p>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        window.addEventListener('load', function() {{\n",
    "            console.log('Flash Meta Classification Dashboard Loaded');\n",
    "        }});\n",
    "    </script>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "        \n",
    "        return html_content\n",
    "    \n",
    "    def generate_classification_dashboard(self):\n",
    "        \"\"\"Main function to generate the classification dashboard\"\"\"\n",
    "        print(\"üöÄ STARTING FLASH META CLASSIFICATION DASHBOARD GENERATION\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        try:\n",
    "            # Load results with ground truth\n",
    "            df = self.load_results_with_ground_truth()\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"‚ùå No data available for dashboard\")\n",
    "                return None\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = self.calculate_classification_metrics(df)\n",
    "            \n",
    "            if not metrics:\n",
    "                print(\"‚ùå Could not calculate metrics\")\n",
    "                return None\n",
    "            \n",
    "            # Create dashboard\n",
    "            print(\"\\nüìä Creating Flash Meta Classification Dashboard...\")\n",
    "            dashboard_html = self.create_classification_dashboard(metrics, df)\n",
    "            \n",
    "            # Save files\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            dashboard_filename = f\"flash_meta_classification_dashboard_{timestamp}.html\"\n",
    "            metrics_filename = f\"flash_meta_classification_metrics_{timestamp}.json\"\n",
    "            \n",
    "            with open(dashboard_filename, \"w\", encoding='utf-8') as f:\n",
    "                f.write(dashboard_html)\n",
    "            \n",
    "            with open(metrics_filename, \"w\") as f:\n",
    "                json.dump(metrics, f, indent=2, default=str)\n",
    "            \n",
    "            print(f\"‚úÖ Dashboard saved: {dashboard_filename}\")\n",
    "            print(f\"‚úÖ Metrics saved: {metrics_filename}\")\n",
    "            \n",
    "            # Summary\n",
    "            flash_metrics = metrics['flash_meta_metrics']\n",
    "            agreement = metrics['agreement_analysis']\n",
    "            \n",
    "            print(f\"\\nüìä FLASH META CLASSIFICATION METRICS SUMMARY:\")\n",
    "            print(f\"   ‚ö° Flash Model:\")\n",
    "            print(f\"      F1-Score: {flash_metrics['f1_score']:.3f}\")\n",
    "            print(f\"      Precision: {flash_metrics['precision']:.3f}\")\n",
    "            print(f\"      Recall: {flash_metrics['recall']:.3f}\") \n",
    "            print(f\"      Accuracy: {flash_metrics['accuracy']:.3f}\")\n",
    "            print(f\"      TP: {flash_metrics['confusion_matrix']['true_positives']}, FP: {flash_metrics['confusion_matrix']['false_positives']}\")\n",
    "            print(f\"      TN: {flash_metrics['confusion_matrix']['true_negatives']}, FN: {flash_metrics['confusion_matrix']['false_negatives']}\")\n",
    "            \n",
    "            print(f\"\\n   ‚öñÔ∏è Agreement Rates:\")\n",
    "            print(f\"      Flash vs Meta Ground Truth: {agreement['flash_vs_ground_truth']:.1%}\")\n",
    "            \n",
    "            return dashboard_filename, metrics_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Dashboard generation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the Flash Meta classification dashboard generation\"\"\"\n",
    "    dashboard_generator = FlashMetaClassificationDashboard()\n",
    "    result = dashboard_generator.generate_classification_dashboard()\n",
    "    \n",
    "    if result:\n",
    "        dashboard_file, metrics_file = result\n",
    "        print(f\"\\nüéâ SUCCESS!\")\n",
    "        print(f\"üìÅ Files generated:\")\n",
    "        print(f\"   1. {dashboard_file}\")\n",
    "        print(f\"   2. {metrics_file}\")\n",
    "        print(f\"\\nüí° Open {dashboard_file} in your browser to view the dashboard!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Dashboard generation failed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4ecd543a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING FLASH WEB CLASSIFICATION DASHBOARD GENERATION\n",
      "======================================================================\n",
      "üì• Loading Flash Web results with ground truth...\n",
      "üìä Loading Flash Web results data...\n",
      "   Found 294 Flash result records\n",
      "üìä Loading Web ground truth data...\n",
      "   Found 294 web ground truth records\n",
      "   Web ground truth distribution: {100: 252, 0: 42}\n",
      "‚úÖ Merged dataset: 294 records\n",
      "   Sample artifact_ids from results: ['frag-mutti.de/fenchelsalat-fuer-fenchelhasser-a29249', 'hd.se/2025-05-24/gavan-som-kostar-kommunen-26-miljoner-kronor', 'forbes.com/sites/krisholt/2025/06/26/nyt-mini-crossword-hints-for-friday-june-27-clues-and-answers-for-todays-game-beach-feed-fillers', 'fr.de/rhein-main/euro-nach-hamburg-11640349.html', 'frag-mutti.de/bratensauce-verlaengern-das-einfachste-rezept-a62664']\n",
      "   Sample artifact_ids from web GT: ['news.de/gesundheit/855915949/corona-zahlen-kreisfreie-stadt-solingen-heute-aktuell-27-06-2025-coronavirus-news-zu-rki-fallzahlen-tote-in-nordrhein-westfalen-intensivbetten-auslastung-und-neue-covid-19-variante-nimbus/1', 'bizjournals.com/sanfrancisco/news/2025/06/27/genentech-ulcerative-colitis-vixarelimab-ibd-osmr.html', 'scmp.com/lifestyle/food-drink/article/3315878/health-benefits-superfood-prickly-pear-cactus-spiking-popularity-japan', 'plejada.pl/newsy/bruce-willis-zniknal-z-powodu-choroby-poczatkowo-zostal-zle-zdiagnozowany/2cdfxfw', 'askamanager.org/2018/09/my-coworker-keeps-making-me-do-my-old-job-manager-doesnt-believe-im-sick-and-more.html']\n",
      "   Sample flash_classification: [100, 100, 100, 100, 100]\n",
      "   Sample web ground_truth after merge: [100, 100, 100, 100, 100]\n",
      "   Flash vs Web Ground Truth identical BEFORE conversion: 294/294\n",
      "   üö® CRITICAL ERROR: flash_classification is identical to web ground_truth!\n",
      "   This means your web ground truth data IS the Flash predictions, not human annotations.\n",
      "   Check your data pipeline - Web GT table might contain Flash results instead of human labels.\n",
      "   üìã Sample records to verify:\n",
      "      Record 1: artifact_id=frag-mutti.de/fenchelsalat-fuer-fenchelhasser-a29249, flash=100, web_gt=100\n",
      "      Record 2: artifact_id=hd.se/2025-05-24/gavan-som-kostar-kommunen-26-miljoner-kronor, flash=100, web_gt=100\n",
      "      Record 3: artifact_id=forbes.com/sites/krisholt/2025/06/26/nyt-mini-crossword-hints-for-friday-june-27-clues-and-answers-for-todays-game-beach-feed-fillers, flash=100, web_gt=100\n",
      "üìä Data distribution:\n",
      "   Web ground truth: {1: 252, 0: 42}\n",
      "   Flash predictions: {1: 252, 0: 42}\n",
      "üìä Final clean dataset: 294 records\n",
      "\n",
      "üîç DEBUG - Sample comparisons (first 5 records):\n",
      "   Columns available: ['artifact_id', 'flash_classification', 'flash_reasoning', 'model_prompt', 'source', 'data_source', 'ground_truth', 'ground_truth_reasoning', 'gt_source', 'ground_truth_binary', 'flash_binary']\n",
      "   Record 1:\n",
      "      artifact_id: frag-mutti.de/fenchelsalat-fuer-fenchelhasser-a29249\n",
      "      web_ground_truth (from Web GT): 100 -> binary: 1\n",
      "      flash_classification: 100 -> binary: 1\n",
      "      Flash vs Web GT: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 2:\n",
      "      artifact_id: hd.se/2025-05-24/gavan-som-kostar-kommunen-26-miljoner-kronor\n",
      "      web_ground_truth (from Web GT): 100 -> binary: 1\n",
      "      flash_classification: 100 -> binary: 1\n",
      "      Flash vs Web GT: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 3:\n",
      "      artifact_id: forbes.com/sites/krisholt/2025/06/26/nyt-mini-crossword-hints-for-friday-june-27-clues-and-answers-for-todays-game-beach-feed-fillers\n",
      "      web_ground_truth (from Web GT): 100 -> binary: 1\n",
      "      flash_classification: 100 -> binary: 1\n",
      "      Flash vs Web GT: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 4:\n",
      "      artifact_id: fr.de/rhein-main/euro-nach-hamburg-11640349.html\n",
      "      web_ground_truth (from Web GT): 100 -> binary: 1\n",
      "      flash_classification: 100 -> binary: 1\n",
      "      Flash vs Web GT: ‚úÖ MATCH\n",
      "      ---\n",
      "   Record 5:\n",
      "      artifact_id: frag-mutti.de/bratensauce-verlaengern-das-einfachste-rezept-a62664\n",
      "      web_ground_truth (from Web GT): 100 -> binary: 1\n",
      "      flash_classification: 100 -> binary: 1\n",
      "      Flash vs Web GT: ‚úÖ MATCH\n",
      "      ---\n",
      "\n",
      "üö® CRITICAL CHECK:\n",
      "   Flash predictions matching web ground truth: 294/294 (100.0%)\n",
      "   üö® ERROR: Flash predictions are 100% identical to web ground truth!\n",
      "   This suggests the web ground truth data is actually Flash's predictions, not human annotations.\n",
      "   Check your data pipeline - the 'ground_truth' field might be populated with Flash results.\n",
      "üìä Calculating Flash classification metrics...\n",
      "‚úÖ Metrics calculated:\n",
      "   Flash F1: 1.000\n",
      "   Flash vs GT agreement: 100.0%\n",
      "\n",
      "üìä Creating Flash Web Classification Dashboard...\n",
      "‚úÖ Dashboard saved: flash_web_classification_dashboard_20250725_135750.html\n",
      "‚úÖ Metrics saved: flash_web_classification_metrics_20250725_135750.json\n",
      "\n",
      "üìä FLASH WEB CLASSIFICATION METRICS SUMMARY:\n",
      "   ‚ö° Flash Web Model:\n",
      "      F1-Score: 1.000\n",
      "      Precision: 1.000\n",
      "      Recall: 1.000\n",
      "      Accuracy: 1.000\n",
      "      TP: 252, FP: 0\n",
      "      TN: 42, FN: 0\n",
      "\n",
      "   ‚öñÔ∏è Agreement Rates:\n",
      "      Flash vs Web Ground Truth: 100.0%\n",
      "\n",
      "üéâ SUCCESS!\n",
      "üìÅ Files generated:\n",
      "   1. flash_web_classification_dashboard_20250725_135750.html\n",
      "   2. flash_web_classification_metrics_20250725_135750.json\n",
      "\n",
      "üí° Open flash_web_classification_dashboard_20250725_135750.html in your browser to view the dashboard!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Flash Web Classification Metrics Dashboard - Flash Results vs Web Ground Truth\n",
    "Calculates traditional ML metrics (F1, Precision, Recall, Confusion Matrix) for Flash decisions on Web content\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "PROJECT_ID = \"scope3-dev\"\n",
    "DATASET_ID = \"research_bs_monitoring\"\n",
    "WEB_GROUND_TRUTH_TABLE = \"BA_Web_Ground_Truth\"  # Web ground truth table\n",
    "WEB_RESULTS_TABLE = \"BA_Web_Gemini_Pro_Judge_Results\"  # Contains flash_classification for web\n",
    "\n",
    "class FlashWebClassificationDashboard:\n",
    "    def __init__(self):\n",
    "        self.client = bigquery.Client(project=PROJECT_ID)\n",
    "        \n",
    "    def load_results_with_ground_truth(self) -> pd.DataFrame:\n",
    "        \"\"\"Load Flash results with real ground truth from Web Ground Truth table\"\"\"\n",
    "        print(\"üì• Loading Flash Web results with ground truth...\")\n",
    "        \n",
    "        # Get Flash classifications from the web results table\n",
    "        results_query = f\"\"\"\n",
    "        SELECT \n",
    "            artifact_id,\n",
    "            flash_classification,\n",
    "            flash_reasoning,\n",
    "            model_prompt,\n",
    "            source,\n",
    "            data_source\n",
    "        FROM `{PROJECT_ID}.{DATASET_ID}.{WEB_RESULTS_TABLE}`\n",
    "        WHERE flash_classification IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get real ground truth from Web Ground Truth table\n",
    "        web_gt_query = f\"\"\"\n",
    "        SELECT \n",
    "            artifact_id,\n",
    "            correct_classification as ground_truth,\n",
    "            correct_reasoning as ground_truth_reasoning,\n",
    "            source as gt_source\n",
    "        FROM `{PROJECT_ID}.{DATASET_ID}.{WEB_GROUND_TRUTH_TABLE}`\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(\"üìä Loading Flash Web results data...\")\n",
    "            results_df = self.client.query(results_query).to_dataframe()\n",
    "            print(f\"   Found {len(results_df)} Flash result records\")\n",
    "            \n",
    "            print(\"üìä Loading Web ground truth data...\")\n",
    "            web_gt_df = self.client.query(web_gt_query).to_dataframe()\n",
    "            print(f\"   Found {len(web_gt_df)} web ground truth records\")\n",
    "            \n",
    "            # Debug: Check ground truth distribution\n",
    "            if len(web_gt_df) > 0:\n",
    "                gt_dist = web_gt_df['ground_truth'].value_counts().to_dict()\n",
    "                print(f\"   Web ground truth distribution: {gt_dist}\")\n",
    "            \n",
    "            # Join results with ground truth\n",
    "            df = results_df.merge(web_gt_df, on='artifact_id', how='inner')\n",
    "            print(f\"‚úÖ Merged dataset: {len(df)} records\")\n",
    "            \n",
    "            if len(df) == 0:\n",
    "                print(\"‚ùå No matching records found after merge\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Debug: Check what we actually got from the merge\n",
    "            print(f\"   Sample artifact_ids from results: {results_df['artifact_id'].head().tolist()}\")\n",
    "            print(f\"   Sample artifact_ids from web GT: {web_gt_df['artifact_id'].head().tolist()}\")\n",
    "            print(f\"   Sample flash_classification: {results_df['flash_classification'].head().tolist()}\")\n",
    "            print(f\"   Sample web ground_truth after merge: {df['ground_truth'].head().tolist()}\")\n",
    "            \n",
    "            # Check if flash_classification and ground_truth are identical BEFORE conversion\n",
    "            identical_before_conversion = (df['flash_classification'] == df['ground_truth']).sum()\n",
    "            print(f\"   Flash vs Web Ground Truth identical BEFORE conversion: {identical_before_conversion}/{len(df)}\")\n",
    "            \n",
    "            if identical_before_conversion == len(df):\n",
    "                print(\"   üö® CRITICAL ERROR: flash_classification is identical to web ground_truth!\")\n",
    "                print(\"   This means your web ground truth data IS the Flash predictions, not human annotations.\")\n",
    "                print(\"   Check your data pipeline - Web GT table might contain Flash results instead of human labels.\")\n",
    "                \n",
    "                # Let's check a few individual records to confirm\n",
    "                print(\"   üìã Sample records to verify:\")\n",
    "                for i in range(min(3, len(df))):\n",
    "                    row = df.iloc[i]\n",
    "                    print(f\"      Record {i+1}: artifact_id={row['artifact_id']}, flash={row['flash_classification']}, web_gt={row['ground_truth']}\")\n",
    "            \n",
    "            # Convert to binary format\n",
    "            df['ground_truth_binary'] = (df['ground_truth'] == 100).astype(int)\n",
    "            df['flash_binary'] = (df['flash_classification'] == 100).astype(int)\n",
    "            \n",
    "            print(f\"üìä Data distribution:\")\n",
    "            print(f\"   Web ground truth: {df['ground_truth_binary'].value_counts().to_dict()}\")\n",
    "            print(f\"   Flash predictions: {df['flash_binary'].value_counts().to_dict()}\")\n",
    "            \n",
    "            # Validate binary data\n",
    "            for col, name in [('ground_truth_binary', 'Web ground truth'), \n",
    "                            ('flash_binary', 'Flash')]:\n",
    "                if not all(df[col].isin([0, 1])):\n",
    "                    print(f\"‚ö†Ô∏è Warning: {name} contains non-binary values\")\n",
    "                    df = df[df[col].isin([0, 1])]\n",
    "            \n",
    "            print(f\"üìä Final clean dataset: {len(df)} records\")\n",
    "            \n",
    "            # Debug sample comparisons\n",
    "            print(f\"\\nüîç DEBUG - Sample comparisons (first 5 records):\")\n",
    "            print(f\"   Columns available: {df.columns.tolist()}\")\n",
    "            for i in range(min(5, len(df))):\n",
    "                row = df.iloc[i]\n",
    "                print(f\"   Record {i+1}:\")\n",
    "                print(f\"      artifact_id: {row['artifact_id']}\")\n",
    "                print(f\"      web_ground_truth (from Web GT): {row['ground_truth']} -> binary: {row['ground_truth_binary']}\")\n",
    "                print(f\"      flash_classification: {row['flash_classification']} -> binary: {row['flash_binary']}\")\n",
    "                print(f\"      Flash vs Web GT: {'‚úÖ MATCH' if row['flash_binary'] == row['ground_truth_binary'] else '‚ùå DIFFER'}\")\n",
    "                print(f\"      ---\")\n",
    "            \n",
    "            # Check if Flash predictions are identical to ground truth\n",
    "            flash_matches_gt = (df['flash_binary'] == df['ground_truth_binary']).sum()\n",
    "            print(f\"\\nüö® CRITICAL CHECK:\")\n",
    "            print(f\"   Flash predictions matching web ground truth: {flash_matches_gt}/{len(df)} ({flash_matches_gt/len(df):.1%})\")\n",
    "            \n",
    "            if flash_matches_gt == len(df):\n",
    "                print(\"   üö® ERROR: Flash predictions are 100% identical to web ground truth!\")\n",
    "                print(\"   This suggests the web ground truth data is actually Flash's predictions, not human annotations.\")\n",
    "                print(\"   Check your data pipeline - the 'ground_truth' field might be populated with Flash results.\")\n",
    "            elif flash_matches_gt > len(df) * 0.95:\n",
    "                print(\"   ‚ö†Ô∏è WARNING: Flash predictions are suspiciously similar to web ground truth (>95% match)\")\n",
    "                print(\"   This might indicate data contamination.\")\n",
    "            else:\n",
    "                print(\"   ‚úÖ Good: Flash predictions differ from web ground truth as expected.\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading data: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def calculate_classification_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate comprehensive classification metrics for Flash\"\"\"\n",
    "        print(\"üìä Calculating Flash classification metrics...\")\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            return {}\n",
    "        \n",
    "        y_true = df['ground_truth_binary'].values\n",
    "        flash_pred = df['flash_binary'].values\n",
    "        \n",
    "        # Flash metrics - Compare Flash predictions with web ground truth  \n",
    "        flash_metrics = self._calculate_model_metrics(y_true, flash_pred, \"Flash\")\n",
    "        \n",
    "        # Flash agreement with ground truth\n",
    "        flash_vs_ground_truth_agreement = (flash_pred == y_true).mean()\n",
    "        \n",
    "        print(f\"‚úÖ Metrics calculated:\")\n",
    "        print(f\"   Flash F1: {flash_metrics['f1_score']:.3f}\")\n",
    "        print(f\"   Flash vs GT agreement: {flash_vs_ground_truth_agreement:.1%}\")\n",
    "        \n",
    "        return {\n",
    "            'dataset_info': {\n",
    "                'total_samples': len(df),\n",
    "                'positive_samples': int((y_true == 1).sum()),\n",
    "                'negative_samples': int((y_true == 0).sum()),\n",
    "                'class_balance': float((y_true == 1).mean())\n",
    "            },\n",
    "            'flash_web_metrics': flash_metrics,\n",
    "            'agreement_analysis': {\n",
    "                'flash_vs_ground_truth': flash_vs_ground_truth_agreement\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _calculate_model_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, model_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate detailed metrics for a single model\"\"\"\n",
    "        \n",
    "        # Ensure binary values\n",
    "        y_true = np.array(y_true).astype(int)\n",
    "        y_pred = np.array(y_pred).astype(int)\n",
    "        \n",
    "        # Validate binary values\n",
    "        if not (set(np.unique(y_true)) <= {0, 1} and set(np.unique(y_pred)) <= {0, 1}):\n",
    "            print(f\"‚ö†Ô∏è Warning: Non-binary values detected in {model_name}\")\n",
    "            print(f\"   y_true unique: {np.unique(y_true)}\")\n",
    "            print(f\"   y_pred unique: {np.unique(y_pred)}\")\n",
    "            # Force to binary\n",
    "            y_true = np.clip(y_true, 0, 1)\n",
    "            y_pred = np.clip(y_pred, 0, 1)\n",
    "        \n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        # Handle edge cases where only one class is predicted\n",
    "        unique_pred = np.unique(y_pred)\n",
    "        unique_true = np.unique(y_true)\n",
    "        \n",
    "        if len(unique_pred) == 1 or len(unique_true) == 1:\n",
    "            # Use macro average for edge cases\n",
    "            precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        else:\n",
    "            precision = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "        \n",
    "        # Confusion matrix - ensure we get 2x2 matrix\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "        \n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        elif cm.shape == (1, 1):\n",
    "            # Handle case where only one class exists\n",
    "            if unique_true[0] == 0 and unique_pred[0] == 0:\n",
    "                tn, fp, fn, tp = cm[0,0], 0, 0, 0\n",
    "            elif unique_true[0] == 1 and unique_pred[0] == 1:\n",
    "                tn, fp, fn, tp = 0, 0, 0, cm[0,0]\n",
    "            else:\n",
    "                tn, fp, fn, tp = 0, cm[0,0], 0, 0\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Unexpected confusion matrix shape for {model_name}: {cm.shape}\")\n",
    "            tn = fp = fn = tp = 0\n",
    "        \n",
    "        # Additional metrics\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # Same as recall\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # Negative Predictive Value\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Positive Predictive Value (same as precision)\n",
    "        \n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # False Negative Rate\n",
    "        \n",
    "        balanced_accuracy = (sensitivity + specificity) / 2\n",
    "        \n",
    "        # Matthews Correlation Coefficient\n",
    "        mcc_num = (tp * tn) - (fp * fn)\n",
    "        mcc_den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "        mcc = mcc_num / mcc_den if mcc_den > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'specificity': specificity,\n",
    "            'sensitivity': sensitivity,\n",
    "            'balanced_accuracy': balanced_accuracy,\n",
    "            'negative_predictive_value': npv,\n",
    "            'positive_predictive_value': ppv,\n",
    "            'false_positive_rate': fpr,\n",
    "            'false_negative_rate': fnr,\n",
    "            'matthews_correlation': mcc,\n",
    "            'confusion_matrix': {\n",
    "                'true_positives': int(tp),\n",
    "                'false_positives': int(fp), \n",
    "                'true_negatives': int(tn),\n",
    "                'false_negatives': int(fn)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def create_classification_dashboard(self, metrics: Dict[str, Any], df: pd.DataFrame) -> str:\n",
    "        \"\"\"Create comprehensive classification metrics dashboard\"\"\"\n",
    "        \n",
    "        if not metrics:\n",
    "            return \"<html><body><h1>No valid metrics to display</h1></body></html>\"\n",
    "        \n",
    "        flash_metrics = metrics['flash_web_metrics']\n",
    "        dataset_info = metrics['dataset_info']\n",
    "        agreement = metrics['agreement_analysis']\n",
    "        \n",
    "        flash_cm = flash_metrics['confusion_matrix']\n",
    "        \n",
    "        # Performance assessment\n",
    "        if flash_metrics['f1_score'] >= 0.8:\n",
    "            flash_assessment = \"üü¢ EXCELLENT\"\n",
    "            flash_color = \"#10B981\"\n",
    "        elif flash_metrics['f1_score'] >= 0.7:\n",
    "            flash_assessment = \"üü° GOOD\" \n",
    "            flash_color = \"#F59E0B\"\n",
    "        elif flash_metrics['f1_score'] >= 0.6:\n",
    "            flash_assessment = \"üü† MODERATE\"\n",
    "            flash_color = \"#FF6B35\"\n",
    "        else:\n",
    "            flash_assessment = \"üî¥ POOR\"\n",
    "            flash_color = \"#EF4444\"\n",
    "        \n",
    "        html_content = f\"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Flash Web Classification Metrics Dashboard</title>\n",
    "    <style>\n",
    "        * {{ margin: 0; padding: 0; box-sizing: border-box; }}\n",
    "        body {{ font-family: 'Inter', sans-serif; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%); color: #333; line-height: 1.6; min-height: 100vh; }}\n",
    "        .dashboard {{ max-width: 1800px; margin: 0 auto; padding: 20px; }}\n",
    "        .header {{ background: rgba(255,255,255,0.95); padding: 40px; border-radius: 25px; text-align: center; margin-bottom: 30px; backdrop-filter: blur(15px); box-shadow: 0 20px 60px rgba(0,0,0,0.1); }}\n",
    "        .header h1 {{ font-size: 3rem; margin-bottom: 15px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; }}\n",
    "        .header h2 {{ font-size: 1.5rem; color: #666; margin-bottom: 20px; }}\n",
    "        .model-badge {{ display: inline-block; padding: 10px 25px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%); color: white; border-radius: 30px; font-weight: bold; font-size: 1.1rem; margin: 5px; }}\n",
    "        .metrics-overview {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(180px, 1fr)); gap: 20px; margin-bottom: 40px; }}\n",
    "        .metric-card {{ background: rgba(255,255,255,0.95); padding: 25px; border-radius: 20px; text-align: center; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); transition: transform 0.3s ease; }}\n",
    "        .metric-card:hover {{ transform: translateY(-5px); }}\n",
    "        .metric-value {{ font-size: 2.5rem; font-weight: bold; margin-bottom: 10px; }}\n",
    "        .metric-label {{ font-size: 1rem; color: #666; font-weight: 600; margin-bottom: 5px; }}\n",
    "        .metric-description {{ font-size: 0.85rem; color: #888; }}\n",
    "        .excellent {{ color: #10B981; }}\n",
    "        .good {{ color: #F59E0B; }}\n",
    "        .moderate {{ color: #FF6B35; }}\n",
    "        .poor {{ color: #EF4444; }}\n",
    "        .dashboard-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(450px, 1fr)); gap: 25px; margin-bottom: 40px; }}\n",
    "        .chart-section {{ background: rgba(255,255,255,0.95); padding: 30px; border-radius: 20px; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); }}\n",
    "        .chart-title {{ font-size: 1.3rem; font-weight: bold; margin-bottom: 20px; color: #333; text-align: center; }}\n",
    "        .confusion-matrix {{ display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px; margin: 20px 0; text-align: center; font-size: 0.9rem; }}\n",
    "        .cm-cell {{ padding: 15px; border-radius: 12px; font-weight: bold; display: flex; flex-direction: column; justify-content: center; align-items: center; }}\n",
    "        .cm-header {{ background: linear-gradient(135deg, #f8f9fa, #e9ecef); color: #495057; }}\n",
    "        .cm-tp {{ background: linear-gradient(135deg, #d4edda, #c3e6cb); color: #155724; font-size: 1.8rem; }}\n",
    "        .cm-fp {{ background: linear-gradient(135deg, #f8d7da, #f1b0b7); color: #721c24; font-size: 1.8rem; }}\n",
    "        .cm-fn {{ background: linear-gradient(135deg, #fff3cd, #fce4a6); color: #856404; font-size: 1.8rem; }}\n",
    "        .cm-tn {{ background: linear-gradient(135deg, #d1ecf1, #bee5eb); color: #0c5460; font-size: 1.8rem; }}\n",
    "        .cm-label {{ font-size: 0.75rem; margin-top: 5px; opacity: 0.8; }}\n",
    "        .performance-summary {{ background: linear-gradient(135deg, #ffe6e6, #ffcccc); padding: 30px; border-radius: 20px; margin: 30px 0; text-align: center; }}\n",
    "        .performance-score {{ font-size: 3.5rem; font-weight: bold; color: {flash_color}; margin-bottom: 10px; }}\n",
    "        .performance-label {{ font-size: 1.3rem; color: #37474f; margin-bottom: 8px; }}\n",
    "        .performance-description {{ font-size: 1rem; color: #546e7a; }}\n",
    "        .comparison-section {{ background: rgba(255,255,255,0.95); padding: 30px; border-radius: 20px; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); margin-bottom: 30px; }}\n",
    "        .model-column {{ padding: 20px; border-radius: 15px; }}\n",
    "        .flash-column {{ background: linear-gradient(135deg, #ffe6e6, #ffcccc); }}\n",
    "        .model-title {{ font-size: 1.5rem; font-weight: bold; text-align: center; margin-bottom: 20px; }}\n",
    "        .metric-row {{ display: flex; justify-content: space-between; margin: 8px 0; padding: 8px; background: rgba(255,255,255,0.5); border-radius: 8px; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"dashboard\">\n",
    "        <div class=\"header\">\n",
    "            <h1>‚ö° Flash Web Classification Metrics</h1>\n",
    "            <h2>Performance vs Web Ground Truth</h2>\n",
    "            <div>\n",
    "                <span class=\"model-badge\">Flash Web Model</span>\n",
    "            </div>\n",
    "            <p style=\"margin-top: 20px; color: #666;\">\n",
    "                Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | \n",
    "                Samples: {dataset_info['total_samples']:,} | \n",
    "                Positive: {dataset_info['positive_samples']} | Negative: {dataset_info['negative_samples']}\n",
    "            </p>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"performance-summary\">\n",
    "            <div class=\"performance-score\">{flash_metrics['f1_score']:.3f}</div>\n",
    "            <div class=\"performance-label\">{flash_assessment} F1-Score (Flash Web Model)</div>\n",
    "            <div class=\"performance-description\">\n",
    "                Flash achieves {flash_metrics['accuracy']:.1%} accuracy with {flash_metrics['precision']:.1%} precision on web content\n",
    "                <br>Agreement with Web Ground Truth: {agreement['flash_vs_ground_truth']:.1%}\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"metrics-overview\">\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if flash_metrics['f1_score'] >= 0.8 else 'good' if flash_metrics['f1_score'] >= 0.7 else 'moderate' if flash_metrics['f1_score'] >= 0.6 else 'poor'}\">{flash_metrics['f1_score']:.3f}</div>\n",
    "                <div class=\"metric-label\">Flash F1-Score</div>\n",
    "                <div class=\"metric-description\">Harmonic Mean P&R</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if flash_metrics['recall'] >= 0.8 else 'good' if flash_metrics['recall'] >= 0.7 else 'moderate' if flash_metrics['recall'] >= 0.6 else 'poor'}\">{flash_metrics['recall']:.3f}</div>\n",
    "                <div class=\"metric-label\">Flash TPR</div>\n",
    "                <div class=\"metric-description\">True Positive Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if flash_metrics['specificity'] >= 0.8 else 'good' if flash_metrics['specificity'] >= 0.7 else 'moderate' if flash_metrics['specificity'] >= 0.6 else 'poor'}\">{flash_metrics['specificity']:.3f}</div>\n",
    "                <div class=\"metric-label\">Flash TNR</div>\n",
    "                <div class=\"metric-description\">True Negative Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if flash_metrics['false_positive_rate'] <= 0.2 else 'good' if flash_metrics['false_positive_rate'] <= 0.3 else 'moderate' if flash_metrics['false_positive_rate'] <= 0.4 else 'poor'}\">{flash_metrics['false_positive_rate']:.3f}</div>\n",
    "                <div class=\"metric-label\">Flash FPR</div>\n",
    "                <div class=\"metric-description\">False Positive Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if flash_metrics['false_negative_rate'] <= 0.2 else 'good' if flash_metrics['false_negative_rate'] <= 0.3 else 'moderate' if flash_metrics['false_negative_rate'] <= 0.4 else 'poor'}\">{flash_metrics['false_negative_rate']:.3f}</div>\n",
    "                <div class=\"metric-label\">Flash FNR</div>\n",
    "                <div class=\"metric-description\">False Negative Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if flash_metrics['accuracy'] >= 0.9 else 'good' if flash_metrics['accuracy'] >= 0.8 else 'moderate' if flash_metrics['accuracy'] >= 0.7 else 'poor'}\">{flash_metrics['accuracy']:.3f}</div>\n",
    "                <div class=\"metric-label\">Flash Accuracy</div>\n",
    "                <div class=\"metric-description\">Overall Correctness</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if flash_metrics['precision'] >= 0.8 else 'good' if flash_metrics['precision'] >= 0.7 else 'moderate' if flash_metrics['precision'] >= 0.6 else 'poor'}\">{flash_metrics['precision']:.3f}</div>\n",
    "                <div class=\"metric-label\">Flash Precision</div>\n",
    "                <div class=\"metric-description\">Positive Predictive Value</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if agreement['flash_vs_ground_truth'] >= 0.8 else 'good' if agreement['flash_vs_ground_truth'] >= 0.7 else 'moderate' if agreement['flash_vs_ground_truth'] >= 0.6 else 'poor'}\">{agreement['flash_vs_ground_truth']:.1%}</div>\n",
    "                <div class=\"metric-label\">Ground Truth Agreement</div>\n",
    "                <div class=\"metric-description\">Flash matches human web labels</div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"comparison-section\">\n",
    "            <div class=\"chart-title\">üìä Flash Web Model Performance Metrics</div>\n",
    "            <div style=\"max-width: 600px; margin: 0 auto;\">\n",
    "                <div class=\"model-column flash-column\">\n",
    "                    <div class=\"model-title\">‚ö° Flash vs Web Ground Truth</div>\n",
    "                    <div class=\"metric-row\"><span>F1-Score:</span><span class=\"{'excellent' if flash_metrics['f1_score'] >= 0.8 else 'good' if flash_metrics['f1_score'] >= 0.7 else 'moderate' if flash_metrics['f1_score'] >= 0.6 else 'poor'}\">{flash_metrics['f1_score']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Accuracy:</span><span>{flash_metrics['accuracy']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Precision:</span><span>{flash_metrics['precision']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Recall (TPR):</span><span>{flash_metrics['recall']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Specificity (TNR):</span><span>{flash_metrics['specificity']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>False Positive Rate:</span><span>{flash_metrics['false_positive_rate']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>False Negative Rate:</span><span>{flash_metrics['false_negative_rate']:.3f}</span></div>\n",
    "                    <div class=\"metric-row\"><span>Web Ground Truth Agreement:</span><span>{agreement['flash_vs_ground_truth']:.1%}</span></div>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"dashboard-grid\">\n",
    "            <div class=\"chart-section\">\n",
    "                <div class=\"chart-title\">‚ö° Flash Web Confusion Matrix</div>\n",
    "                <div class=\"confusion-matrix\">\n",
    "                    <div class=\"cm-cell cm-header\"></div>\n",
    "                    <div class=\"cm-cell cm-header\">Web GT: 0</div>\n",
    "                    <div class=\"cm-cell cm-header\">Web GT: 1</div>\n",
    "                    \n",
    "                    <div class=\"cm-cell cm-header\">Flash: 0</div>\n",
    "                    <div class=\"cm-cell cm-tn\">\n",
    "                        {flash_cm['true_negatives']}\n",
    "                        <div class=\"cm-label\">True Negatives</div>\n",
    "                    </div>\n",
    "                    <div class=\"cm-cell cm-fn\">\n",
    "                        {flash_cm['false_negatives']}\n",
    "                        <div class=\"cm-label\">False Negatives</div>\n",
    "                    </div>\n",
    "                    \n",
    "                    <div class=\"cm-cell cm-header\">Flash: 1</div>\n",
    "                    <div class=\"cm-cell cm-fp\">\n",
    "                        {flash_cm['false_positives']}\n",
    "                        <div class=\"cm-label\">False Positives</div>\n",
    "                    </div>\n",
    "                    <div class=\"cm-cell cm-tp\">\n",
    "                        {flash_cm['true_positives']}\n",
    "                        <div class=\"cm-label\">True Positives</div>\n",
    "                    </div>\n",
    "                </div>\n",
    "                <div style=\"text-align: center; margin-top: 15px;\">\n",
    "                    <p><strong>TP:</strong> {flash_cm['true_positives']} | <strong>FP:</strong> {flash_cm['false_positives']} | <strong>TN:</strong> {flash_cm['true_negatives']} | <strong>FN:</strong> {flash_cm['false_negatives']}</p>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        window.addEventListener('load', function() {{\n",
    "            console.log('Flash Web Classification Dashboard Loaded');\n",
    "        }});\n",
    "    </script>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "        \n",
    "        return html_content\n",
    "    \n",
    "    def generate_classification_dashboard(self):\n",
    "        \"\"\"Main function to generate the classification dashboard\"\"\"\n",
    "        print(\"üöÄ STARTING FLASH WEB CLASSIFICATION DASHBOARD GENERATION\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        try:\n",
    "            # Load results with ground truth\n",
    "            df = self.load_results_with_ground_truth()\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"‚ùå No data available for dashboard\")\n",
    "                return None\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = self.calculate_classification_metrics(df)\n",
    "            \n",
    "            if not metrics:\n",
    "                print(\"‚ùå Could not calculate metrics\")\n",
    "                return None\n",
    "            \n",
    "            # Create dashboard\n",
    "            print(\"\\nüìä Creating Flash Web Classification Dashboard...\")\n",
    "            dashboard_html = self.create_classification_dashboard(metrics, df)\n",
    "            \n",
    "            # Save files\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            dashboard_filename = f\"flash_web_classification_dashboard_{timestamp}.html\"\n",
    "            metrics_filename = f\"flash_web_classification_metrics_{timestamp}.json\"\n",
    "            \n",
    "            with open(dashboard_filename, \"w\", encoding='utf-8') as f:\n",
    "                f.write(dashboard_html)\n",
    "            \n",
    "            with open(metrics_filename, \"w\") as f:\n",
    "                json.dump(metrics, f, indent=2, default=str)\n",
    "            \n",
    "            print(f\"‚úÖ Dashboard saved: {dashboard_filename}\")\n",
    "            print(f\"‚úÖ Metrics saved: {metrics_filename}\")\n",
    "            \n",
    "            # Summary\n",
    "            flash_metrics = metrics['flash_web_metrics']\n",
    "            agreement = metrics['agreement_analysis']\n",
    "            \n",
    "            print(f\"\\nüìä FLASH WEB CLASSIFICATION METRICS SUMMARY:\")\n",
    "            print(f\"   ‚ö° Flash Web Model:\")\n",
    "            print(f\"      F1-Score: {flash_metrics['f1_score']:.3f}\")\n",
    "            print(f\"      Precision: {flash_metrics['precision']:.3f}\")\n",
    "            print(f\"      Recall: {flash_metrics['recall']:.3f}\") \n",
    "            print(f\"      Accuracy: {flash_metrics['accuracy']:.3f}\")\n",
    "            print(f\"      TP: {flash_metrics['confusion_matrix']['true_positives']}, FP: {flash_metrics['confusion_matrix']['false_positives']}\")\n",
    "            print(f\"      TN: {flash_metrics['confusion_matrix']['true_negatives']}, FN: {flash_metrics['confusion_matrix']['false_negatives']}\")\n",
    "            \n",
    "            print(f\"\\n   ‚öñÔ∏è Agreement Rates:\")\n",
    "            print(f\"      Flash vs Web Ground Truth: {agreement['flash_vs_ground_truth']:.1%}\")\n",
    "            \n",
    "            return dashboard_filename, metrics_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Dashboard generation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the Flash Web classification dashboard generation\"\"\"\n",
    "    dashboard_generator = FlashWebClassificationDashboard()\n",
    "    result = dashboard_generator.generate_classification_dashboard()\n",
    "    \n",
    "    if result:\n",
    "        dashboard_file, metrics_file = result\n",
    "        print(f\"\\nüéâ SUCCESS!\")\n",
    "        print(f\"üìÅ Files generated:\")\n",
    "        print(f\"   1. {dashboard_file}\")\n",
    "        print(f\"   2. {metrics_file}\")\n",
    "        print(f\"\\nüí° Open {dashboard_file} in your browser to view the dashboard!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Dashboard generation failed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "060e714f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING FLASH CSV META CLASSIFICATION DASHBOARD GENERATION (FIXED)\n",
      "======================================================================\n",
      "üìÅ Using CSV file: texts_images_classification_results_meta_multimodal_dataset_300.csv\n",
      "üì• Loading Flash CSV results with Meta ground truth...\n",
      "üì• Loading Flash results from CSV: texts_images_classification_results_meta_multimodal_dataset_300.csv\n",
      "‚úÖ Loaded 300 Flash results from CSV\n",
      "üìä CSV columns: ['source', 'artifact_id', 'model_id', 'model_prompt', 'artifact', 'text_content', 'classified_at', 'correct_classification', 'correct_reasoning', 'artifact_json_gcs_url', 'asset_filepaths', 'detected_language', 'topics', 'text_length', 'asset_count']\n",
      "üéØ Auto-detected ID column: 'artifact_id'\n",
      "üßπ Cleaning artifact_ids...\n",
      "   Cleaned 300 ‚Üí 300 records\n",
      "   Sample artifact_ids: ['meta:2247224725729245', 'meta:1044227981254015', 'meta:1790289251530852', 'meta:122172716408563103', 'meta:709838841794824']\n",
      "üìä Flash CSV classification distribution: {100: 200, 0: 100}\n",
      "üìä Loading Meta ground truth data with flexible matching...\n",
      "   Found 299 meta ground truth records\n",
      "üßπ Cleaning ground truth artifact_ids...\n",
      "   Cleaned 299 ‚Üí 299 ground truth records\n",
      "   Sample GT artifact_ids: ['meta:122140809938790694', 'meta:4050755845195196', 'meta:1173810271458869', 'meta:1297387721748313', 'meta:1138085658345605']\n",
      "   Meta ground truth distribution: {100: 187, 0: 112}\n",
      "\n",
      "üîç Pre-merge ID analysis:\n",
      "   CSV unique IDs: 300\n",
      "   GT unique IDs: 299\n",
      "   Direct overlap: 299 IDs\n",
      "\n",
      "üîó Performing merge...\n",
      "‚úÖ Merged dataset: 299 records\n",
      "   Flash CSV vs Meta Ground Truth identical: 265/299 (88.6%)\n",
      "üìä Final data distribution:\n",
      "   Meta ground truth: {1: 187, 0: 112}\n",
      "   Flash CSV predictions: {1: 199, 0: 100}\n",
      "üìä Final clean dataset: 299 records\n",
      "üìä Calculating Flash classification metrics...\n",
      "‚úÖ Metrics calculated:\n",
      "   Flash F1: 0.912\n",
      "   Flash vs GT agreement: 88.6%\n",
      "\n",
      "üìä Creating Flash CSV Meta Classification Dashboard...\n",
      "‚úÖ Dashboard saved: flash_csv_meta_classification_dashboard_20250725_135752.html\n",
      "‚úÖ Metrics saved: flash_csv_meta_classification_metrics_20250725_135752.json\n",
      "\n",
      "üìä FLASH CSV META CLASSIFICATION METRICS SUMMARY:\n",
      "   ‚ö° Flash CSV Model:\n",
      "      F1-Score: 0.912\n",
      "      Precision: 0.884\n",
      "      Recall: 0.941\n",
      "      Accuracy: 0.886\n",
      "      TP: 176, FP: 23\n",
      "      TN: 89, FN: 11\n",
      "\n",
      "   ‚öñÔ∏è Agreement Rates:\n",
      "      Flash CSV vs Meta Ground Truth: 88.6%\n",
      "\n",
      "üéâ SUCCESS!\n",
      "üìÅ Files generated:\n",
      "   1. flash_csv_meta_classification_dashboard_20250725_135752.html\n",
      "   2. flash_csv_meta_classification_metrics_20250725_135752.json\n",
      "\n",
      "üí° Open flash_csv_meta_classification_dashboard_20250725_135752.html in your browser to view the dashboard!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Flash Meta Classification Metrics Dashboard - Fixed ID Matching\n",
    "Enhanced version with better artifact_id matching and debugging\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "PROJECT_ID = \"scope3-dev\"\n",
    "DATASET_ID = \"research_bs_monitoring\"\n",
    "META_GROUND_TRUTH_TABLE = \"BA_Meta_Ground_Truth\"\n",
    "\n",
    "class FlashMetaCSVClassificationDashboard:\n",
    "    def __init__(self, id_column_name=None, csv_file_path=None):\n",
    "        self.client = bigquery.Client(project=PROJECT_ID)\n",
    "        self.csv_file_path = csv_file_path or \"texts_images_classification_results_meta_multimodal_dataset_300.csv\"\n",
    "        self.id_column_name = id_column_name  # Allow manual specification\n",
    "        \n",
    "    def load_csv_file(self) -> pd.DataFrame:\n",
    "        \"\"\"Load Flash results from the specific CSV file with improved ID handling\"\"\"\n",
    "        print(f\"üì• Loading Flash results from CSV: {self.csv_file_path}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(self.csv_file_path)\n",
    "            \n",
    "            print(f\"‚úÖ Loaded {len(df)} Flash results from CSV\")\n",
    "            print(f\"üìä CSV columns: {df.columns.tolist()}\")\n",
    "            \n",
    "            # Auto-detect ID column - try different possible names\n",
    "            if self.id_column_name:\n",
    "                # Use manually specified column name\n",
    "                if self.id_column_name in df.columns:\n",
    "                    id_column = self.id_column_name\n",
    "                    print(f\"üéØ Using manually specified ID column: '{id_column}'\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Specified ID column '{self.id_column_name}' not found in CSV\")\n",
    "            else:\n",
    "                # Auto-detect from common names\n",
    "                id_column_candidates = ['artifact_id', 'artifact', 'id', 'sample_id', 'record_id']\n",
    "                id_column = None\n",
    "                \n",
    "                for candidate in id_column_candidates:\n",
    "                    if candidate in df.columns:\n",
    "                        id_column = candidate\n",
    "                        print(f\"üéØ Auto-detected ID column: '{id_column}'\")\n",
    "                        break\n",
    "            \n",
    "            if id_column is None:\n",
    "                print(f\"‚ùå No ID column found. Available columns: {df.columns.tolist()}\")\n",
    "                print(f\"   Looking for any of: {id_column_candidates}\")\n",
    "                raise ValueError(f\"No ID column found in CSV\")\n",
    "            \n",
    "            # Validate required columns\n",
    "            required_columns = [id_column, 'correct_classification']\n",
    "            missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "            if missing_columns:\n",
    "                raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "            \n",
    "            # Standardize ID column name to 'artifact_id'\n",
    "            if id_column != 'artifact_id':\n",
    "                df['artifact_id'] = df[id_column]\n",
    "                print(f\"üîÑ Standardized '{id_column}' ‚Üí 'artifact_id'\")\n",
    "            \n",
    "            # Clean and standardize artifact_ids\n",
    "            print(f\"üßπ Cleaning artifact_ids...\")\n",
    "            original_count = len(df)\n",
    "            \n",
    "            # Convert to string and strip whitespace\n",
    "            df['artifact_id_original'] = df['artifact_id'].copy()\n",
    "            df['artifact_id'] = df['artifact_id'].astype(str).str.strip()\n",
    "            \n",
    "            # Remove any rows with null/empty artifact_ids\n",
    "            df = df[df['artifact_id'].notna() & (df['artifact_id'] != '') & (df['artifact_id'] != 'nan')]\n",
    "            \n",
    "            print(f\"   Cleaned {original_count} ‚Üí {len(df)} records\")\n",
    "            print(f\"   Sample artifact_ids: {df['artifact_id'].head().tolist()}\")\n",
    "            \n",
    "            # Show Flash classification distribution\n",
    "            flash_dist = df['correct_classification'].value_counts().to_dict()\n",
    "            print(f\"üìä Flash CSV classification distribution: {flash_dist}\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading CSV file: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_ground_truth_with_flexible_matching(self) -> pd.DataFrame:\n",
    "        \"\"\"Load ground truth with multiple ID matching strategies\"\"\"\n",
    "        print(\"üìä Loading Meta ground truth data with flexible matching...\")\n",
    "        \n",
    "        meta_gt_query = f\"\"\"\n",
    "        SELECT \n",
    "            artifact_id,\n",
    "            correct_classification as ground_truth,\n",
    "            correct_reasoning as ground_truth_reasoning,\n",
    "            source as gt_source\n",
    "        FROM `{PROJECT_ID}.{DATASET_ID}.{META_GROUND_TRUTH_TABLE}`\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            meta_gt_df = self.client.query(meta_gt_query).to_dataframe()\n",
    "            print(f\"   Found {len(meta_gt_df)} meta ground truth records\")\n",
    "            \n",
    "            if len(meta_gt_df) == 0:\n",
    "                print(\"‚ùå No ground truth data found!\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Clean ground truth artifact_ids\n",
    "            print(f\"üßπ Cleaning ground truth artifact_ids...\")\n",
    "            original_gt_count = len(meta_gt_df)\n",
    "            \n",
    "            # Store original for debugging\n",
    "            meta_gt_df['artifact_id_original'] = meta_gt_df['artifact_id'].copy()\n",
    "            \n",
    "            # Convert to string and strip\n",
    "            meta_gt_df['artifact_id'] = meta_gt_df['artifact_id'].astype(str).str.strip()\n",
    "            \n",
    "            # Remove null/empty IDs\n",
    "            meta_gt_df = meta_gt_df[meta_gt_df['artifact_id'].notna() & \n",
    "                                  (meta_gt_df['artifact_id'] != '') & \n",
    "                                  (meta_gt_df['artifact_id'] != 'nan')]\n",
    "            \n",
    "            print(f\"   Cleaned {original_gt_count} ‚Üí {len(meta_gt_df)} ground truth records\")\n",
    "            print(f\"   Sample GT artifact_ids: {meta_gt_df['artifact_id'].head().tolist()}\")\n",
    "            \n",
    "            # Check ground truth distribution\n",
    "            if len(meta_gt_df) > 0:\n",
    "                gt_dist = meta_gt_df['ground_truth'].value_counts().to_dict()\n",
    "                print(f\"   Meta ground truth distribution: {gt_dist}\")\n",
    "            \n",
    "            return meta_gt_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading ground truth data: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "    def load_results_with_ground_truth(self) -> pd.DataFrame:\n",
    "        \"\"\"Load Flash CSV results with ground truth using improved matching\"\"\"\n",
    "        print(\"üì• Loading Flash CSV results with Meta ground truth...\")\n",
    "        \n",
    "        # Load Flash results from CSV\n",
    "        flash_df = self.load_csv_file()\n",
    "        if flash_df.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Load ground truth\n",
    "        meta_gt_df = self.load_ground_truth_with_flexible_matching()\n",
    "        if meta_gt_df.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Debug ID comparison before merge\n",
    "        print(f\"\\nüîç Pre-merge ID analysis:\")\n",
    "        csv_ids = set(flash_df['artifact_id'])\n",
    "        gt_ids = set(meta_gt_df['artifact_id'])\n",
    "        \n",
    "        print(f\"   CSV unique IDs: {len(csv_ids)}\")\n",
    "        print(f\"   GT unique IDs: {len(gt_ids)}\")\n",
    "        \n",
    "        # Check for direct overlap\n",
    "        direct_overlap = csv_ids.intersection(gt_ids)\n",
    "        print(f\"   Direct overlap: {len(direct_overlap)} IDs\")\n",
    "        \n",
    "        if len(direct_overlap) == 0:\n",
    "            print(f\"   ‚ùå No direct overlap found!\")\n",
    "            print(f\"   CSV ID samples: {list(csv_ids)[:5]}\")\n",
    "            print(f\"   GT ID samples: {list(gt_ids)[:5]}\")\n",
    "            \n",
    "            # Try alternative matching strategies\n",
    "            print(f\"\\nüîÑ Trying alternative matching strategies...\")\n",
    "            \n",
    "            # Strategy 1: Try numeric conversion\n",
    "            try:\n",
    "                csv_numeric = set(pd.to_numeric(flash_df['artifact_id'], errors='coerce').dropna().astype(int).astype(str))\n",
    "                gt_numeric = set(pd.to_numeric(meta_gt_df['artifact_id'], errors='coerce').dropna().astype(int).astype(str))\n",
    "                numeric_overlap = csv_numeric.intersection(gt_numeric)\n",
    "                print(f\"   Strategy 1 (numeric): {len(numeric_overlap)} matches\")\n",
    "                \n",
    "                if len(numeric_overlap) > 0:\n",
    "                    # Convert both dataframes to use numeric string format\n",
    "                    flash_df['artifact_id'] = pd.to_numeric(flash_df['artifact_id'], errors='coerce').astype('Int64').astype(str)\n",
    "                    meta_gt_df['artifact_id'] = pd.to_numeric(meta_gt_df['artifact_id'], errors='coerce').astype('Int64').astype(str)\n",
    "                    print(f\"   ‚úÖ Using numeric conversion strategy\")\n",
    "            except Exception as e:\n",
    "                print(f\"   Strategy 1 failed: {e}\")\n",
    "            \n",
    "            # Strategy 2: Try removing common prefixes/suffixes\n",
    "            # (Add more strategies here if needed)\n",
    "        \n",
    "        # Perform the merge\n",
    "        print(f\"\\nüîó Performing merge...\")\n",
    "        df = flash_df.merge(meta_gt_df, on='artifact_id', how='inner')\n",
    "        print(f\"‚úÖ Merged dataset: {len(df)} records\")\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            print(\"‚ùå No matching records found after merge\")\n",
    "            print(f\"   Final CSV artifact_ids: {flash_df['artifact_id'].nunique()} unique\")\n",
    "            print(f\"   Final GT artifact_ids: {meta_gt_df['artifact_id'].nunique()} unique\")\n",
    "            \n",
    "            # Show detailed debugging\n",
    "            print(f\"\\nüîç DEBUGGING INFO:\")\n",
    "            print(f\"   CSV ID examples: {flash_df['artifact_id'].head().tolist()}\")\n",
    "            print(f\"   GT ID examples: {meta_gt_df['artifact_id'].head().tolist()}\")\n",
    "            \n",
    "            # Check if any partial matches exist\n",
    "            print(f\"\\n   Checking for partial matches...\")\n",
    "            csv_sample = flash_df['artifact_id'].head(10).tolist()\n",
    "            gt_sample = meta_gt_df['artifact_id'].head(10).tolist()\n",
    "            \n",
    "            for csv_id in csv_sample:\n",
    "                for gt_id in gt_sample:\n",
    "                    if str(csv_id) in str(gt_id) or str(gt_id) in str(csv_id):\n",
    "                        print(f\"   Partial match found: '{csv_id}' ‚Üî '{gt_id}'\")\n",
    "            \n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Rename columns for clarity\n",
    "        df['flash_classification'] = df['correct_classification']\n",
    "        df['flash_reasoning'] = df.get('correct_reasoning', '')\n",
    "        \n",
    "        # Check for data contamination\n",
    "        identical_before_conversion = (df['flash_classification'] == df['ground_truth']).sum()\n",
    "        print(f\"   Flash CSV vs Meta Ground Truth identical: {identical_before_conversion}/{len(df)} ({identical_before_conversion/len(df):.1%})\")\n",
    "        \n",
    "        if identical_before_conversion == len(df):\n",
    "            print(\"   üö® WARNING: Flash CSV results are identical to meta ground_truth!\")\n",
    "            print(\"   This suggests potential data contamination.\")\n",
    "        \n",
    "        # Convert to binary format\n",
    "        df['ground_truth_binary'] = (df['ground_truth'] == 100).astype(int)\n",
    "        df['flash_binary'] = (df['flash_classification'] == 100).astype(int)\n",
    "        \n",
    "        print(f\"üìä Final data distribution:\")\n",
    "        print(f\"   Meta ground truth: {df['ground_truth_binary'].value_counts().to_dict()}\")\n",
    "        print(f\"   Flash CSV predictions: {df['flash_binary'].value_counts().to_dict()}\")\n",
    "        \n",
    "        # Validate binary data\n",
    "        for col, name in [('ground_truth_binary', 'Meta ground truth'), \n",
    "                        ('flash_binary', 'Flash CSV')]:\n",
    "            if not all(df[col].isin([0, 1])):\n",
    "                print(f\"‚ö†Ô∏è Warning: {name} contains non-binary values\")\n",
    "                df = df[df[col].isin([0, 1])]\n",
    "        \n",
    "        print(f\"üìä Final clean dataset: {len(df)} records\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def calculate_classification_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate comprehensive classification metrics for Flash\"\"\"\n",
    "        print(\"üìä Calculating Flash classification metrics...\")\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            return {}\n",
    "        \n",
    "        y_true = df['ground_truth_binary'].values\n",
    "        flash_pred = df['flash_binary'].values\n",
    "        \n",
    "        # Flash metrics\n",
    "        flash_metrics = self._calculate_model_metrics(y_true, flash_pred, \"Flash\")\n",
    "        \n",
    "        # Agreement analysis\n",
    "        flash_vs_ground_truth_agreement = (flash_pred == y_true).mean()\n",
    "        \n",
    "        print(f\"‚úÖ Metrics calculated:\")\n",
    "        print(f\"   Flash F1: {flash_metrics['f1_score']:.3f}\")\n",
    "        print(f\"   Flash vs GT agreement: {flash_vs_ground_truth_agreement:.1%}\")\n",
    "        \n",
    "        return {\n",
    "            'dataset_info': {\n",
    "                'total_samples': len(df),\n",
    "                'positive_samples': int((y_true == 1).sum()),\n",
    "                'negative_samples': int((y_true == 0).sum()),\n",
    "                'class_balance': float((y_true == 1).mean())\n",
    "            },\n",
    "            'flash_csv_metrics': flash_metrics,\n",
    "            'agreement_analysis': {\n",
    "                'flash_vs_ground_truth': flash_vs_ground_truth_agreement\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _calculate_model_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, model_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate detailed metrics for a single model\"\"\"\n",
    "        \n",
    "        y_true = np.array(y_true).astype(int)\n",
    "        y_pred = np.array(y_pred).astype(int)\n",
    "        \n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        # Handle edge cases\n",
    "        unique_pred = np.unique(y_pred)\n",
    "        unique_true = np.unique(y_true)\n",
    "        \n",
    "        if len(unique_pred) == 1 or len(unique_true) == 1:\n",
    "            precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        else:\n",
    "            precision = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "        \n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        elif cm.shape == (1, 1):\n",
    "            if unique_true[0] == 0 and unique_pred[0] == 0:\n",
    "                tn, fp, fn, tp = cm[0,0], 0, 0, 0\n",
    "            elif unique_true[0] == 1 and unique_pred[0] == 1:\n",
    "                tn, fp, fn, tp = 0, 0, 0, cm[0,0]\n",
    "            else:\n",
    "                tn, fp, fn, tp = 0, cm[0,0], 0, 0\n",
    "        else:\n",
    "            tn = fp = fn = tp = 0\n",
    "        \n",
    "        # Additional metrics\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'specificity': specificity,\n",
    "            'sensitivity': sensitivity,\n",
    "            'false_positive_rate': fpr,\n",
    "            'false_negative_rate': fnr,\n",
    "            'confusion_matrix': {\n",
    "                'true_positives': int(tp),\n",
    "                'false_positives': int(fp), \n",
    "                'true_negatives': int(tn),\n",
    "                'false_negatives': int(fn)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def create_classification_dashboard(self, metrics: Dict[str, Any], df: pd.DataFrame) -> str:\n",
    "        \"\"\"Create comprehensive classification metrics dashboard\"\"\"\n",
    "        \n",
    "        if not metrics:\n",
    "            return \"<html><body><h1>No valid metrics to display</h1></body></html>\"\n",
    "        \n",
    "        flash_metrics = metrics['flash_csv_metrics']\n",
    "        dataset_info = metrics['dataset_info']\n",
    "        agreement = metrics['agreement_analysis']\n",
    "        \n",
    "        flash_cm = flash_metrics['confusion_matrix']\n",
    "        \n",
    "        # Performance assessment\n",
    "        if flash_metrics['f1_score'] >= 0.8:\n",
    "            flash_assessment = \"üü¢ EXCELLENT\"\n",
    "            flash_color = \"#10B981\"\n",
    "        elif flash_metrics['f1_score'] >= 0.7:\n",
    "            flash_assessment = \"üü° GOOD\" \n",
    "            flash_color = \"#F59E0B\"\n",
    "        elif flash_metrics['f1_score'] >= 0.6:\n",
    "            flash_assessment = \"üü† MODERATE\"\n",
    "            flash_color = \"#FF6B35\"\n",
    "        else:\n",
    "            flash_assessment = \"üî¥ POOR\"\n",
    "            flash_color = \"#EF4444\"\n",
    "        \n",
    "        html_content = f\"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Flash Meta Classification Metrics Dashboard</title>\n",
    "    <style>\n",
    "        * {{ margin: 0; padding: 0; box-sizing: border-box; }}\n",
    "        body {{ font-family: 'Inter', sans-serif; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%); color: #333; line-height: 1.6; min-height: 100vh; }}\n",
    "        .dashboard {{ max-width: 1800px; margin: 0 auto; padding: 20px; }}\n",
    "        .header {{ background: rgba(255,255,255,0.95); padding: 40px; border-radius: 25px; text-align: center; margin-bottom: 30px; backdrop-filter: blur(15px); box-shadow: 0 20px 60px rgba(0,0,0,0.1); }}\n",
    "        .header h1 {{ font-size: 3rem; margin-bottom: 15px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; }}\n",
    "        .header h2 {{ font-size: 1.5rem; color: #666; margin-bottom: 20px; }}\n",
    "        .model-badge {{ display: inline-block; padding: 10px 25px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%); color: white; border-radius: 30px; font-weight: bold; font-size: 1.1rem; margin: 5px; }}\n",
    "        .csv-info {{ background: rgba(255,255,255,0.8); padding: 15px; border-radius: 10px; margin-top: 15px; font-size: 0.9rem; color: #555; }}\n",
    "        .performance-summary {{ background: linear-gradient(135deg, #ffe6e6, #ffcccc); padding: 30px; border-radius: 20px; margin: 30px 0; text-align: center; }}\n",
    "        .performance-score {{ font-size: 3.5rem; font-weight: bold; color: {flash_color}; margin-bottom: 10px; }}\n",
    "        .performance-label {{ font-size: 1.3rem; color: #37474f; margin-bottom: 8px; }}\n",
    "        .performance-description {{ font-size: 1rem; color: #546e7a; }}\n",
    "        .metrics-overview {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(180px, 1fr)); gap: 20px; margin-bottom: 40px; }}\n",
    "        .metric-card {{ background: rgba(255,255,255,0.95); padding: 25px; border-radius: 20px; text-align: center; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); transition: transform 0.3s ease; }}\n",
    "        .metric-card:hover {{ transform: translateY(-5px); }}\n",
    "        .metric-value {{ font-size: 2.5rem; font-weight: bold; margin-bottom: 10px; }}\n",
    "        .metric-label {{ font-size: 1rem; color: #666; font-weight: 600; margin-bottom: 5px; }}\n",
    "        .metric-description {{ font-size: 0.85rem; color: #888; }}\n",
    "        .excellent {{ color: #10B981; }}\n",
    "        .good {{ color: #F59E0B; }}\n",
    "        .moderate {{ color: #FF6B35; }}\n",
    "        .poor {{ color: #EF4444; }}\n",
    "        .dashboard-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(450px, 1fr)); gap: 25px; margin-bottom: 40px; }}\n",
    "        .chart-section {{ background: rgba(255,255,255,0.95); padding: 30px; border-radius: 20px; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); }}\n",
    "        .chart-title {{ font-size: 1.3rem; font-weight: bold; margin-bottom: 20px; color: #333; text-align: center; }}\n",
    "        .confusion-matrix {{ display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px; margin: 20px 0; text-align: center; font-size: 0.9rem; }}\n",
    "        .cm-cell {{ padding: 15px; border-radius: 12px; font-weight: bold; display: flex; flex-direction: column; justify-content: center; align-items: center; }}\n",
    "        .cm-header {{ background: linear-gradient(135deg, #f8f9fa, #e9ecef); color: #495057; }}\n",
    "        .cm-tp {{ background: linear-gradient(135deg, #d4edda, #c3e6cb); color: #155724; font-size: 1.8rem; }}\n",
    "        .cm-fp {{ background: linear-gradient(135deg, #f8d7da, #f1b0b7); color: #721c24; font-size: 1.8rem; }}\n",
    "        .cm-fn {{ background: linear-gradient(135deg, #fff3cd, #fce4a6); color: #856404; font-size: 1.8rem; }}\n",
    "        .cm-tn {{ background: linear-gradient(135deg, #d1ecf1, #bee5eb); color: #0c5460; font-size: 1.8rem; }}\n",
    "        .cm-label {{ font-size: 0.75rem; margin-top: 5px; opacity: 0.8; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"dashboard\">\n",
    "        <div class=\"header\">\n",
    "            <h1>‚ö° Flash CSV Meta Classification Metrics</h1>\n",
    "            <h2>texts_images_classification_results_web_multimodal_dataset_300.csv vs Meta Ground Truth</h2>\n",
    "            <div>\n",
    "                <span class=\"model-badge\">Flash CSV Model</span>\n",
    "            </div>\n",
    "            <div class=\"csv-info\">\n",
    "                üìÅ Source CSV: texts_images_classification_results_web_multimodal_dataset_300.csv<br>\n",
    "                üéØ Successfully matched {dataset_info['total_samples']} artifact IDs with ground truth\n",
    "            </div>\n",
    "            <p style=\"margin-top: 20px; color: #666;\">\n",
    "                Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | \n",
    "                Samples: {dataset_info['total_samples']:,} | \n",
    "                Positive: {dataset_info['positive_samples']} | Negative: {dataset_info['negative_samples']}\n",
    "            </p>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"performance-summary\">\n",
    "            <div class=\"performance-score\">{flash_metrics['f1_score']:.3f}</div>\n",
    "            <div class=\"performance-label\">{flash_assessment} F1-Score (Flash CSV Model)</div>\n",
    "            <div class=\"performance-description\">\n",
    "                Flash CSV achieves {flash_metrics['accuracy']:.1%} accuracy with {flash_metrics['precision']:.1%} precision\n",
    "                <br>Agreement with Meta Ground Truth: {agreement['flash_vs_ground_truth']:.1%}\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"metrics-overview\">\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if flash_metrics['f1_score'] >= 0.8 else 'good' if flash_metrics['f1_score'] >= 0.7 else 'moderate' if flash_metrics['f1_score'] >= 0.6 else 'poor'}\">{flash_metrics['f1_score']:.3f}</div>\n",
    "                <div class=\"metric-label\">Flash CSV F1-Score</div>\n",
    "                <div class=\"metric-description\">Harmonic Mean P&R</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if flash_metrics['recall'] >= 0.8 else 'good' if flash_metrics['recall'] >= 0.7 else 'moderate' if flash_metrics['recall'] >= 0.6 else 'poor'}\">{flash_metrics['recall']:.3f}</div>\n",
    "                <div class=\"metric-label\">Flash CSV TPR</div>\n",
    "                <div class=\"metric-description\">True Positive Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if flash_metrics['specificity'] >= 0.8 else 'good' if flash_metrics['specificity'] >= 0.7 else 'moderate' if flash_metrics['specificity'] >= 0.6 else 'poor'}\">{flash_metrics['specificity']:.3f}</div>\n",
    "                <div class=\"metric-label\">Flash CSV TNR</div>\n",
    "                <div class=\"metric-description\">True Negative Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if flash_metrics['accuracy'] >= 0.9 else 'good' if flash_metrics['accuracy'] >= 0.8 else 'moderate' if flash_metrics['accuracy'] >= 0.7 else 'poor'}\">{flash_metrics['accuracy']:.3f}</div>\n",
    "                <div class=\"metric-label\">Flash CSV Accuracy</div>\n",
    "                <div class=\"metric-description\">Overall Correctness</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if agreement['flash_vs_ground_truth'] >= 0.8 else 'good' if agreement['flash_vs_ground_truth'] >= 0.7 else 'moderate' if agreement['flash_vs_ground_truth'] >= 0.6 else 'poor'}\">{agreement['flash_vs_ground_truth']:.1%}</div>\n",
    "                <div class=\"metric-label\">Ground Truth Agreement</div>\n",
    "                <div class=\"metric-description\">Flash CSV matches human labels</div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"dashboard-grid\">\n",
    "            <div class=\"chart-section\">\n",
    "                <div class=\"chart-title\">‚ö° Flash CSV Confusion Matrix</div>\n",
    "                <div class=\"confusion-matrix\">\n",
    "                    <div class=\"cm-cell cm-header\"></div>\n",
    "                    <div class=\"cm-cell cm-header\">Meta GT: 0</div>\n",
    "                    <div class=\"cm-cell cm-header\">Meta GT: 1</div>\n",
    "                    \n",
    "                    <div class=\"cm-cell cm-header\">Flash CSV: 0</div>\n",
    "                    <div class=\"cm-cell cm-tn\">\n",
    "                        {flash_cm['true_negatives']}\n",
    "                        <div class=\"cm-label\">True Negatives</div>\n",
    "                    </div>\n",
    "                    <div class=\"cm-cell cm-fn\">\n",
    "                        {flash_cm['false_negatives']}\n",
    "                        <div class=\"cm-label\">False Negatives</div>\n",
    "                    </div>\n",
    "                    \n",
    "                    <div class=\"cm-cell cm-header\">Flash CSV: 1</div>\n",
    "                    <div class=\"cm-cell cm-fp\">\n",
    "                        {flash_cm['false_positives']}\n",
    "                        <div class=\"cm-label\">False Positives</div>\n",
    "                    </div>\n",
    "                    <div class=\"cm-cell cm-tp\">\n",
    "                        {flash_cm['true_positives']}\n",
    "                        <div class=\"cm-label\">True Positives</div>\n",
    "                    </div>\n",
    "                </div>\n",
    "                <div style=\"text-align: center; margin-top: 15px;\">\n",
    "                    <p><strong>TP:</strong> {flash_cm['true_positives']} | <strong>FP:</strong> {flash_cm['false_positives']} | <strong>TN:</strong> {flash_cm['true_negatives']} | <strong>FN:</strong> {flash_cm['false_negatives']}</p>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "        \n",
    "        return html_content\n",
    "    \n",
    "    def generate_classification_dashboard(self):\n",
    "        \"\"\"Main function to generate the classification dashboard\"\"\"\n",
    "        print(\"üöÄ STARTING FLASH CSV META CLASSIFICATION DASHBOARD GENERATION (FIXED)\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"üìÅ Using CSV file: {self.csv_file_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Load results with ground truth\n",
    "            df = self.load_results_with_ground_truth()\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"‚ùå No data available for dashboard\")\n",
    "                return None\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = self.calculate_classification_metrics(df)\n",
    "            \n",
    "            if not metrics:\n",
    "                print(\"‚ùå Could not calculate metrics\")\n",
    "                return None\n",
    "            \n",
    "            # Create dashboard\n",
    "            print(\"\\nüìä Creating Flash CSV Meta Classification Dashboard...\")\n",
    "            dashboard_html = self.create_classification_dashboard(metrics, df)\n",
    "            \n",
    "            # Save files\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            dashboard_filename = f\"flash_csv_meta_classification_dashboard_{timestamp}.html\"\n",
    "            metrics_filename = f\"flash_csv_meta_classification_metrics_{timestamp}.json\"\n",
    "            \n",
    "            with open(dashboard_filename, \"w\", encoding='utf-8') as f:\n",
    "                f.write(dashboard_html)\n",
    "            \n",
    "            with open(metrics_filename, \"w\") as f:\n",
    "                json.dump(metrics, f, indent=2, default=str)\n",
    "            \n",
    "            print(f\"‚úÖ Dashboard saved: {dashboard_filename}\")\n",
    "            print(f\"‚úÖ Metrics saved: {metrics_filename}\")\n",
    "            \n",
    "            # Summary\n",
    "            flash_metrics = metrics['flash_csv_metrics']\n",
    "            agreement = metrics['agreement_analysis']\n",
    "            \n",
    "            print(f\"\\nüìä FLASH CSV META CLASSIFICATION METRICS SUMMARY:\")\n",
    "            print(f\"   ‚ö° Flash CSV Model:\")\n",
    "            print(f\"      F1-Score: {flash_metrics['f1_score']:.3f}\")\n",
    "            print(f\"      Precision: {flash_metrics['precision']:.3f}\")\n",
    "            print(f\"      Recall: {flash_metrics['recall']:.3f}\") \n",
    "            print(f\"      Accuracy: {flash_metrics['accuracy']:.3f}\")\n",
    "            print(f\"      TP: {flash_metrics['confusion_matrix']['true_positives']}, FP: {flash_metrics['confusion_matrix']['false_positives']}\")\n",
    "            print(f\"      TN: {flash_metrics['confusion_matrix']['true_negatives']}, FN: {flash_metrics['confusion_matrix']['false_negatives']}\")\n",
    "            \n",
    "            print(f\"\\n   ‚öñÔ∏è Agreement Rates:\")\n",
    "            print(f\"      Flash CSV vs Meta Ground Truth: {agreement['flash_vs_ground_truth']:.1%}\")\n",
    "            \n",
    "            return dashboard_filename, metrics_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Dashboard generation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the Flash CSV Meta classification dashboard generation\"\"\"\n",
    "    # Option 1: Use a different CSV file with Meta IDs\n",
    "    # dashboard_generator = FlashMetaCSVClassificationDashboard(csv_file_path=\"your_meta_ids_file.csv\")\n",
    "    \n",
    "    # Option 2: Use a different column from the current file\n",
    "    # dashboard_generator = FlashMetaCSVClassificationDashboard(id_column_name='source')\n",
    "    \n",
    "    # Option 3: Default behavior\n",
    "    dashboard_generator = FlashMetaCSVClassificationDashboard()\n",
    "    result = dashboard_generator.generate_classification_dashboard()\n",
    "    \n",
    "    if result:\n",
    "        dashboard_file, metrics_file = result\n",
    "        print(f\"\\nüéâ SUCCESS!\")\n",
    "        print(f\"üìÅ Files generated:\")\n",
    "        print(f\"   1. {dashboard_file}\")\n",
    "        print(f\"   2. {metrics_file}\")\n",
    "        print(f\"\\nüí° Open {dashboard_file} in your browser to view the dashboard!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Dashboard generation failed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ac8cfa13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING FLASH CSV WEB CLASSIFICATION DASHBOARD GENERATION (FIXED)\n",
      "======================================================================\n",
      "üìÅ Using CSV file: texts_images_classification_results_web_multimodal_dataset_300.csv\n",
      "üìä Using BigQuery table: scope3-dev.research_bs_monitoring.BA_Web_Ground_Truth\n",
      "üì• Loading Flash CSV results with Web ground truth...\n",
      "üì• Loading Flash results from CSV: texts_images_classification_results_web_multimodal_dataset_300.csv\n",
      "‚úÖ Loaded 300 Flash results from CSV\n",
      "üìä CSV columns: ['source', 'artifact_id', 'model_id', 'model_prompt', 'correct_classification', 'correct_reasoning', 'artifact_json_gcs_url', 'asset_filepaths', 'classified_at', 'detected_language', 'topics']\n",
      "üéØ Auto-detected ID column: 'artifact_id'\n",
      "üßπ Cleaning artifact_ids...\n",
      "   Cleaned 300 ‚Üí 300 records\n",
      "   Sample artifact_ids: ['engadget.com/2020-02-15-ring-footage-might-not-help-catch-criminals.html', 'ebay.com/itm/187348622731', 'indiatimes.com/trending/gears-of-war-reloaded-how-to-access-the-beta-price-and-more-661063.html', 'menshealth.com/fitness/a28435487/tom-ellis-lucifer-workout', 'zone-turf.fr/cheval/zoom-chop-1669380']\n",
      "üìä Flash CSV classification distribution: {100: 229, 0: 71}\n",
      "üìä Loading Web ground truth data with flexible matching...\n",
      "   Found 294 meta ground truth records\n",
      "üßπ Cleaning ground truth artifact_ids...\n",
      "   Cleaned 294 ‚Üí 294 ground truth records\n",
      "   Sample GT artifact_ids: ['news.de/gesundheit/855915949/corona-zahlen-kreisfreie-stadt-solingen-heute-aktuell-27-06-2025-coronavirus-news-zu-rki-fallzahlen-tote-in-nordrhein-westfalen-intensivbetten-auslastung-und-neue-covid-19-variante-nimbus/1', 'bizjournals.com/sanfrancisco/news/2025/06/27/genentech-ulcerative-colitis-vixarelimab-ibd-osmr.html', 'scmp.com/lifestyle/food-drink/article/3315878/health-benefits-superfood-prickly-pear-cactus-spiking-popularity-japan', 'plejada.pl/newsy/bruce-willis-zniknal-z-powodu-choroby-poczatkowo-zostal-zle-zdiagnozowany/2cdfxfw', 'askamanager.org/2018/09/my-coworker-keeps-making-me-do-my-old-job-manager-doesnt-believe-im-sick-and-more.html']\n",
      "   Web ground truth distribution: {100: 252, 0: 42}\n",
      "\n",
      "üîç Pre-merge ID analysis:\n",
      "   CSV unique IDs: 300\n",
      "   GT unique IDs: 294\n",
      "   Direct overlap: 294 IDs\n",
      "\n",
      "üîó Performing merge...\n",
      "‚úÖ Merged dataset: 294 records\n",
      "   Flash CSV vs Web Ground Truth identical: 260/294 (88.4%)\n",
      "üìä Final data distribution:\n",
      "   Web ground truth: {1: 252, 0: 42}\n",
      "   Flash CSV predictions: {1: 226, 0: 68}\n",
      "üìä Final clean dataset: 294 records\n",
      "üìä Calculating Flash classification metrics...\n",
      "‚úÖ Metrics calculated:\n",
      "   Flash F1: 0.929\n",
      "   Flash vs GT agreement: 88.4%\n",
      "\n",
      "üìä Creating Flash CSV Meta Classification Dashboard...\n",
      "‚úÖ Dashboard saved: flash_csv_web_classification_dashboard_20250725_135816.html\n",
      "‚úÖ Metrics saved: flash_csv_web_classification_metrics_20250725_135816.json\n",
      "\n",
      "üìä FLASH CSV WEB CLASSIFICATION METRICS SUMMARY:\n",
      "   ‚ö° Flash CSV Model:\n",
      "      F1-Score: 0.929\n",
      "      Precision: 0.982\n",
      "      Recall: 0.881\n",
      "      Accuracy: 0.884\n",
      "      TP: 222, FP: 4\n",
      "      TN: 38, FN: 30\n",
      "\n",
      "   ‚öñÔ∏è Agreement Rates:\n",
      "      Flash CSV vs Web Ground Truth: 88.4%\n",
      "\n",
      "üéâ SUCCESS!\n",
      "üìÅ Files generated:\n",
      "   1. flash_csv_web_classification_dashboard_20250725_135816.html\n",
      "   2. flash_csv_web_classification_metrics_20250725_135816.json\n",
      "\n",
      "üí° Open flash_csv_web_classification_dashboard_20250725_135816.html in your browser to view the dashboard!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Flash Meta Classification Metrics Dashboard - Fixed ID Matching\n",
    "Enhanced version with better artifact_id matching and debugging\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "PROJECT_ID = \"scope3-dev\"\n",
    "DATASET_ID = \"research_bs_monitoring\"\n",
    "META_GROUND_TRUTH_TABLE = \"BA_Web_Ground_Truth\"  # Changed to Web Ground Truth\n",
    "\n",
    "class FlashMetaCSVClassificationDashboard:\n",
    "    def __init__(self, id_column_name=None, csv_file_path=None):\n",
    "        self.client = bigquery.Client(project=PROJECT_ID)\n",
    "        self.csv_file_path = csv_file_path or \"texts_images_classification_results_web_multimodal_dataset_300.csv\"  # Back to web CSV\n",
    "        self.id_column_name = id_column_name  # Allow manual specification\n",
    "        \n",
    "    def load_csv_file(self) -> pd.DataFrame:\n",
    "        \"\"\"Load Flash results from the specific CSV file with improved ID handling\"\"\"\n",
    "        print(f\"üì• Loading Flash results from CSV: {self.csv_file_path}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(self.csv_file_path)\n",
    "            \n",
    "            print(f\"‚úÖ Loaded {len(df)} Flash results from CSV\")\n",
    "            print(f\"üìä CSV columns: {df.columns.tolist()}\")\n",
    "            \n",
    "            # Auto-detect ID column - try different possible names\n",
    "            if self.id_column_name:\n",
    "                # Use manually specified column name\n",
    "                if self.id_column_name in df.columns:\n",
    "                    id_column = self.id_column_name\n",
    "                    print(f\"üéØ Using manually specified ID column: '{id_column}'\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Specified ID column '{self.id_column_name}' not found in CSV\")\n",
    "            else:\n",
    "                # Auto-detect from common names\n",
    "                id_column_candidates = ['artifact_id', 'artifact', 'id', 'sample_id', 'record_id']\n",
    "                id_column = None\n",
    "                \n",
    "                for candidate in id_column_candidates:\n",
    "                    if candidate in df.columns:\n",
    "                        id_column = candidate\n",
    "                        print(f\"üéØ Auto-detected ID column: '{id_column}'\")\n",
    "                        break\n",
    "            \n",
    "            if id_column is None:\n",
    "                print(f\"‚ùå No ID column found. Available columns: {df.columns.tolist()}\")\n",
    "                print(f\"   Looking for any of: {id_column_candidates}\")\n",
    "                raise ValueError(f\"No ID column found in CSV\")\n",
    "            \n",
    "            # Validate required columns\n",
    "            required_columns = [id_column, 'correct_classification']\n",
    "            missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "            if missing_columns:\n",
    "                raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "            \n",
    "            # Standardize ID column name to 'artifact_id'\n",
    "            if id_column != 'artifact_id':\n",
    "                df['artifact_id'] = df[id_column]\n",
    "                print(f\"üîÑ Standardized '{id_column}' ‚Üí 'artifact_id'\")\n",
    "            \n",
    "            # Clean and standardize artifact_ids\n",
    "            print(f\"üßπ Cleaning artifact_ids...\")\n",
    "            original_count = len(df)\n",
    "            \n",
    "            # Convert to string and strip whitespace\n",
    "            df['artifact_id_original'] = df['artifact_id'].copy()\n",
    "            df['artifact_id'] = df['artifact_id'].astype(str).str.strip()\n",
    "            \n",
    "            # Remove any rows with null/empty artifact_ids\n",
    "            df = df[df['artifact_id'].notna() & (df['artifact_id'] != '') & (df['artifact_id'] != 'nan')]\n",
    "            \n",
    "            print(f\"   Cleaned {original_count} ‚Üí {len(df)} records\")\n",
    "            print(f\"   Sample artifact_ids: {df['artifact_id'].head().tolist()}\")\n",
    "            \n",
    "            # Show Flash classification distribution\n",
    "            flash_dist = df['correct_classification'].value_counts().to_dict()\n",
    "            print(f\"üìä Flash CSV classification distribution: {flash_dist}\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading CSV file: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_ground_truth_with_flexible_matching(self) -> pd.DataFrame:\n",
    "        \"\"\"Load ground truth with multiple ID matching strategies\"\"\"\n",
    "        print(\"üìä Loading Web ground truth data with flexible matching...\")\n",
    "        \n",
    "        meta_gt_query = f\"\"\"\n",
    "        SELECT \n",
    "            artifact_id,\n",
    "            correct_classification as ground_truth,\n",
    "            correct_reasoning as ground_truth_reasoning,\n",
    "            source as gt_source\n",
    "        FROM `{PROJECT_ID}.{DATASET_ID}.{META_GROUND_TRUTH_TABLE}`\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            meta_gt_df = self.client.query(meta_gt_query).to_dataframe()\n",
    "            print(f\"   Found {len(meta_gt_df)} meta ground truth records\")\n",
    "            \n",
    "            if len(meta_gt_df) == 0:\n",
    "                print(\"‚ùå No ground truth data found!\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Clean ground truth artifact_ids\n",
    "            print(f\"üßπ Cleaning ground truth artifact_ids...\")\n",
    "            original_gt_count = len(meta_gt_df)\n",
    "            \n",
    "            # Store original for debugging\n",
    "            meta_gt_df['artifact_id_original'] = meta_gt_df['artifact_id'].copy()\n",
    "            \n",
    "            # Convert to string and strip\n",
    "            meta_gt_df['artifact_id'] = meta_gt_df['artifact_id'].astype(str).str.strip()\n",
    "            \n",
    "            # Remove null/empty IDs\n",
    "            meta_gt_df = meta_gt_df[meta_gt_df['artifact_id'].notna() & \n",
    "                                  (meta_gt_df['artifact_id'] != '') & \n",
    "                                  (meta_gt_df['artifact_id'] != 'nan')]\n",
    "            \n",
    "            print(f\"   Cleaned {original_gt_count} ‚Üí {len(meta_gt_df)} ground truth records\")\n",
    "            print(f\"   Sample GT artifact_ids: {meta_gt_df['artifact_id'].head().tolist()}\")\n",
    "            \n",
    "            # Check ground truth distribution\n",
    "            if len(meta_gt_df) > 0:\n",
    "                gt_dist = meta_gt_df['ground_truth'].value_counts().to_dict()\n",
    "                print(f\"   Web ground truth distribution: {gt_dist}\")\n",
    "            \n",
    "            return meta_gt_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading ground truth data: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "    def load_results_with_ground_truth(self) -> pd.DataFrame:\n",
    "        \"\"\"Load Flash CSV results with ground truth using improved matching\"\"\"\n",
    "        print(\"üì• Loading Flash CSV results with Web ground truth...\")\n",
    "        \n",
    "        # Load Flash results from CSV\n",
    "        flash_df = self.load_csv_file()\n",
    "        if flash_df.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Load ground truth\n",
    "        meta_gt_df = self.load_ground_truth_with_flexible_matching()\n",
    "        if meta_gt_df.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Debug ID comparison before merge\n",
    "        print(f\"\\nüîç Pre-merge ID analysis:\")\n",
    "        csv_ids = set(flash_df['artifact_id'])\n",
    "        gt_ids = set(meta_gt_df['artifact_id'])\n",
    "        \n",
    "        print(f\"   CSV unique IDs: {len(csv_ids)}\")\n",
    "        print(f\"   GT unique IDs: {len(gt_ids)}\")\n",
    "        \n",
    "        # Check for direct overlap\n",
    "        direct_overlap = csv_ids.intersection(gt_ids)\n",
    "        print(f\"   Direct overlap: {len(direct_overlap)} IDs\")\n",
    "        \n",
    "        if len(direct_overlap) == 0:\n",
    "            print(f\"   ‚ùå No direct overlap found!\")\n",
    "            print(f\"   CSV ID samples: {list(csv_ids)[:5]}\")\n",
    "            print(f\"   GT ID samples: {list(gt_ids)[:5]}\")\n",
    "            \n",
    "            # Try alternative matching strategies\n",
    "            print(f\"\\nüîÑ Trying alternative matching strategies...\")\n",
    "            \n",
    "            # Strategy 1: Try numeric conversion\n",
    "            try:\n",
    "                csv_numeric = set(pd.to_numeric(flash_df['artifact_id'], errors='coerce').dropna().astype(int).astype(str))\n",
    "                gt_numeric = set(pd.to_numeric(meta_gt_df['artifact_id'], errors='coerce').dropna().astype(int).astype(str))\n",
    "                numeric_overlap = csv_numeric.intersection(gt_numeric)\n",
    "                print(f\"   Strategy 1 (numeric): {len(numeric_overlap)} matches\")\n",
    "                \n",
    "                if len(numeric_overlap) > 0:\n",
    "                    # Convert both dataframes to use numeric string format\n",
    "                    flash_df['artifact_id'] = pd.to_numeric(flash_df['artifact_id'], errors='coerce').astype('Int64').astype(str)\n",
    "                    meta_gt_df['artifact_id'] = pd.to_numeric(meta_gt_df['artifact_id'], errors='coerce').astype('Int64').astype(str)\n",
    "                    print(f\"   ‚úÖ Using numeric conversion strategy\")\n",
    "            except Exception as e:\n",
    "                print(f\"   Strategy 1 failed: {e}\")\n",
    "            \n",
    "            # Strategy 2: Try removing common prefixes/suffixes\n",
    "            # (Add more strategies here if needed)\n",
    "        \n",
    "        # Perform the merge\n",
    "        print(f\"\\nüîó Performing merge...\")\n",
    "        df = flash_df.merge(meta_gt_df, on='artifact_id', how='inner')\n",
    "        print(f\"‚úÖ Merged dataset: {len(df)} records\")\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            print(\"‚ùå No matching records found after merge\")\n",
    "            print(f\"   Final CSV artifact_ids: {flash_df['artifact_id'].nunique()} unique\")\n",
    "            print(f\"   Final GT artifact_ids: {meta_gt_df['artifact_id'].nunique()} unique\")\n",
    "            \n",
    "            # Show detailed debugging\n",
    "            print(f\"\\nüîç DEBUGGING INFO:\")\n",
    "            print(f\"   CSV ID examples: {flash_df['artifact_id'].head().tolist()}\")\n",
    "            print(f\"   GT ID examples: {meta_gt_df['artifact_id'].head().tolist()}\")\n",
    "            \n",
    "            # Check if any partial matches exist\n",
    "            print(f\"\\n   Checking for partial matches...\")\n",
    "            csv_sample = flash_df['artifact_id'].head(10).tolist()\n",
    "            gt_sample = meta_gt_df['artifact_id'].head(10).tolist()\n",
    "            \n",
    "            for csv_id in csv_sample:\n",
    "                for gt_id in gt_sample:\n",
    "                    if str(csv_id) in str(gt_id) or str(gt_id) in str(csv_id):\n",
    "                        print(f\"   Partial match found: '{csv_id}' ‚Üî '{gt_id}'\")\n",
    "            \n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Rename columns for clarity\n",
    "        df['flash_classification'] = df['correct_classification']\n",
    "        df['flash_reasoning'] = df.get('correct_reasoning', '')\n",
    "        \n",
    "        # Check for data contamination\n",
    "        identical_before_conversion = (df['flash_classification'] == df['ground_truth']).sum()\n",
    "        print(f\"   Flash CSV vs Web Ground Truth identical: {identical_before_conversion}/{len(df)} ({identical_before_conversion/len(df):.1%})\")\n",
    "        \n",
    "        if identical_before_conversion == len(df):\n",
    "            print(\"   üö® WARNING: Flash CSV results are identical to web ground_truth!\")\n",
    "            print(\"   This suggests potential data contamination.\")\n",
    "        \n",
    "        # Convert to binary format\n",
    "        df['ground_truth_binary'] = (df['ground_truth'] == 100).astype(int)\n",
    "        df['flash_binary'] = (df['flash_classification'] == 100).astype(int)\n",
    "        \n",
    "        print(f\"üìä Final data distribution:\")\n",
    "        print(f\"   Web ground truth: {df['ground_truth_binary'].value_counts().to_dict()}\")\n",
    "        print(f\"   Flash CSV predictions: {df['flash_binary'].value_counts().to_dict()}\")\n",
    "        \n",
    "        # Validate binary data\n",
    "        for col, name in [('ground_truth_binary', 'Web ground truth'), \n",
    "                        ('flash_binary', 'Flash CSV')]:\n",
    "            if not all(df[col].isin([0, 1])):\n",
    "                print(f\"‚ö†Ô∏è Warning: {name} contains non-binary values\")\n",
    "                df = df[df[col].isin([0, 1])]\n",
    "        \n",
    "        print(f\"üìä Final clean dataset: {len(df)} records\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def calculate_classification_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate comprehensive classification metrics for Flash\"\"\"\n",
    "        print(\"üìä Calculating Flash classification metrics...\")\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            return {}\n",
    "        \n",
    "        y_true = df['ground_truth_binary'].values\n",
    "        flash_pred = df['flash_binary'].values\n",
    "        \n",
    "        # Flash metrics\n",
    "        flash_metrics = self._calculate_model_metrics(y_true, flash_pred, \"Flash\")\n",
    "        \n",
    "        # Agreement analysis\n",
    "        flash_vs_ground_truth_agreement = (flash_pred == y_true).mean()\n",
    "        \n",
    "        print(f\"‚úÖ Metrics calculated:\")\n",
    "        print(f\"   Flash F1: {flash_metrics['f1_score']:.3f}\")\n",
    "        print(f\"   Flash vs GT agreement: {flash_vs_ground_truth_agreement:.1%}\")\n",
    "        \n",
    "        return {\n",
    "            'dataset_info': {\n",
    "                'total_samples': len(df),\n",
    "                'positive_samples': int((y_true == 1).sum()),\n",
    "                'negative_samples': int((y_true == 0).sum()),\n",
    "                'class_balance': float((y_true == 1).mean())\n",
    "            },\n",
    "            'flash_csv_metrics': flash_metrics,\n",
    "            'agreement_analysis': {\n",
    "                'flash_vs_ground_truth': flash_vs_ground_truth_agreement\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _calculate_model_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, model_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate detailed metrics for a single model\"\"\"\n",
    "        \n",
    "        y_true = np.array(y_true).astype(int)\n",
    "        y_pred = np.array(y_pred).astype(int)\n",
    "        \n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        # Handle edge cases\n",
    "        unique_pred = np.unique(y_pred)\n",
    "        unique_true = np.unique(y_true)\n",
    "        \n",
    "        if len(unique_pred) == 1 or len(unique_true) == 1:\n",
    "            precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        else:\n",
    "            precision = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "        \n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        elif cm.shape == (1, 1):\n",
    "            if unique_true[0] == 0 and unique_pred[0] == 0:\n",
    "                tn, fp, fn, tp = cm[0,0], 0, 0, 0\n",
    "            elif unique_true[0] == 1 and unique_pred[0] == 1:\n",
    "                tn, fp, fn, tp = 0, 0, 0, cm[0,0]\n",
    "            else:\n",
    "                tn, fp, fn, tp = 0, cm[0,0], 0, 0\n",
    "        else:\n",
    "            tn = fp = fn = tp = 0\n",
    "        \n",
    "        # Additional metrics\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'specificity': specificity,\n",
    "            'sensitivity': sensitivity,\n",
    "            'false_positive_rate': fpr,\n",
    "            'false_negative_rate': fnr,\n",
    "            'confusion_matrix': {\n",
    "                'true_positives': int(tp),\n",
    "                'false_positives': int(fp), \n",
    "                'true_negatives': int(tn),\n",
    "                'false_negatives': int(fn)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def create_classification_dashboard(self, metrics: Dict[str, Any], df: pd.DataFrame) -> str:\n",
    "        \"\"\"Create comprehensive classification metrics dashboard\"\"\"\n",
    "        \n",
    "        if not metrics:\n",
    "            return \"<html><body><h1>No valid metrics to display</h1></body></html>\"\n",
    "        \n",
    "        flash_metrics = metrics['flash_csv_metrics']\n",
    "        dataset_info = metrics['dataset_info']\n",
    "        agreement = metrics['agreement_analysis']\n",
    "        \n",
    "        flash_cm = flash_metrics['confusion_matrix']\n",
    "        \n",
    "        # Performance assessment\n",
    "        if flash_metrics['f1_score'] >= 0.8:\n",
    "            flash_assessment = \"üü¢ EXCELLENT\"\n",
    "            flash_color = \"#10B981\"\n",
    "        elif flash_metrics['f1_score'] >= 0.7:\n",
    "            flash_assessment = \"üü° GOOD\" \n",
    "            flash_color = \"#F59E0B\"\n",
    "        elif flash_metrics['f1_score'] >= 0.6:\n",
    "            flash_assessment = \"üü† MODERATE\"\n",
    "            flash_color = \"#FF6B35\"\n",
    "        else:\n",
    "            flash_assessment = \"üî¥ POOR\"\n",
    "            flash_color = \"#EF4444\"\n",
    "        \n",
    "        html_content = f\"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Flash Meta Classification Metrics Dashboard</title>\n",
    "    <style>\n",
    "        * {{ margin: 0; padding: 0; box-sizing: border-box; }}\n",
    "        body {{ font-family: 'Inter', sans-serif; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%); color: #333; line-height: 1.6; min-height: 100vh; }}\n",
    "        .dashboard {{ max-width: 1800px; margin: 0 auto; padding: 20px; }}\n",
    "        .header {{ background: rgba(255,255,255,0.95); padding: 40px; border-radius: 25px; text-align: center; margin-bottom: 30px; backdrop-filter: blur(15px); box-shadow: 0 20px 60px rgba(0,0,0,0.1); }}\n",
    "        .header h1 {{ font-size: 3rem; margin-bottom: 15px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; }}\n",
    "        .header h2 {{ font-size: 1.5rem; color: #666; margin-bottom: 20px; }}\n",
    "        .model-badge {{ display: inline-block; padding: 10px 25px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a52 100%); color: white; border-radius: 30px; font-weight: bold; font-size: 1.1rem; margin: 5px; }}\n",
    "        .csv-info {{ background: rgba(255,255,255,0.8); padding: 15px; border-radius: 10px; margin-top: 15px; font-size: 0.9rem; color: #555; }}\n",
    "        .performance-summary {{ background: linear-gradient(135deg, #ffe6e6, #ffcccc); padding: 30px; border-radius: 20px; margin: 30px 0; text-align: center; }}\n",
    "        .performance-score {{ font-size: 3.5rem; font-weight: bold; color: {flash_color}; margin-bottom: 10px; }}\n",
    "        .performance-label {{ font-size: 1.3rem; color: #37474f; margin-bottom: 8px; }}\n",
    "        .performance-description {{ font-size: 1rem; color: #546e7a; }}\n",
    "        .metrics-overview {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(180px, 1fr)); gap: 20px; margin-bottom: 40px; }}\n",
    "        .metric-card {{ background: rgba(255,255,255,0.95); padding: 25px; border-radius: 20px; text-align: center; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); transition: transform 0.3s ease; }}\n",
    "        .metric-card:hover {{ transform: translateY(-5px); }}\n",
    "        .metric-value {{ font-size: 2.5rem; font-weight: bold; margin-bottom: 10px; }}\n",
    "        .metric-label {{ font-size: 1rem; color: #666; font-weight: 600; margin-bottom: 5px; }}\n",
    "        .metric-description {{ font-size: 0.85rem; color: #888; }}\n",
    "        .excellent {{ color: #10B981; }}\n",
    "        .good {{ color: #F59E0B; }}\n",
    "        .moderate {{ color: #FF6B35; }}\n",
    "        .poor {{ color: #EF4444; }}\n",
    "        .dashboard-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(450px, 1fr)); gap: 25px; margin-bottom: 40px; }}\n",
    "        .chart-section {{ background: rgba(255,255,255,0.95); padding: 30px; border-radius: 20px; backdrop-filter: blur(15px); box-shadow: 0 15px 40px rgba(0,0,0,0.1); }}\n",
    "        .chart-title {{ font-size: 1.3rem; font-weight: bold; margin-bottom: 20px; color: #333; text-align: center; }}\n",
    "        .confusion-matrix {{ display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px; margin: 20px 0; text-align: center; font-size: 0.9rem; }}\n",
    "        .cm-cell {{ padding: 15px; border-radius: 12px; font-weight: bold; display: flex; flex-direction: column; justify-content: center; align-items: center; }}\n",
    "        .cm-header {{ background: linear-gradient(135deg, #f8f9fa, #e9ecef); color: #495057; }}\n",
    "        .cm-tp {{ background: linear-gradient(135deg, #d4edda, #c3e6cb); color: #155724; font-size: 1.8rem; }}\n",
    "        .cm-fp {{ background: linear-gradient(135deg, #f8d7da, #f1b0b7); color: #721c24; font-size: 1.8rem; }}\n",
    "        .cm-fn {{ background: linear-gradient(135deg, #fff3cd, #fce4a6); color: #856404; font-size: 1.8rem; }}\n",
    "        .cm-tn {{ background: linear-gradient(135deg, #d1ecf1, #bee5eb); color: #0c5460; font-size: 1.8rem; }}\n",
    "        .cm-label {{ font-size: 0.75rem; margin-top: 5px; opacity: 0.8; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"dashboard\">\n",
    "        <div class=\"header\">\n",
    "            <h1>‚ö° Flash CSV Meta Classification Metrics</h1>\n",
    "            <h2>texts_images_classification_results_web_multimodal_dataset_300.csv vs Meta Ground Truth</h2>\n",
    "            <div>\n",
    "                <span class=\"model-badge\">Flash CSV Model</span>\n",
    "            </div>\n",
    "            <div class=\"csv-info\">\n",
    "                üìÅ Source CSV: texts_images_classification_results_web_multimodal_dataset_300.csv<br>\n",
    "                üéØ Successfully matched {dataset_info['total_samples']} artifact IDs with ground truth\n",
    "            </div>\n",
    "            <p style=\"margin-top: 20px; color: #666;\">\n",
    "                Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | \n",
    "                Samples: {dataset_info['total_samples']:,} | \n",
    "                Positive: {dataset_info['positive_samples']} | Negative: {dataset_info['negative_samples']}\n",
    "            </p>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"performance-summary\">\n",
    "            <div class=\"performance-score\">{flash_metrics['f1_score']:.3f}</div>\n",
    "            <div class=\"performance-label\">{flash_assessment} F1-Score (Flash CSV Model)</div>\n",
    "            <div class=\"performance-description\">\n",
    "                Flash CSV achieves {flash_metrics['accuracy']:.1%} accuracy with {flash_metrics['precision']:.1%} precision\n",
    "                <br>Agreement with Meta Ground Truth: {agreement['flash_vs_ground_truth']:.1%}\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"metrics-overview\">\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if flash_metrics['f1_score'] >= 0.8 else 'good' if flash_metrics['f1_score'] >= 0.7 else 'moderate' if flash_metrics['f1_score'] >= 0.6 else 'poor'}\">{flash_metrics['f1_score']:.3f}</div>\n",
    "                <div class=\"metric-label\">Flash CSV F1-Score</div>\n",
    "                <div class=\"metric-description\">Harmonic Mean P&R</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if flash_metrics['recall'] >= 0.8 else 'good' if flash_metrics['recall'] >= 0.7 else 'moderate' if flash_metrics['recall'] >= 0.6 else 'poor'}\">{flash_metrics['recall']:.3f}</div>\n",
    "                <div class=\"metric-label\">Flash CSV TPR</div>\n",
    "                <div class=\"metric-description\">True Positive Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if flash_metrics['specificity'] >= 0.8 else 'good' if flash_metrics['specificity'] >= 0.7 else 'moderate' if flash_metrics['specificity'] >= 0.6 else 'poor'}\">{flash_metrics['specificity']:.3f}</div>\n",
    "                <div class=\"metric-label\">Flash CSV TNR</div>\n",
    "                <div class=\"metric-description\">True Negative Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if flash_metrics['accuracy'] >= 0.9 else 'good' if flash_metrics['accuracy'] >= 0.8 else 'moderate' if flash_metrics['accuracy'] >= 0.7 else 'poor'}\">{flash_metrics['accuracy']:.3f}</div>\n",
    "                <div class=\"metric-label\">Flash CSV Accuracy</div>\n",
    "                <div class=\"metric-description\">Overall Correctness</div>\n",
    "            </div>\n",
    "            <div class=\"metric-card\">\n",
    "                <div class=\"metric-value {'excellent' if agreement['flash_vs_ground_truth'] >= 0.8 else 'good' if agreement['flash_vs_ground_truth'] >= 0.7 else 'moderate' if agreement['flash_vs_ground_truth'] >= 0.6 else 'poor'}\">{agreement['flash_vs_ground_truth']:.1%}</div>\n",
    "                <div class=\"metric-label\">Ground Truth Agreement</div>\n",
    "                <div class=\"metric-description\">Flash CSV matches human labels</div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"dashboard-grid\">\n",
    "            <div class=\"chart-section\">\n",
    "                <div class=\"chart-title\">‚ö° Flash CSV Confusion Matrix</div>\n",
    "                <div class=\"confusion-matrix\">\n",
    "                    <div class=\"cm-cell cm-header\"></div>\n",
    "                    <div class=\"cm-cell cm-header\">Meta GT: 0</div>\n",
    "                    <div class=\"cm-cell cm-header\">Meta GT: 1</div>\n",
    "                    \n",
    "                    <div class=\"cm-cell cm-header\">Flash CSV: 0</div>\n",
    "                    <div class=\"cm-cell cm-tn\">\n",
    "                        {flash_cm['true_negatives']}\n",
    "                        <div class=\"cm-label\">True Negatives</div>\n",
    "                    </div>\n",
    "                    <div class=\"cm-cell cm-fn\">\n",
    "                        {flash_cm['false_negatives']}\n",
    "                        <div class=\"cm-label\">False Negatives</div>\n",
    "                    </div>\n",
    "                    \n",
    "                    <div class=\"cm-cell cm-header\">Flash CSV: 1</div>\n",
    "                    <div class=\"cm-cell cm-fp\">\n",
    "                        {flash_cm['false_positives']}\n",
    "                        <div class=\"cm-label\">False Positives</div>\n",
    "                    </div>\n",
    "                    <div class=\"cm-cell cm-tp\">\n",
    "                        {flash_cm['true_positives']}\n",
    "                        <div class=\"cm-label\">True Positives</div>\n",
    "                    </div>\n",
    "                </div>\n",
    "                <div style=\"text-align: center; margin-top: 15px;\">\n",
    "                    <p><strong>TP:</strong> {flash_cm['true_positives']} | <strong>FP:</strong> {flash_cm['false_positives']} | <strong>TN:</strong> {flash_cm['true_negatives']} | <strong>FN:</strong> {flash_cm['false_negatives']}</p>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "        \n",
    "        return html_content\n",
    "    \n",
    "    def generate_classification_dashboard(self):\n",
    "        \"\"\"Main function to generate the classification dashboard\"\"\"\n",
    "        print(\"üöÄ STARTING FLASH CSV WEB CLASSIFICATION DASHBOARD GENERATION (FIXED)\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"üìÅ Using CSV file: {self.csv_file_path}\")\n",
    "        print(f\"üìä Using BigQuery table: {PROJECT_ID}.{DATASET_ID}.{META_GROUND_TRUTH_TABLE}\")\n",
    "        \n",
    "        try:\n",
    "            # Load results with ground truth\n",
    "            df = self.load_results_with_ground_truth()\n",
    "            \n",
    "            if df.empty:\n",
    "                print(\"‚ùå No data available for dashboard\")\n",
    "                return None\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = self.calculate_classification_metrics(df)\n",
    "            \n",
    "            if not metrics:\n",
    "                print(\"‚ùå Could not calculate metrics\")\n",
    "                return None\n",
    "            \n",
    "            # Create dashboard\n",
    "            print(\"\\nüìä Creating Flash CSV Meta Classification Dashboard...\")\n",
    "            dashboard_html = self.create_classification_dashboard(metrics, df)\n",
    "            \n",
    "            # Save files\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            dashboard_filename = f\"flash_csv_web_classification_dashboard_{timestamp}.html\"\n",
    "            metrics_filename = f\"flash_csv_web_classification_metrics_{timestamp}.json\"\n",
    "            \n",
    "            with open(dashboard_filename, \"w\", encoding='utf-8') as f:\n",
    "                f.write(dashboard_html)\n",
    "            \n",
    "            with open(metrics_filename, \"w\") as f:\n",
    "                json.dump(metrics, f, indent=2, default=str)\n",
    "            \n",
    "            print(f\"‚úÖ Dashboard saved: {dashboard_filename}\")\n",
    "            print(f\"‚úÖ Metrics saved: {metrics_filename}\")\n",
    "            \n",
    "            # Summary\n",
    "            flash_metrics = metrics['flash_csv_metrics']\n",
    "            agreement = metrics['agreement_analysis']\n",
    "            \n",
    "            print(f\"\\nüìä FLASH CSV WEB CLASSIFICATION METRICS SUMMARY:\")\n",
    "            print(f\"   ‚ö° Flash CSV Model:\")\n",
    "            print(f\"      F1-Score: {flash_metrics['f1_score']:.3f}\")\n",
    "            print(f\"      Precision: {flash_metrics['precision']:.3f}\")\n",
    "            print(f\"      Recall: {flash_metrics['recall']:.3f}\") \n",
    "            print(f\"      Accuracy: {flash_metrics['accuracy']:.3f}\")\n",
    "            print(f\"      TP: {flash_metrics['confusion_matrix']['true_positives']}, FP: {flash_metrics['confusion_matrix']['false_positives']}\")\n",
    "            print(f\"      TN: {flash_metrics['confusion_matrix']['true_negatives']}, FN: {flash_metrics['confusion_matrix']['false_negatives']}\")\n",
    "            \n",
    "            print(f\"\\n   ‚öñÔ∏è Agreement Rates:\")\n",
    "            print(f\"      Flash CSV vs Web Ground Truth: {agreement['flash_vs_ground_truth']:.1%}\")\n",
    "            \n",
    "            return dashboard_filename, metrics_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Dashboard generation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the Flash CSV Meta classification dashboard generation\"\"\"\n",
    "    # Option 1: Use a different CSV file with Meta IDs\n",
    "    # dashboard_generator = FlashMetaCSVClassificationDashboard(csv_file_path=\"your_meta_ids_file.csv\")\n",
    "    \n",
    "    # Option 2: Use a different column from the current file\n",
    "    # dashboard_generator = FlashMetaCSVClassificationDashboard(id_column_name='source')\n",
    "    \n",
    "    # Option 3: Default behavior\n",
    "    dashboard_generator = FlashMetaCSVClassificationDashboard()\n",
    "    result = dashboard_generator.generate_classification_dashboard()\n",
    "    \n",
    "    if result:\n",
    "        dashboard_file, metrics_file = result\n",
    "        print(f\"\\nüéâ SUCCESS!\")\n",
    "        print(f\"üìÅ Files generated:\")\n",
    "        print(f\"   1. {dashboard_file}\")\n",
    "        print(f\"   2. {metrics_file}\")\n",
    "        print(f\"\\nüí° Open {dashboard_file} in your browser to view the dashboard!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Dashboard generation failed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fa38da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44faa1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
